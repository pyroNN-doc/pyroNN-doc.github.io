{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyroNN v-1.1.0 documentation","text":"<p>Download PowerPoint Presentation</p> <p>We will demonstrate the capabilities of Pyro-NN, a differentiable reconstruction framework.</p> <ol> <li>\ud83d\udcd8 Introduction to Pyro-NN: Gain a basic understanding of the theory behind Pyro-NN.</li> <li>\u2699\ufe0f Installation and Setup: Learn how to install and set up Pyro-NN on your machine.</li> <li>\ud83d\udd2c Working with Projection Data: Explore how to work with projection data to initiate a reconstruction.</li> <li>\ud83d\ude80 Advanced Examples: Discover more advanced examples showcasing the usage of Pyro-NN.</li> </ol>"},{"location":"#1-introduction-to-pyro-nn","title":"1. \ud83d\udcd8 Introduction to Pyro-NN","text":""},{"location":"#11-motivation","title":"1.1 Motivation \ud83d\udca1","text":"<ul> <li>We can make use of known operators</li> </ul> <ul> <li>For Deep Learning, the loss function and the amount of parameters to train can be reduced \ud83c\udfaf</li> <li>We can have gradient flow through different domains \ud83d\udd04</li> <li>Parts of the neural network get interpretable, e.g. as filters \ud83e\uddd0</li> </ul>"},{"location":"#12-basic-overview","title":"1.2 Basic Overview \ud83c\udf10","text":"<p>General Notes \ud83d\udccb</p> <pre><code>- \ud83e\udd16 Supports TensorFlow and PyTorch\n- \ud83d\udca5 Full GPU Integration\n- \ud83d\udd13 Open Source\n- \ud83d\udcdc Apache 2.0 License\n</code></pre>"},{"location":"Helpers/","title":"Preprocessing","text":""},{"location":"Helpers/#helper-functions-for-data-preprocessing","title":"Helper functions for Data Preprocessing","text":"STL to Numpy Arrays (.h5 container)Numpy (.h5 container) to Sinograms <pre><code>import os\nimport numpy as np\nimport h5py\nimport stltovoxel\nfrom PIL import Image\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport tempfile\nfrom pathlib import Path\nfrom typing import Iterator, Optional\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport itertools\nfrom datetime import datetime\n\ndef setup_logging():\n    logger = logging.getLogger()\n\n    # \u5982\u679clogger\u5df2\u7ecf\u6709\u5904\u7406\u5668\uff0c\u8bf4\u660e\u5df2\u7ecf\u88ab\u8bbe\u7f6e\u8fc7\uff0c\u76f4\u63a5\u8fd4\u56de\n    if logger.handlers:\n        return logger\n\n    logger.setLevel(logging.INFO)\n\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    return logger\n\n\nlogger = setup_logging()\n\ndef convert_stl_to_png(stl_path: str, resolution: int = 512, pad: int = 0) -&gt; str:\n    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n        png_path = tmp.name\n    stltovoxel.convert_file(stl_path, png_path, resolution=resolution, pad=pad, parallel=True)\n    return png_path\n\ndef load_png_to_numpy(png_path: str, start_num: int, end_num: int) -&gt; np.ndarray:\n    target_size = (512, 512)\n    images = []\n\n    for i in range(start_num, end_num + 1):\n        filename = f\"{png_path[:-4]}_{i:03d}.png\"\n        if not os.path.exists(filename):\n            logger.warning(f\"File {filename} does not exist. Skipping.\")\n            continue\n        with Image.open(filename) as img:\n            resized_img = img.resize(target_size, Image.LANCZOS)\n            images.append(np.array(resized_img))\n\n    images_array = np.array(images)\n\n    if len(images) != 512:\n        padded_array = np.zeros((512, 512, 512), dtype=np.uint8)\n        padded_array[:len(images)] = images_array[:512]\n        return padded_array\n\n    return images_array.astype(np.uint8)\n\ndef process_single_stl(stl_file: str) -&gt; np.ndarray:\n    png_path = convert_stl_to_png(stl_file)\n    array = load_png_to_numpy(png_path, 0, 511)\n\n    # Delete PNG files\n    for png_file in Path(png_path[:-4]).glob('*.png'):\n        png_file.unlink()\n\n    return array\n\ndef stl_file_generator(input_dir: str, max_files: Optional[int] = None) -&gt; Iterator[Path]:\n    count = 0\n    for stl_file in Path(input_dir).rglob('*.stl'):\n        yield stl_file\n        count += 1\n        if max_files is not None and count &gt;= max_files:\n            break\n\ndef process_stl_files(input_dir: str, output_dir: str, batch_size: int = 1000, max_files: Optional[int] = None):\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    num_workers = max(1, multiprocessing.cpu_count() - 1)\n    stl_files = stl_file_generator(input_dir, max_files)\n\n    batch_num = 0\n    file_count = 0\n\n    while True:\n        h5_filename = Path(output_dir) / f'batch_{batch_num:03d}.h5'\n\n        with h5py.File(h5_filename, 'w') as h5f:\n            with ProcessPoolExecutor(max_workers=num_workers) as executor:\n                batch_files = list(itertools.islice(stl_files, batch_size))\n                if not batch_files:\n                    break  # No more files to process\n\n                futures = {executor.submit(process_single_stl, str(stl_file)): i \n                        for i, stl_file in enumerate(batch_files)}\n\n                for future in as_completed(futures):\n                    i = futures[future]\n                    try:\n                        array = future.result()\n                        h5f.create_dataset(f'array_{i:03d}', data=array, compression='gzip')\n                        file_count += 1\n                        logger.info(f\"Processed file {file_count}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing file {file_count + 1}: {e}\")\n\n        logger.info(f\"Batch {batch_num} saved to {h5_filename}\")\n        batch_num += 1\n\n    logger.info(f\"Total processed files: {file_count}\")\n\nif __name__ == \"__main__\":\n    input_dir = r\"D:\\datasets\\stl2\\abc_0000_stl2_v00\"\n    output_dir = r\"D:\\datasets\\stl2\\output_h5\"\n    batch_size = 10\n    max_files = 100  # Set this to the number of files you want to process, or None to process all files\n    process_stl_files(input_dir, output_dir, batch_size=batch_size, max_files=max_files)\n</code></pre> <pre><code>import os\nimport h5py\nimport numpy as np\nimport torch\nfrom pyronn.ct_reconstruction.geometry.geometry import Geometry\nfrom pyronn.ct_reconstruction.helpers.filters import weights, filters\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import (\n    circular_trajectory_3d,\n)\nfrom torch.nn import functional as F\nfrom pyronn.ct_reconstruction.layers.projection_3d import ConeProjection3D\nfrom pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjection3D\nfrom scipy.ndimage import convolve, gaussian_filter, rotate\nfrom scipy.signal import convolve2d\nfrom scipy.signal import fftconvolve\n\nfrom update.beam_harden import simulate_polychromatic_spectrum, apply_beam_hardening4D\n\n\nclass SinogramNoiseSimulator:\n    def __init__(self, sinogram):\n        \"\"\"\n        Initialize the SinogramNoiseSimulator with a sinogram.\n\n        :param sinogram: Original sinogram data of shape (1, num_projections, height, width)\n        \"\"\"\n        self.sinogram = sinogram\n\n    def add_detector_jitter(self, max_jitter):\n        \"\"\"\n        Simulate detector jitter in the sinogram.\n\n        :param max_jitter: Maximum amount of jitter (in pixels)\n        \"\"\"\n        num_projections = self.sinogram.shape[1]\n        jittered_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            jitter = np.random.randint(-max_jitter, max_jitter)\n            jittered_projection = np.roll(self.sinogram[0, i, :, :], jitter, axis=1)\n            jittered_projection = np.roll(jittered_projection, jitter, axis=0)\n\n            jittered_sinogram[0, i, :, :] = jittered_projection\n\n        self.sinogram = jittered_sinogram\n\n    def add_gantry_motion_blur(self, blur_length, angle_range):\n        \"\"\"\n        Apply gantry motion blur to the sinogram.\n\n        :param blur_length: Length of the blur kernel\n        :param angle_range: Range of angles (in degrees) for the gantry rotation during acquisition\n        \"\"\"\n        num_projections, height, width = self.sinogram.shape[1:]\n\n        # Create a curved blur kernel\n        kernel_size = blur_length * 2 + 1\n        y, x = np.ogrid[-blur_length:blur_length+1, -blur_length:blur_length+1]\n        mask = x**2 + y**2 &lt;= blur_length**2\n        kernel = np.zeros((kernel_size, kernel_size))\n        kernel[mask] = 1\n        kernel /= kernel.sum()  # Normalize the kernel\n\n        blurred_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            projection = self.sinogram[0, i, :, :]\n\n            # Calculate the rotation angle for this projection\n            rotation_angle = (i / num_projections) * angle_range\n\n            # Rotate the kernel\n            rotated_kernel = rotate(kernel, rotation_angle, reshape=False)\n\n            # Apply the rotated kernel\n            blurred_projection = convolve2d(projection, rotated_kernel, mode='same', boundary='wrap')\n\n            blurred_sinogram[0, i, :, :] = blurred_projection\n\n        self.sinogram = blurred_sinogram\n\n    def add_high_frequency_noise(self, noise_level, high_freq_strength):\n        \"\"\"\n        Add high-frequency noise to the sinogram.\n\n        :param noise_level: Standard deviation of the Gaussian noise\n        :param high_freq_strength: Strength of the high-frequency component\n        \"\"\"\n        num_projections = self.sinogram.shape[1]\n        noisy_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            noise = np.random.normal(0, noise_level, size=self.sinogram.shape[2:])\n            low_freq_noise = gaussian_filter(noise, sigma=high_freq_strength)\n            high_freq_noise = noise - low_freq_noise\n\n            noisy_sinogram[0, i, :, :] = self.sinogram[0, i, :, :] + high_freq_noise\n\n        self.sinogram = noisy_sinogram\n\n\n    def add_poisson_noise(self, scale_factor):\n        \"\"\"\n        Add Poisson noise to the sinogram.\n\n        :param scale_factor: Scale factor to control the intensity of the noise\n        \"\"\"\n        noisy_sinogram = np.zeros_like(self.sinogram)\n        for i in range(self.sinogram.shape[1]):\n            noisy_sinogram[0, i, :, :] = np.random.poisson(self.sinogram[0, i, :, :] * scale_factor) / scale_factor\n        self.sinogram = noisy_sinogram\n\n    def add_aliasing_artifacts(self, undersample_factor):\n        num_projections = self.sinogram.shape[1]\n        self.sinogram = self.sinogram[:, ::undersample_factor, :, :]\n\n    def add_metal_artifacts(self, metal_positions, metal_intensity):\n        \"\"\"\n        Add metal artifacts to the sinogram.\n\n        :param sinogram: The original sinogram data\n        :param metal_positions: List of tuples indicating the positions of metal objects in the sinogram\n        :param metal_intensity: The intensity of the metal artifact\n        :return: The sinogram with metal artifacts\n        \"\"\"\n        for pos in metal_positions:\n            self.sinogram[:, :, pos[0], pos[1]] += metal_intensity\n\n    def add_gaussian_noise(self, mean, std):\n        \"\"\"\n        Add Gaussian noise to the sinogram.\n\n        :param mean: Mean of the Gaussian noise\n        :param std: Standard deviation of the Gaussian noise\n        \"\"\"\n        gaussian_noise = np.random.normal(mean, std, self.sinogram.shape)\n        self.sinogram += gaussian_noise\n        self.sinogram = self.sinogram.float()\n\n    def add_ring_artifacts(self, num_rings):\n        \"\"\"\n        Add ring artifacts to the sinogram.\n\n        :param num_rings: Number of rings to add\n        \"\"\"\n\n        num_projections = self.sinogram.shape[1]\n        height = self.sinogram.shape[2]\n        width = self.sinogram.shape[3]\n\n        for _ in range(num_rings):\n            # Randomly select detector index\n            broken_detector_index = np.random.randint(0, width)\n\n            # Randomly select the range of heights for the artifact\n            start_height = np.random.randint(0, height - 1)\n            end_height = np.random.randint(start_height + 1, height)\n\n            # Add the artifact to the specified region of the sinogram\n            self.sinogram[:, :, start_height:end_height, broken_detector_index] = 0\n\n    def add_scatter_noise(self, scatter_fraction, energy_MeV=0.14):\n        \"\"\"\n        Add realistic scatter noise to the CT sinogram.\n\n        :param scatter_fraction: Fraction of total intensity that becomes scatter (0-1)\n        :param energy_MeV: X-ray energy in MeV (affects scatter distribution)\n        \"\"\"\n        # Calculate total intensity\n        total_intensity = self.sinogram.sum()\n\n        # Calculate scatter intensity\n        scatter_intensity = scatter_fraction * total_intensity\n\n        # Generate object-dependent scatter distribution\n        scatter_base = self._generate_object_dependent_scatter()\n\n        # Apply energy-dependent scatter kernel\n        scatter_noise = self._apply_scatter_kernel(scatter_base, energy_MeV)\n\n        # Normalize scatter noise to desired intensity\n        scatter_noise = scatter_noise / scatter_noise.sum() * scatter_intensity\n\n        # Add scatter to original sinogram\n        self.sinogram += scatter_noise\n\n    def _generate_object_dependent_scatter(self):\n        \"\"\"Generate initial scatter distribution based on object density.\"\"\"\n        # Use softmax to create a probability distribution based on object density\n        scatter_prob = torch.softmax(self.sinogram.flatten(), dim=0).reshape(self.sinogram.shape)\n\n        # Generate initial scatter based on this probability\n        scatter_base = torch.poisson(scatter_prob * 1000)  # Multiplier for more pronounced effect\n        return scatter_base\n\n    def _apply_scatter_kernel(self, scatter_base, energy_MeV):\n        \"\"\"Apply an energy-dependent scatter kernel.\"\"\"\n        # Convert to NumPy for convolution\n        scatter_np = scatter_base.numpy()\n\n        # Create an anisotropic kernel favoring forward scatter\n        kernel = self._create_anisotropic_kernel(scatter_np.shape, energy_MeV)\n\n        # Apply the kernel using FFT convolution\n        scatter_noise = fftconvolve(scatter_np, kernel, mode='same')\n\n        return torch.tensor(scatter_noise, dtype=torch.float32)\n\n    def _create_anisotropic_kernel(self, shape, energy_MeV):\n        \"\"\"Create an anisotropic scatter kernel based on energy and sinogram shape.\"\"\"\n        # Ensure we're working with the last two dimensions for 2D convolution\n        if len(shape) &gt; 2:\n            rows, cols = shape[-2], shape[-1]\n        else:\n            rows, cols = shape\n\n        y, x = np.ogrid[-rows//2:rows//2, -cols//2:cols//2]\n\n        # Adjust these parameters based on your specific CT geometry and energy range\n        forward_sigma = cols / 8 * (energy_MeV / 0.1)**0.5  # Increases with energy\n        lateral_sigma = rows / 16\n\n        # Create an elliptical Gaussian kernel\n        kernel = np.exp(-(x**2 / (2 * forward_sigma**2) + y**2 / (2 * lateral_sigma**2)))\n\n        # Normalize the kernel\n        kernel /= kernel.sum()\n\n        # If the sinogram has more than 2 dimensions, expand the kernel\n        if len(shape) &gt; 2:\n            for _ in range(len(shape) - 2):\n                kernel = np.expand_dims(kernel, axis=0)\n\n        return kernel\n    def add_beam_hardening(self, material):\n        print(\"adding beam hardening\")\n        energy_bins = np.linspace(1, 120, 100)\n\n        # Simulate polychromatic spectrum\n        spectrum = simulate_polychromatic_spectrum(energy_bins)\n\n        # Apply beam hardening\n        hardened_projections = apply_beam_hardening4D(self.sinogram[0], energy_bins, spectrum, material)\n\n        hardened_projections = torch.tensor(\n            np.expand_dims(hardened_projections, axis=0).copy(), dtype=torch.float32\n        )\n        self.sinogram = hardened_projections\n\ndef apply_noise(noise_simulator, noise_type):\n    if noise_type == \"add_gantry_motion_blur\":\n        noise_simulator.add_gantry_motion_blur(blur_length=5, angle_range=0.5)\n    elif noise_type == \"add_poisson_noise\":\n        noise_simulator.add_poisson_noise(scale_factor=10)\n    elif noise_type == \"add_ring_artifacts\":\n        noise_simulator.add_ring_artifacts(num_rings=5)\n    elif noise_type == \"add_scatter_noise\":\n        noise_simulator.add_scatter_noise(0.5)\n    elif noise_type == \"add_beam_hardening\":\n        noise_simulator.add_beam_hardening(material=\"bone\")\n    elif noise_type == \"add_detector_jitter\":\n        noise_simulator.add_detector_jitter(max_jitter=5)\n    else:\n        raise ValueError(f\"Unknown noise type: {noise_type}\")\n\ndef save_results(results, input_file, output_path, noise_type, compression=\"gzip\"):\n    base_name = os.path.basename(input_file)\n    name_without_extension = os.path.splitext(base_name)[0]\n\n    output_file = os.path.join(output_path, f\"{name_without_extension}_{noise_type}_processed.h5\")\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n    with h5py.File(output_file, \"w\") as h5f:\n        for i, (sinogram, noisy_sinogram, volume, noisy_volume) in enumerate(results):\n            h5f.create_dataset(f\"sinogram_{i:03d}\", data=sinogram.numpy(), compression=compression)\n            h5f.create_dataset(f\"noisy_sinogram_{i:03d}\", data=noisy_sinogram, compression=compression)\n            h5f.create_dataset(f\"volume_{i:03d}\", data=volume.numpy(), compression=compression)\n            h5f.create_dataset(f\"noisy_volume_{i:03d}\", data=noisy_volume.numpy(), compression=compression)\n    print(f\"Processed data for {noise_type} saved to {output_file}\")\n\ndef process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types):\n    with h5py.File(input_file, 'r') as h5f:\n        num_volumes = sum(1 for key in h5f.keys() if key.startswith('array_'))\n        print(f\"Number of volumes in the dataset: {num_volumes}\")\n\n        if num_volumes == 0:\n            raise ValueError(\"No datasets found with 'array_' prefix in the input file.\")\n\n        # Read the first volume to get the shape\n        first_volume = h5f['array_000'][()]\n        volume_shape = first_volume.shape\n        print(f\"Volume shape: {volume_shape}\")\n\n        # Assuming cubic voxels, calculate volume spacing\n        volume_spacing = (1, 1, 1)  # You might want to adjust this if you have specific spacing information\n\n        # Initialize geometry\n        geometry = Geometry()\n        geometry.init_from_parameters(\n            volume_shape=volume_shape,\n            volume_spacing=volume_spacing,\n            detector_shape=detector_shape,\n            detector_spacing=detector_spacing,\n            number_of_projections=num_projections,\n            angular_range=angular_range,\n            trajectory=circular_trajectory_3d,\n            source_isocenter_distance=sid,\n            source_detector_distance=sdd,\n        )\n\n        reco_filter = torch.tensor(\n            filters.ram_lak_3D(\n                geometry.detector_shape,\n                geometry.detector_spacing,\n                geometry.number_of_projections,\n            ),\n            dtype=torch.float32,\n        ).cuda()\n\n        for noise_type in noise_types:\n            results = []\n            for index in range(num_volumes):\n                volume = h5f[f'array_{index:03d}'][()]\n                volume_tensor = torch.tensor(\n                    np.expand_dims(volume, axis=0), dtype=torch.float32\n                ).cuda()\n                sinogram = ConeProjection3D(hardware_interp=True).forward(\n                    volume_tensor, **geometry\n                )\n                sinogram_tensor = sinogram.detach().cpu()\n\n                noise_simulator = SinogramNoiseSimulator(sinogram_tensor)\n\n                # Apply the current noise type\n                apply_noise(noise_simulator, noise_type)\n\n                noisy_sinogram = noise_simulator.sinogram\n                noisy_sinogram = noisy_sinogram * weights.cosine_weights_3d(geometry)\n\n                x = torch.fft.fft(torch.Tensor(noisy_sinogram).cuda(), dim=-1)\n                x = torch.multiply(x, reco_filter)\n                x = torch.fft.ifft(x, dim=-1).real\n\n                noisy_reco = ConeBackProjection3D(hardware_interp=True).forward(\n                    x.contiguous(), **geometry\n                )\n\n                noisy_reco = F.relu(noisy_reco)\n                noisy_reco = noisy_reco.cpu()\n\n                results.append((sinogram_tensor, noisy_sinogram, volume_tensor.cpu(), noisy_reco))\n                print(f\"Processed volume {index + 1}/{num_volumes} with {noise_type}\")\n\n            save_results(results, input_file, output_path, noise_type)\n\ndef main():\n    # Define parameters\n    detector_row = 800\n    detector_col = 800\n    detector_spacer = 1\n    num_projections = 400\n    angular_range = 2 * np.pi\n    sdd = 3000  # Source-Detector distance\n    sid = 2400  # Source-Isocenter distance\n    output_path = r\"D:\\datasets\\stl2\\output_h5\\noisy_data\"\n    input_file = r\"D:\\datasets\\stl2\\output_h5\\batch_003.h5\"\n    # input_file = r\"C:\\Users\\sun\\OneDrive - Fraunhofer\\PhD\\known_operator\\2D-2-3D\\pancreas_ct_data.h5\"\n\n    os.makedirs(output_path, exist_ok=True)\n\n    detector_shape = (detector_row, detector_col)\n    detector_spacing = (detector_spacer, detector_spacer)\n\n    noise_types = [\n        \"add_gantry_motion_blur\",\n        # \"add_poisson_noise\",\n        # \"add_ring_artifacts\",\n        # # \"add_scatter_noise\",\n        # \"add_beam_hardening\",\n        \"add_detector_jitter\"\n    ]\n\n    process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"PyroNN_demonstration/","title":"Filtered Back Projection","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport pyronn\nimport numpy as np\n</pre> import torch import pyronn import numpy as np In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom get_natsorted_images import get_images_sorted\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Define folder and load image natually sorted. \nprojection_folder = r\"path/to/your/projection/folder\"\nprojection_paths = get_images_sorted(projection_folder)\nimages = list()\nheaders = list()\n# Define basic transformation to get projections as tensors and with data range (0,1)\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0, 65535,inplace = True)]) #For Range -1,1 : (32767.5, 32767.5)\nfor i,proj in enumerate(projection_paths):\n    header, img = np.load_file(proj) #if header/metadata available\n    img = Image.fromarray(img)\n    # Convert image to float before normalization\n    img = img.convert('F')       \n    images.append(transform(img).float())\n    headers.append(header)\n\n# Visualization of the images\n# Create the figure and axis object\nfig, ax = plt.subplots()\n\n# Load a single image, transform to numpy array for plotting\nimage = images[5].cpu().squeeze().detach().numpy()\n\n# Display the image\nplt.figure()\nax.imshow(image, cmap=\"gray\")\nax.axis('off')\n\n# Show the figure\nplt.show()\n</pre> import matplotlib.pyplot as plt from get_natsorted_images import get_images_sorted from PIL import Image import torchvision.transforms as transforms  # Define folder and load image natually sorted.  projection_folder = r\"path/to/your/projection/folder\" projection_paths = get_images_sorted(projection_folder) images = list() headers = list() # Define basic transformation to get projections as tensors and with data range (0,1) transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0, 65535,inplace = True)]) #For Range -1,1 : (32767.5, 32767.5) for i,proj in enumerate(projection_paths):     header, img = np.load_file(proj) #if header/metadata available     img = Image.fromarray(img)     # Convert image to float before normalization     img = img.convert('F')            images.append(transform(img).float())     headers.append(header)  # Visualization of the images # Create the figure and axis object fig, ax = plt.subplots()  # Load a single image, transform to numpy array for plotting image = images[5].cpu().squeeze().detach().numpy()  # Display the image plt.figure() ax.imshow(image, cmap=\"gray\") ax.axis('off')  # Show the figure plt.show()  <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>from pyronn.ct_reconstruction.geometry.geometry import Geometry\ngeom = Geometry()\n# Volume shape in Voxel, Volume spacing in mm\ngeom.init_from_header(projection_headers =headers,volume_shape=[300,300,300],volume_spacing=0.5) #custom metadata function if available\n</pre> from pyronn.ct_reconstruction.geometry.geometry import Geometry geom = Geometry() # Volume shape in Voxel, Volume spacing in mm geom.init_from_header(projection_headers =headers,volume_shape=[300,300,300],volume_spacing=0.5) #custom metadata function if available <pre>circ\ncirc\n</pre> <p>Since we already have some projections available, let's build a sinogram. In future, the following method should be included in the Pyro-NN Package:</p> In\u00a0[5]: Copied! <pre>def build_sinogram(images):\n    n = len(images)\n    nc = 1\n    _, width, height = images[0].size()\n    transformed_images = torch.zeros((nc, n, height, width), dtype=torch.float32)\n    # 0 you get when you input 1. in a log function. The maximum value not being infinity is torch.finfo(torch.float32).eps\n    transform = transforms.Compose([ transforms.Normalize(0,5)]) #-torch.log(torch.tensor(torch.finfo(torch.float32).eps))\n    for i in range(0, n):\n        transformed_images[:, i, :, :] = transform(-torch.log(images[i].float()))\n    return transformed_images\n\nsinogram = build_sinogram(images=images).cuda()\n\nprint(\"Shape of Sinogram: \",sinogram.shape)\nprint(\"Minimum Value in Sinogram: \",sinogram.flatten().min())\nprint(\"Maximum Value in Sinogram: \",sinogram.flatten().max())\n</pre> def build_sinogram(images):     n = len(images)     nc = 1     _, width, height = images[0].size()     transformed_images = torch.zeros((nc, n, height, width), dtype=torch.float32)     # 0 you get when you input 1. in a log function. The maximum value not being infinity is torch.finfo(torch.float32).eps     transform = transforms.Compose([ transforms.Normalize(0,5)]) #-torch.log(torch.tensor(torch.finfo(torch.float32).eps))     for i in range(0, n):         transformed_images[:, i, :, :] = transform(-torch.log(images[i].float()))     return transformed_images  sinogram = build_sinogram(images=images).cuda()  print(\"Shape of Sinogram: \",sinogram.shape) print(\"Minimum Value in Sinogram: \",sinogram.flatten().min()) print(\"Maximum Value in Sinogram: \",sinogram.flatten().max()) <pre>Shape of Sinogram:  torch.Size([1, 90, 512, 512])\nMinimum Value in Sinogram:  tensor(0.0055, device='cuda:0')\nMaximum Value in Sinogram:  tensor(0.7568, device='cuda:0')\n</pre> In\u00a0[6]: Copied! <pre># Visualization of the log transformed sinogram\n# Create the figure and axis object\nfig, ax = plt.subplots()\n\n# Load a single image, transform to numpy array for plotting\nimage = sinogram[0][5].cpu().squeeze().detach().numpy()\n\n# Display the image\nplt.figure()\nax.imshow(image, cmap=\"gray\")\nax.axis('off')\n\n# Show the figure\nplt.show()\n</pre> # Visualization of the log transformed sinogram # Create the figure and axis object fig, ax = plt.subplots()  # Load a single image, transform to numpy array for plotting image = sinogram[0][5].cpu().squeeze().detach().numpy()  # Display the image plt.figure() ax.imshow(image, cmap=\"gray\") ax.axis('off')  # Show the figure plt.show() <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> <p>Next, we need to initialize the reconstruction layer. For presentation purpose, we initialize all implemented layers.</p> In\u00a0[7]: Copied! <pre>from pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjection3D\nfrom pyronn.ct_reconstruction.layers.projection_3d import ConeProjection3D\nfrom pyronn.ct_reconstruction.layers.projection_2d import FanProjection2D,ParallelProjection2D\nfrom pyronn.ct_reconstruction.layers.backprojection_2d import FanBackProjection2D,ParallelBackProjection2D\n\n# Cone Beam 3D Forward and Backward Projector\nconeForwardProjection3D = ConeProjection3D(hardware_interp=True) # Reconstruction -&gt; Sinogram\nconeBackwardProjection3D = ConeBackProjection3D(hardware_interp=True) # Sinogram -&gt; Reconstruction\n\n# Fan Beam 2D Forward and Backward Projector\nfanForwardProjection2D = FanProjection2D() # Reconstruction -&gt; Sinogram\nfanBackwardProjection2D = FanBackProjection2D() # Sinogram -&gt; Reconstruction\n\n# Parallel Beam 2D Forward and Backward Projector\nparallelForwardProjection2D = ParallelProjection2D() # Reconstruction -&gt; Sinogram\nparallelBackwardProjection2D = ParallelBackProjection2D() # Sinogram -&gt; Reconstruction\n</pre> from pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjection3D from pyronn.ct_reconstruction.layers.projection_3d import ConeProjection3D from pyronn.ct_reconstruction.layers.projection_2d import FanProjection2D,ParallelProjection2D from pyronn.ct_reconstruction.layers.backprojection_2d import FanBackProjection2D,ParallelBackProjection2D  # Cone Beam 3D Forward and Backward Projector coneForwardProjection3D = ConeProjection3D(hardware_interp=True) # Reconstruction -&gt; Sinogram coneBackwardProjection3D = ConeBackProjection3D(hardware_interp=True) # Sinogram -&gt; Reconstruction  # Fan Beam 2D Forward and Backward Projector fanForwardProjection2D = FanProjection2D() # Reconstruction -&gt; Sinogram fanBackwardProjection2D = FanBackProjection2D() # Sinogram -&gt; Reconstruction  # Parallel Beam 2D Forward and Backward Projector parallelForwardProjection2D = ParallelProjection2D() # Reconstruction -&gt; Sinogram parallelBackwardProjection2D = ParallelBackProjection2D() # Sinogram -&gt; Reconstruction <p>With all of those initialized, we can start reconstructing our projections. Hence, we will perform a basic filtered backprojection for a circular scan.</p> In\u00a0[8]: Copied! <pre>from pyronn.ct_reconstruction.helpers.filters.filters import ramp, ramp_3D, ram_lak_3D, shepp_logan_3D\nfrom visualize_reconstruction import show_reco_views\nfrom pyronn.ct_reconstruction.helpers.phantoms import shepp_logan\n\ndef FBP_ConeBeam(sinogram,geometry):\n    reco_filter = torch.tensor(shepp_logan_3D(geometry.detector_shape,geometry.detector_spacing,geometry.number_of_projections),dtype=torch.float32).cuda()\n    # Filter Sinogram in Fourier Domain\n    x = torch.fft.fft(sinogram.cuda(),dim=-1,norm=\"ortho\")\n    x = torch.multiply(x,reco_filter)\n    x = torch.fft.ifft(x,dim=-1,norm=\"ortho\").real\n\n    # Reconstruction\n    reconstruction = coneBackwardProjection3D.forward(x.contiguous(), **geometry)\n\n    return reconstruction\n\nreconstruction = FBP_ConeBeam(sinogram,geom)\nshow_reco_views(reconstruction,geom)\n</pre> from pyronn.ct_reconstruction.helpers.filters.filters import ramp, ramp_3D, ram_lak_3D, shepp_logan_3D from visualize_reconstruction import show_reco_views from pyronn.ct_reconstruction.helpers.phantoms import shepp_logan  def FBP_ConeBeam(sinogram,geometry):     reco_filter = torch.tensor(shepp_logan_3D(geometry.detector_shape,geometry.detector_spacing,geometry.number_of_projections),dtype=torch.float32).cuda()     # Filter Sinogram in Fourier Domain     x = torch.fft.fft(sinogram.cuda(),dim=-1,norm=\"ortho\")     x = torch.multiply(x,reco_filter)     x = torch.fft.ifft(x,dim=-1,norm=\"ortho\").real      # Reconstruction     reconstruction = coneBackwardProjection3D.forward(x.contiguous(), **geometry)      return reconstruction  reconstruction = FBP_ConeBeam(sinogram,geom) show_reco_views(reconstruction,geom)"},{"location":"PyroNN_demonstration/#filtered-back-projection","title":"Filtered Back Projection\u00b6","text":""},{"location":"PyroNN_demonstration/#working-with-projection-data","title":"Working with Projection Data\u00b6","text":"<p>We will demonstrate an example of reconstructing. To begin, we load the projections with Pytorch.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#cone-3d-reconstruction","title":"Cone 3D Reconstruction","text":"example_cone_3d.py<pre><code># Copyright [2019] [Christopher Syben, Markus Michen]\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# TODO: better imports\nfrom pyronn.ct_reconstruction.layers.projection_3d import ConeProjectionFor3D\nfrom pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjectionFor3D\nfrom pyronn.ct_reconstruction.geometry.geometry_base import GeometryCone3D\nfrom pyronn.ct_reconstruction.helpers.filters import filters\nfrom pyronn.ct_reconstruction.helpers.filters import weights\nfrom pyronn.ct_reconstruction.helpers.phantoms import shepp_logan\nfrom pyronn.ct_reconstruction.helpers.misc.general_utils import fft_and_ifft\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_3d\nfrom pyronn.ct_reconstruction.helpers.filters.filters import shepp_logan_3D\n\n\ndef example_cone_3d():\n    # ------------------ Declare Parameters ------------------\n\n    # Volume Parameters:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    volume_size = 256\n    volume_shape = [volume_size, volume_size, volume_size]\n    volume_spacing = [0.5, 0.5, 0.5]\n\n    # Detector Parameters:\n    detector_shape = [400, 600]\n    detector_spacing = [1, 1]\n\n    # Trajectory Parameters:\n    number_of_projections = 360\n    angular_range = 2 * np.pi\n\n    sdd = 1200\n    sid = 750\n\n    # create Geometry class\n    geometry = GeometryCone3D(volume_shape=volume_shape,volume_spacing=volume_spacing,\n                                detector_shape=detector_shape,detector_spacing=detector_spacing,\n                                number_of_projections=number_of_projections,angular_range=angular_range,\n                                source_isocenter_distance=sid, source_detector_distance=sdd)\n    geometry.set_trajectory(circular_trajectory_3d(geometry.number_of_projections, geometry.angular_range,\n                                                   geometry.detector_spacing, geometry.detector_origin,\n                                                   geometry.source_isocenter_distance,\n                                                   geometry.source_detector_distance,\n                                                   True))\n    # Get Phantom 3d\n    phantom = shepp_logan.shepp_logan_3d(volume_shape)\n    # Add required batch dimension\n    phantom = np.expand_dims(phantom, axis=0)\n\n    # ------------------ Call Layers ------------------\n    # The following code is the new TF2.0 experimental way to tell\n    # Tensorflow only to allocate GPU memory needed rather then allocate every GPU memory available.\n    # This is important for the use of the hardware interpolation projector, otherwise there might be not enough memory left\n    # to allocate the texture memory on the GPU\n\n    sinogram = ConeProjectionFor3D().forward(phantom, geometry)\n\n    reco_filter = shepp_logan_3D(geometry.detector_shape,geometry.detector_spacing,geometry.number_of_projections)\n    x = fft_and_ifft(sinogram, reco_filter)\n\n    reco = ConeBackProjectionFor3D().forward(x, geometry)\n\n    plt.figure()\n    plt.imshow(np.squeeze(reco)[volume_shape[0]//2], cmap=plt.get_cmap('gist_gray'))\n    plt.axis('off')\n    plt.savefig(f'3d_cone_reco.png', dpi=150, transparent=False, bbox_inches='tight')\n\n\nif __name__ == '__main__':\n    example_cone_3d()\n</code></pre>"},{"location":"examples/#fan-2d-reconstruction","title":"Fan 2D Reconstruction","text":"example_fan_2d.py<pre><code># Copyright [2019] [Christopher Syben, Markus Michen]\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# TODO: better imports\nfrom pyronn.ct_reconstruction.layers.projection_2d import FanProjectionFor2D\nfrom pyronn.ct_reconstruction.layers.backprojection_2d import FanBackProjectionFor2D\nfrom pyronn.ct_reconstruction.geometry.geometry_base import GeometryFan2D\nfrom pyronn.ct_reconstruction.helpers.filters import filters\nfrom pyronn.ct_reconstruction.helpers.filters import weights\nfrom pyronn.ct_reconstruction.helpers.phantoms import shepp_logan\nfrom pyronn.ct_reconstruction.helpers.misc.general_utils import fft_and_ifft\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d\n\n\ndef example_fan_2d():\n    # ------------------ Declare Parameters ------------------\n\n    # Volume Parameters:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    volume_size = 512\n    volume_shape = [volume_size, volume_size]\n    volume_spacing = [0.23655,0.23655]\n\n    # Detector Parameters:\n    detector_shape = [512]\n    detector_spacing = [0.8, 0.8]\n\n    # Trajectory Parameters:\n    number_of_projections = 512\n    angular_range = 2*np.pi\n\n    sdd = 200\n    sid = 100\n\n    # create Geometry class\n    geometry = GeometryFan2D(volume_shape, volume_spacing,\n                             detector_shape, detector_spacing,\n                             number_of_projections, angular_range,\n                             sdd, sid)\n    geometry.set_trajectory(circular_trajectory_2d(geometry.number_of_projections, geometry.angular_range, True))\n\n    # Get Phantom\n    phantom = shepp_logan.shepp_logan_enhanced(volume_shape)\n    # Add required batch dimension\n    phantom = np.expand_dims(phantom,axis=0)\n    # ------------------ Call Layers ------------------\n\n    sinogram = FanProjectionFor2D().forward(phantom, geometry)\n\n    #TODO: Add Cosine weighting\n\n    redundancy_weights = weights.parker_weights_2d(geometry)\n    sinogram_redun_weighted = sinogram * redundancy_weights\n    reco_filter = filters.ram_lak_2D(detector_shape, detector_spacing, number_of_projections)\n    x = fft_and_ifft(sinogram, reco_filter)\n\n    reco = FanBackProjectionFor2D().forward(x, geometry)\n\n    plt.figure()\n    plt.imshow(np.squeeze(reco), cmap=plt.get_cmap('gist_gray'))\n    plt.axis('off')\n    plt.savefig(f'2d_fan_reco.png', dpi=150, transparent=False, bbox_inches='tight')\n\n\nif __name__ == '__main__':\n    example_fan_2d()\n</code></pre>"},{"location":"examples/#parallel-2d-reconstruction","title":"Parallel 2D Reconstruction","text":"example_parallel_2d.py<pre><code># Copyright [2019] [Christopher Syben, Markus Michen]\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom pyronn.ct_reconstruction.layers.projection_2d import ParallelProjectionFor2D\nfrom pyronn.ct_reconstruction.layers.backprojection_2d import ParallelBackProjectionFor2D\nfrom pyronn.ct_reconstruction.geometry.geometry_base import GeometryParallel2D\nfrom pyronn.ct_reconstruction.helpers.filters import filters\nfrom pyronn.ct_reconstruction.helpers.phantoms import shepp_logan\nfrom pyronn.ct_reconstruction.helpers.misc.general_utils import fft_and_ifft\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d\nimport matplotlib.pyplot as plt\n\ndef example_parallel_2d():\n    # ------------------ Declare Parameters ------------------\n\n    # Volume Parameters:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    volume_size = 256\n    volume_shape = [volume_size, volume_size]\n    volume_spacing = [1, 1]\n\n    # Detector Parameters:\n    detector_shape = [800]\n    detector_spacing = [1]\n\n    # Trajectory Parameters:\n    number_of_projections = 360\n    angular_range = 2* np.pi\n\n    # create Geometry class\n    geometry = GeometryParallel2D(volume_shape, volume_spacing, detector_shape, detector_spacing, number_of_projections, angular_range)\n    geometry.set_trajectory(circular_trajectory_2d(geometry.number_of_projections, geometry.angular_range, True))\n\n    # Get Phantom\n    phantom = shepp_logan.shepp_logan_enhanced(volume_shape)\n    # Add required batch dimension\n    phantom = np.expand_dims(phantom, axis=0)\n\n    # ------------------ Call Layers ------------------\n    sinogram = ParallelProjectionFor2D().forward(phantom, geometry)\n\n    #sinogram = sinogram + np.random.normal(\n    #    loc=np.mean(np.abs(sinogram)), scale=np.std(sinogram), size=sinogram.shape) * 0.02\n\n    reco_filter = filters.shepp_logan_2D(geometry.detector_shape, geometry.detector_spacing, geometry.number_of_projections)\n\n    # # one for all\n    # x = fft_and_ifft(sinogram, reco_filter)\n\n    # You can also do it step by step\n    import torch\n    x = torch.fft.fft(torch.tensor(sinogram).cuda(), dim=-1, norm='ortho')\n    x = torch.multiply(x, torch.tensor(reco_filter).cuda())\n    x = torch.fft.ifft(x, dim=-1, norm='ortho').real\n\n    reco = ParallelBackProjectionFor2D().forward(x, geometry)\n\n    plt.figure()\n    plt.imshow(np.squeeze(reco), cmap=plt.get_cmap('gist_gray'))\n    plt.axis('off')\n    plt.savefig(f'2d_par_reco.png', dpi=150, transparent=False, bbox_inches='tight')\n\n\nif __name__ == '__main__':\n    example_parallel_2d()\n</code></pre>"},{"location":"installation/","title":"Installation and Setup \ud83d\udee0\ufe0f","text":""},{"location":"installation/#installation","title":"Installation \ud83d\udce5","text":"<p>Pyro-NN works with PyTorch and TensorFlow. This guide focuses on PyTorch.  </p>"},{"location":"installation/#requirements","title":"Requirements:","text":"<ul> <li><code>Microsoft Visual Studio (for self-build) \ud83d\udcbb</code></li> <li><code>Microsoft Visual C++ 14.0+</code></li> <li><code>Python package</code>build<code>\ud83d\udc0d</code></li> <li><code>CUDA &gt;10.2 \ud83d\ude80</code></li> </ul> <p>Once all of these requirements are fulfilled, follow these steps::</p> <ul> <li> <p>Clone the repo and switch to the <code>torch+tf</code> branch if not done yet: <pre><code>git checkout torch+tf\n</code></pre></p> </li> <li> <p>Run the command:  <pre><code>python -m build . \n</code></pre></p> </li> <li> <p>Switch to the newly created sub-directory <code>dist</code> <pre><code>cd dist/ \n</code></pre></p> </li> <li> <p>Build the wheel file using the following command:  <pre><code>pip install pyronn-(*version_number*).whl\n</code></pre></p> </li> </ul> <p>Note</p> <pre><code>    If necessary, you can modify the `pyproject.toml` file to specify a \n    particular torch version. However, be cautious and only make changes \n    if you are confident in what you are doing!\n</code></pre>"},{"location":"iterative_reconstruction_parallel/","title":"Iterative Reconstruction","text":"In\u00a0[2]: Copied! <pre>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.optim as optim\nimport h5py\n\nfrom pyronn.ct_reconstruction.geometry.geometry import Geometry\nfrom pyronn.ct_reconstruction.helpers.filters import filters\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d\nfrom pyronn.ct_reconstruction.layers.projection_2d import ParallelProjection2D\nfrom pyronn.ct_reconstruction.layers.backprojection_2d import ParallelBackProjection2D\n\n# Initializing CT geometry parameters:\n\n# Volume parameters:\nvolume_size = 362  # size of the volume/image\nvolume_shape = [volume_size, volume_size]  # shape of the volume as [height, width]\nvolume_spacing = [1, 1]  # spacing between pixels in the volume\n\n# Detector parameters:\ndetector_shape = [513]  # shape of the detector\ndetector_spacing = [1]  # spacing between detector pixels\n\n# Trajectory parameters:\nnumber_of_projections = 1000  # number of projections in the sinogram\nangular_range = -np.pi  # angular range of the trajectory (half-circle in this case)\n\n# Create an instance of the Geometry class and initialize it with the above parameters\ngeometry = Geometry()\ngeometry.init_from_parameters(volume_shape=volume_shape, volume_spacing=volume_spacing,\n                              detector_shape=detector_shape, detector_spacing=detector_spacing,\n                              number_of_projections=number_of_projections, angular_range=angular_range,\n                              trajectory=circular_trajectory_2d)\n</pre> import torch import numpy as np import matplotlib.pyplot as plt import torch.nn as nn import torch.optim as optim import h5py  from pyronn.ct_reconstruction.geometry.geometry import Geometry from pyronn.ct_reconstruction.helpers.filters import filters from pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d from pyronn.ct_reconstruction.layers.projection_2d import ParallelProjection2D from pyronn.ct_reconstruction.layers.backprojection_2d import ParallelBackProjection2D  # Initializing CT geometry parameters:  # Volume parameters: volume_size = 362  # size of the volume/image volume_shape = [volume_size, volume_size]  # shape of the volume as [height, width] volume_spacing = [1, 1]  # spacing between pixels in the volume  # Detector parameters: detector_shape = [513]  # shape of the detector detector_spacing = [1]  # spacing between detector pixels  # Trajectory parameters: number_of_projections = 1000  # number of projections in the sinogram angular_range = -np.pi  # angular range of the trajectory (half-circle in this case)  # Create an instance of the Geometry class and initialize it with the above parameters geometry = Geometry() geometry.init_from_parameters(volume_shape=volume_shape, volume_spacing=volume_spacing,                               detector_shape=detector_shape, detector_spacing=detector_spacing,                               number_of_projections=number_of_projections, angular_range=angular_range,                               trajectory=circular_trajectory_2d)  In\u00a0[3]: Copied! <pre>def load_data_from_hdf5(filename, dataset_name=\"data\"):\n    with h5py.File(filename, 'r') as f:\n        # Assuming the dataset is 3D and you want the first image\n        data = f[dataset_name][-1, :, :]\n    return data\n\n\n# Loading the ground truth image from a file:\nphantom = load_data_from_hdf5(\n    \"C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_test\\ground_truth_test_000.hdf5\")\nphantom = torch.tensor(np.expand_dims(phantom, axis=0).copy(), dtype=torch.float32).cuda()\nprint(phantom.shape)\nsinogram = load_data_from_hdf5(\n    \"C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_test\\observation_test_000.hdf5\")\nsinogram = torch.tensor(np.expand_dims(sinogram, axis=0).copy(), dtype=torch.float32).cuda()\nprint(sinogram.shape)\n</pre> def load_data_from_hdf5(filename, dataset_name=\"data\"):     with h5py.File(filename, 'r') as f:         # Assuming the dataset is 3D and you want the first image         data = f[dataset_name][-1, :, :]     return data   # Loading the ground truth image from a file: phantom = load_data_from_hdf5(     \"C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_test\\ground_truth_test_000.hdf5\") phantom = torch.tensor(np.expand_dims(phantom, axis=0).copy(), dtype=torch.float32).cuda() print(phantom.shape) sinogram = load_data_from_hdf5(     \"C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_test\\observation_test_000.hdf5\") sinogram = torch.tensor(np.expand_dims(sinogram, axis=0).copy(), dtype=torch.float32).cuda() print(sinogram.shape) <pre>torch.Size([1, 362, 362])\ntorch.Size([1, 1000, 513])\n</pre> In\u00a0[4]: Copied! <pre>class IterativeRecoModel(nn.Module):\n    def __init__(self, geo: Geometry):\n        super().__init__()\n        self.geometry = geo\n        self.reco = nn.Parameter(torch.zeros(geo.volume_shape).cuda())\n        self.A = ParallelProjection2D()\n\n    def forward(self, x):\n        updated_reco = x + self.reco\n        current_sino = self.A(updated_reco, **self.geometry)\n        return current_sino, updated_reco\n</pre> class IterativeRecoModel(nn.Module):     def __init__(self, geo: Geometry):         super().__init__()         self.geometry = geo         self.reco = nn.Parameter(torch.zeros(geo.volume_shape).cuda())         self.A = ParallelProjection2D()      def forward(self, x):         updated_reco = x + self.reco         current_sino = self.A(updated_reco, **self.geometry)         return current_sino, updated_reco  In\u00a0[5]: Copied! <pre>class Pipeline:\n    def __init__(self, ir, geo, epoches=100):\n        self.ir = ir\n        self.epoches = epoches\n        self.geometry = geo\n        self.model = IterativeRecoModel(geo)\n        self.regularizer_weight = 2e-3\n        self.optimizer = optim.Adam(list(self.model.parameters()), lr=self.ir)\n\n    def loss(self, prediction, label, regular=True):\n        def compute_tv_loss(prediction):\n            # Calculate the difference in the horizontal direction (left-right)\n            diff_horz = torch.sum(torch.abs(prediction[:, :, :-1] - prediction[:, :, 1:]))\n            # Calculate the difference in the vertical direction (up-down)\n            diff_vert = torch.sum(torch.abs(prediction[:, :-1, :] - prediction[:, 1:, :]))\n            # Combine the two differences\n            tv_loss = (\n                              diff_horz + diff_vert) / prediction.numel()  # Normalize by number of elements in the prediction tensor\n            return tv_loss\n\n        mse_loss = nn.MSELoss()(prediction, label)\n        tv_loss = 0\n        if regular:\n            tv_loss = compute_tv_loss(prediction)\n        return mse_loss + self.regularizer_weight * tv_loss\n\n    def train(self, input, label, regular: bool):\n        loss_values = []\n        for epoch in range(self.epoches):\n            self.optimizer.zero_grad()\n            predictions, current_reco = self.model(input)\n            loss_value = self.loss(predictions, label, regular)\n            loss_value.backward()\n            self.optimizer.step()\n            loss_values.append(loss_value.item())\n        return loss_values\n</pre> class Pipeline:     def __init__(self, ir, geo, epoches=100):         self.ir = ir         self.epoches = epoches         self.geometry = geo         self.model = IterativeRecoModel(geo)         self.regularizer_weight = 2e-3         self.optimizer = optim.Adam(list(self.model.parameters()), lr=self.ir)      def loss(self, prediction, label, regular=True):         def compute_tv_loss(prediction):             # Calculate the difference in the horizontal direction (left-right)             diff_horz = torch.sum(torch.abs(prediction[:, :, :-1] - prediction[:, :, 1:]))             # Calculate the difference in the vertical direction (up-down)             diff_vert = torch.sum(torch.abs(prediction[:, :-1, :] - prediction[:, 1:, :]))             # Combine the two differences             tv_loss = (                               diff_horz + diff_vert) / prediction.numel()  # Normalize by number of elements in the prediction tensor             return tv_loss          mse_loss = nn.MSELoss()(prediction, label)         tv_loss = 0         if regular:             tv_loss = compute_tv_loss(prediction)         return mse_loss + self.regularizer_weight * tv_loss      def train(self, input, label, regular: bool):         loss_values = []         for epoch in range(self.epoches):             self.optimizer.zero_grad()             predictions, current_reco = self.model(input)             loss_value = self.loss(predictions, label, regular)             loss_value.backward()             self.optimizer.step()             loss_values.append(loss_value.item())         return loss_values   In\u00a0[6]: Copied! <pre>pipeline_instance = Pipeline(ir=1e-3, geo=geometry, epoches=100)\n</pre> pipeline_instance = Pipeline(ir=1e-3, geo=geometry, epoches=100) In\u00a0[7]: Copied! <pre>ini_guess = torch.zeros_like(phantom)\nloss_value = pipeline_instance.train(ini_guess, sinogram, True)\nplt.figure()\nplt.plot(loss_value)\n</pre> ini_guess = torch.zeros_like(phantom) loss_value = pipeline_instance.train(ini_guess, sinogram, True) plt.figure() plt.plot(loss_value) Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x20c8121a110&gt;]</pre> In\u00a0[8]: Copied! <pre># def normalize_image(image: np.ndarray) -&gt; np.ndarray:\n#     \"\"\"Normalize the pixel values of an image to the range [0, 1].\"\"\"\n#     return image / np.mean(image)\n\ndef normalize_image(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize the pixel values of an image to the range [0, 1] while preserving contrast.\"\"\"\n    min_val = np.min(image)\n    max_val = np.max(image)\n\n    return (image - min_val) / (max_val - min_val)\n</pre> # def normalize_image(image: np.ndarray) -&gt; np.ndarray: #     \"\"\"Normalize the pixel values of an image to the range [0, 1].\"\"\" #     return image / np.mean(image)  def normalize_image(image: np.ndarray) -&gt; np.ndarray:     \"\"\"Normalize the pixel values of an image to the range [0, 1] while preserving contrast.\"\"\"     min_val = np.min(image)     max_val = np.max(image)      return (image - min_val) / (max_val - min_val) In\u00a0[14]: Copied! <pre>reco = pipeline_instance.model(ini_guess)[1].squeeze().cpu().detach().numpy()\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(normalize_image(reco), cmap=\"gray\")\nplt.title('IR')\n\nplt.subplot(1, 2, 2)\nplt.imshow((normalize_image(phantom.squeeze().cpu().numpy())), cmap=\"gray\")\nplt.title('Label')\nplt.show()\n</pre> reco = pipeline_instance.model(ini_guess)[1].squeeze().cpu().detach().numpy()  plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.imshow(normalize_image(reco), cmap=\"gray\") plt.title('IR')  plt.subplot(1, 2, 2) plt.imshow((normalize_image(phantom.squeeze().cpu().numpy())), cmap=\"gray\") plt.title('Label') plt.show() In\u00a0[13]: Copied! <pre>difference_ir = normalize_image(reco) - normalize_image(phantom.squeeze().cpu().numpy())\nplt.imshow(difference_ir, cmap='gray')\nplt.title('Difference (IR vs. Phantom)')\nplt.tight_layout()\nplt.show()\n\n# Calculate the MSE for each reconstruction method.\nmse_ir = np.mean(difference_ir ** 2)\n\nprint(f\"Mean Squared Error (MSE) between IR and Phantom: {mse_ir:.4f}\")\n</pre> difference_ir = normalize_image(reco) - normalize_image(phantom.squeeze().cpu().numpy()) plt.imshow(difference_ir, cmap='gray') plt.title('Difference (IR vs. Phantom)') plt.tight_layout() plt.show()  # Calculate the MSE for each reconstruction method. mse_ir = np.mean(difference_ir ** 2)  print(f\"Mean Squared Error (MSE) between IR and Phantom: {mse_ir:.4f}\") <pre>Mean Squared Error (MSE) between IR and Phantom: 0.0061\n</pre>"},{"location":"noisy_data_gen_2/","title":"Noisy data gen 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport h5py\nimport numpy as np\nimport torch\nfrom pyronn.ct_reconstruction.geometry.geometry import Geometry\nfrom pyronn.ct_reconstruction.helpers.filters import weights, filters\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import (\n    circular_trajectory_3d,\n)\nfrom torch.nn import functional as F\nfrom pyronn.ct_reconstruction.layers.projection_3d import ConeProjection3D\nfrom pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjection3D\nfrom scipy.ndimage import convolve, gaussian_filter, rotate\nfrom scipy.signal import convolve2d\nfrom scipy.signal import fftconvolve\n</pre> import os import h5py import numpy as np import torch from pyronn.ct_reconstruction.geometry.geometry import Geometry from pyronn.ct_reconstruction.helpers.filters import weights, filters from pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import (     circular_trajectory_3d, ) from torch.nn import functional as F from pyronn.ct_reconstruction.layers.projection_3d import ConeProjection3D from pyronn.ct_reconstruction.layers.backprojection_3d import ConeBackProjection3D from scipy.ndimage import convolve, gaussian_filter, rotate from scipy.signal import convolve2d from scipy.signal import fftconvolve In\u00a0[\u00a0]: Copied! <pre>from update.beam_harden import simulate_polychromatic_spectrum, apply_beam_hardening4D\n</pre> from update.beam_harden import simulate_polychromatic_spectrum, apply_beam_hardening4D In\u00a0[\u00a0]: Copied! <pre>class SinogramNoiseSimulator:\n    def __init__(self, sinogram):\n        \"\"\"\n        Initialize the SinogramNoiseSimulator with a sinogram.\n\n        :param sinogram: Original sinogram data of shape (1, num_projections, height, width)\n        \"\"\"\n        self.sinogram = sinogram\n\n    def add_detector_jitter(self, max_jitter):\n        \"\"\"\n        Simulate detector jitter in the sinogram.\n\n        :param max_jitter: Maximum amount of jitter (in pixels)\n        \"\"\"\n        num_projections = self.sinogram.shape[1]\n        jittered_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            jitter = np.random.randint(-max_jitter, max_jitter)\n            jittered_projection = np.roll(self.sinogram[0, i, :, :], jitter, axis=1)\n            jittered_projection = np.roll(jittered_projection, jitter, axis=0)\n\n            jittered_sinogram[0, i, :, :] = jittered_projection\n\n        self.sinogram = jittered_sinogram\n\n    def add_gantry_motion_blur(self, blur_length, angle_range):\n        \"\"\"\n        Apply gantry motion blur to the sinogram.\n\n        :param blur_length: Length of the blur kernel\n        :param angle_range: Range of angles (in degrees) for the gantry rotation during acquisition\n        \"\"\"\n        num_projections, height, width = self.sinogram.shape[1:]\n        \n        # Create a curved blur kernel\n        kernel_size = blur_length * 2 + 1\n        y, x = np.ogrid[-blur_length:blur_length+1, -blur_length:blur_length+1]\n        mask = x**2 + y**2 &lt;= blur_length**2\n        kernel = np.zeros((kernel_size, kernel_size))\n        kernel[mask] = 1\n        kernel /= kernel.sum()  # Normalize the kernel\n\n        blurred_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            projection = self.sinogram[0, i, :, :]\n            \n            # Calculate the rotation angle for this projection\n            rotation_angle = (i / num_projections) * angle_range\n            \n            # Rotate the kernel\n            rotated_kernel = rotate(kernel, rotation_angle, reshape=False)\n            \n            # Apply the rotated kernel\n            blurred_projection = convolve2d(projection, rotated_kernel, mode='same', boundary='wrap')\n            \n            blurred_sinogram[0, i, :, :] = blurred_projection\n\n        self.sinogram = blurred_sinogram\n\n    def add_high_frequency_noise(self, noise_level, high_freq_strength):\n        \"\"\"\n        Add high-frequency noise to the sinogram.\n\n        :param noise_level: Standard deviation of the Gaussian noise\n        :param high_freq_strength: Strength of the high-frequency component\n        \"\"\"\n        num_projections = self.sinogram.shape[1]\n        noisy_sinogram = np.zeros_like(self.sinogram)\n\n        for i in range(num_projections):\n            noise = np.random.normal(0, noise_level, size=self.sinogram.shape[2:])\n            low_freq_noise = gaussian_filter(noise, sigma=high_freq_strength)\n            high_freq_noise = noise - low_freq_noise\n\n            noisy_sinogram[0, i, :, :] = self.sinogram[0, i, :, :] + high_freq_noise\n\n        self.sinogram = noisy_sinogram\n\n    \n    def add_poisson_noise(self, scale_factor):\n        \"\"\"\n        Add Poisson noise to the sinogram.\n\n        :param scale_factor: Scale factor to control the intensity of the noise\n        \"\"\"\n        noisy_sinogram = np.zeros_like(self.sinogram)\n        for i in range(self.sinogram.shape[1]):\n            noisy_sinogram[0, i, :, :] = np.random.poisson(self.sinogram[0, i, :, :] * scale_factor) / scale_factor\n        self.sinogram = noisy_sinogram\n    \n    def add_aliasing_artifacts(self, undersample_factor):\n        num_projections = self.sinogram.shape[1]\n        self.sinogram = self.sinogram[:, ::undersample_factor, :, :]\n\n    def add_metal_artifacts(self, metal_positions, metal_intensity):\n        \"\"\"\n        Add metal artifacts to the sinogram.\n\n        :param sinogram: The original sinogram data\n        :param metal_positions: List of tuples indicating the positions of metal objects in the sinogram\n        :param metal_intensity: The intensity of the metal artifact\n        :return: The sinogram with metal artifacts\n        \"\"\"\n        for pos in metal_positions:\n            self.sinogram[:, :, pos[0], pos[1]] += metal_intensity\n\n    def add_gaussian_noise(self, mean, std):\n        \"\"\"\n        Add Gaussian noise to the sinogram.\n\n        :param mean: Mean of the Gaussian noise\n        :param std: Standard deviation of the Gaussian noise\n        \"\"\"\n        gaussian_noise = np.random.normal(mean, std, self.sinogram.shape)\n        self.sinogram += gaussian_noise\n        self.sinogram = self.sinogram.float()\n\n    def add_ring_artifacts(self, num_rings):\n        \"\"\"\n        Add ring artifacts to the sinogram.\n\n        :param num_rings: Number of rings to add\n        \"\"\"\n\n        num_projections = self.sinogram.shape[1]\n        height = self.sinogram.shape[2]\n        width = self.sinogram.shape[3]\n\n        for _ in range(num_rings):\n            # Randomly select detector index\n            broken_detector_index = np.random.randint(0, width)\n\n            # Randomly select the range of heights for the artifact\n            start_height = np.random.randint(0, height - 1)\n            end_height = np.random.randint(start_height + 1, height)\n\n            # Add the artifact to the specified region of the sinogram\n            self.sinogram[:, :, start_height:end_height, broken_detector_index] = 0\n\n    def add_scatter_noise(self, scatter_fraction, energy_MeV=0.14):\n        \"\"\"\n        Add realistic scatter noise to the CT sinogram.\n\n        :param scatter_fraction: Fraction of total intensity that becomes scatter (0-1)\n        :param energy_MeV: X-ray energy in MeV (affects scatter distribution)\n        \"\"\"\n        # Calculate total intensity\n        total_intensity = self.sinogram.sum()\n\n        # Calculate scatter intensity\n        scatter_intensity = scatter_fraction * total_intensity\n\n        # Generate object-dependent scatter distribution\n        scatter_base = self._generate_object_dependent_scatter()\n\n        # Apply energy-dependent scatter kernel\n        scatter_noise = self._apply_scatter_kernel(scatter_base, energy_MeV)\n\n        # Normalize scatter noise to desired intensity\n        scatter_noise = scatter_noise / scatter_noise.sum() * scatter_intensity\n\n        # Add scatter to original sinogram\n        self.sinogram += scatter_noise\n\n    def _generate_object_dependent_scatter(self):\n        \"\"\"Generate initial scatter distribution based on object density.\"\"\"\n        # Use softmax to create a probability distribution based on object density\n        scatter_prob = torch.softmax(self.sinogram.flatten(), dim=0).reshape(self.sinogram.shape)\n        \n        # Generate initial scatter based on this probability\n        scatter_base = torch.poisson(scatter_prob * 1000)  # Multiplier for more pronounced effect\n        return scatter_base\n\n    def _apply_scatter_kernel(self, scatter_base, energy_MeV):\n        \"\"\"Apply an energy-dependent scatter kernel.\"\"\"\n        # Convert to NumPy for convolution\n        scatter_np = scatter_base.numpy()\n\n        # Create an anisotropic kernel favoring forward scatter\n        kernel = self._create_anisotropic_kernel(scatter_np.shape, energy_MeV)\n\n        # Apply the kernel using FFT convolution\n        scatter_noise = fftconvolve(scatter_np, kernel, mode='same')\n\n        return torch.tensor(scatter_noise, dtype=torch.float32)\n\n    def _create_anisotropic_kernel(self, shape, energy_MeV):\n        \"\"\"Create an anisotropic scatter kernel based on energy and sinogram shape.\"\"\"\n        # Ensure we're working with the last two dimensions for 2D convolution\n        if len(shape) &gt; 2:\n            rows, cols = shape[-2], shape[-1]\n        else:\n            rows, cols = shape\n\n        y, x = np.ogrid[-rows//2:rows//2, -cols//2:cols//2]\n        \n        # Adjust these parameters based on your specific CT geometry and energy range\n        forward_sigma = cols / 8 * (energy_MeV / 0.1)**0.5  # Increases with energy\n        lateral_sigma = rows / 16\n\n        # Create an elliptical Gaussian kernel\n        kernel = np.exp(-(x**2 / (2 * forward_sigma**2) + y**2 / (2 * lateral_sigma**2)))\n\n        # Normalize the kernel\n        kernel /= kernel.sum()\n        \n        # If the sinogram has more than 2 dimensions, expand the kernel\n        if len(shape) &gt; 2:\n            for _ in range(len(shape) - 2):\n                kernel = np.expand_dims(kernel, axis=0)\n        \n        return kernel\n    def add_beam_hardening(self, material):\n        print(\"adding beam hardening\")\n        energy_bins = np.linspace(1, 120, 100)\n\n        # Simulate polychromatic spectrum\n        spectrum = simulate_polychromatic_spectrum(energy_bins)\n\n        # Apply beam hardening\n        hardened_projections = apply_beam_hardening4D(self.sinogram[0], energy_bins, spectrum, material)\n\n        hardened_projections = torch.tensor(\n            np.expand_dims(hardened_projections, axis=0).copy(), dtype=torch.float32\n        )\n        self.sinogram = hardened_projections\n</pre> class SinogramNoiseSimulator:     def __init__(self, sinogram):         \"\"\"         Initialize the SinogramNoiseSimulator with a sinogram.          :param sinogram: Original sinogram data of shape (1, num_projections, height, width)         \"\"\"         self.sinogram = sinogram      def add_detector_jitter(self, max_jitter):         \"\"\"         Simulate detector jitter in the sinogram.          :param max_jitter: Maximum amount of jitter (in pixels)         \"\"\"         num_projections = self.sinogram.shape[1]         jittered_sinogram = np.zeros_like(self.sinogram)          for i in range(num_projections):             jitter = np.random.randint(-max_jitter, max_jitter)             jittered_projection = np.roll(self.sinogram[0, i, :, :], jitter, axis=1)             jittered_projection = np.roll(jittered_projection, jitter, axis=0)              jittered_sinogram[0, i, :, :] = jittered_projection          self.sinogram = jittered_sinogram      def add_gantry_motion_blur(self, blur_length, angle_range):         \"\"\"         Apply gantry motion blur to the sinogram.          :param blur_length: Length of the blur kernel         :param angle_range: Range of angles (in degrees) for the gantry rotation during acquisition         \"\"\"         num_projections, height, width = self.sinogram.shape[1:]                  # Create a curved blur kernel         kernel_size = blur_length * 2 + 1         y, x = np.ogrid[-blur_length:blur_length+1, -blur_length:blur_length+1]         mask = x**2 + y**2 &lt;= blur_length**2         kernel = np.zeros((kernel_size, kernel_size))         kernel[mask] = 1         kernel /= kernel.sum()  # Normalize the kernel          blurred_sinogram = np.zeros_like(self.sinogram)          for i in range(num_projections):             projection = self.sinogram[0, i, :, :]                          # Calculate the rotation angle for this projection             rotation_angle = (i / num_projections) * angle_range                          # Rotate the kernel             rotated_kernel = rotate(kernel, rotation_angle, reshape=False)                          # Apply the rotated kernel             blurred_projection = convolve2d(projection, rotated_kernel, mode='same', boundary='wrap')                          blurred_sinogram[0, i, :, :] = blurred_projection          self.sinogram = blurred_sinogram      def add_high_frequency_noise(self, noise_level, high_freq_strength):         \"\"\"         Add high-frequency noise to the sinogram.          :param noise_level: Standard deviation of the Gaussian noise         :param high_freq_strength: Strength of the high-frequency component         \"\"\"         num_projections = self.sinogram.shape[1]         noisy_sinogram = np.zeros_like(self.sinogram)          for i in range(num_projections):             noise = np.random.normal(0, noise_level, size=self.sinogram.shape[2:])             low_freq_noise = gaussian_filter(noise, sigma=high_freq_strength)             high_freq_noise = noise - low_freq_noise              noisy_sinogram[0, i, :, :] = self.sinogram[0, i, :, :] + high_freq_noise          self.sinogram = noisy_sinogram           def add_poisson_noise(self, scale_factor):         \"\"\"         Add Poisson noise to the sinogram.          :param scale_factor: Scale factor to control the intensity of the noise         \"\"\"         noisy_sinogram = np.zeros_like(self.sinogram)         for i in range(self.sinogram.shape[1]):             noisy_sinogram[0, i, :, :] = np.random.poisson(self.sinogram[0, i, :, :] * scale_factor) / scale_factor         self.sinogram = noisy_sinogram          def add_aliasing_artifacts(self, undersample_factor):         num_projections = self.sinogram.shape[1]         self.sinogram = self.sinogram[:, ::undersample_factor, :, :]      def add_metal_artifacts(self, metal_positions, metal_intensity):         \"\"\"         Add metal artifacts to the sinogram.          :param sinogram: The original sinogram data         :param metal_positions: List of tuples indicating the positions of metal objects in the sinogram         :param metal_intensity: The intensity of the metal artifact         :return: The sinogram with metal artifacts         \"\"\"         for pos in metal_positions:             self.sinogram[:, :, pos[0], pos[1]] += metal_intensity      def add_gaussian_noise(self, mean, std):         \"\"\"         Add Gaussian noise to the sinogram.          :param mean: Mean of the Gaussian noise         :param std: Standard deviation of the Gaussian noise         \"\"\"         gaussian_noise = np.random.normal(mean, std, self.sinogram.shape)         self.sinogram += gaussian_noise         self.sinogram = self.sinogram.float()      def add_ring_artifacts(self, num_rings):         \"\"\"         Add ring artifacts to the sinogram.          :param num_rings: Number of rings to add         \"\"\"          num_projections = self.sinogram.shape[1]         height = self.sinogram.shape[2]         width = self.sinogram.shape[3]          for _ in range(num_rings):             # Randomly select detector index             broken_detector_index = np.random.randint(0, width)              # Randomly select the range of heights for the artifact             start_height = np.random.randint(0, height - 1)             end_height = np.random.randint(start_height + 1, height)              # Add the artifact to the specified region of the sinogram             self.sinogram[:, :, start_height:end_height, broken_detector_index] = 0      def add_scatter_noise(self, scatter_fraction, energy_MeV=0.14):         \"\"\"         Add realistic scatter noise to the CT sinogram.          :param scatter_fraction: Fraction of total intensity that becomes scatter (0-1)         :param energy_MeV: X-ray energy in MeV (affects scatter distribution)         \"\"\"         # Calculate total intensity         total_intensity = self.sinogram.sum()          # Calculate scatter intensity         scatter_intensity = scatter_fraction * total_intensity          # Generate object-dependent scatter distribution         scatter_base = self._generate_object_dependent_scatter()          # Apply energy-dependent scatter kernel         scatter_noise = self._apply_scatter_kernel(scatter_base, energy_MeV)          # Normalize scatter noise to desired intensity         scatter_noise = scatter_noise / scatter_noise.sum() * scatter_intensity          # Add scatter to original sinogram         self.sinogram += scatter_noise      def _generate_object_dependent_scatter(self):         \"\"\"Generate initial scatter distribution based on object density.\"\"\"         # Use softmax to create a probability distribution based on object density         scatter_prob = torch.softmax(self.sinogram.flatten(), dim=0).reshape(self.sinogram.shape)                  # Generate initial scatter based on this probability         scatter_base = torch.poisson(scatter_prob * 1000)  # Multiplier for more pronounced effect         return scatter_base      def _apply_scatter_kernel(self, scatter_base, energy_MeV):         \"\"\"Apply an energy-dependent scatter kernel.\"\"\"         # Convert to NumPy for convolution         scatter_np = scatter_base.numpy()          # Create an anisotropic kernel favoring forward scatter         kernel = self._create_anisotropic_kernel(scatter_np.shape, energy_MeV)          # Apply the kernel using FFT convolution         scatter_noise = fftconvolve(scatter_np, kernel, mode='same')          return torch.tensor(scatter_noise, dtype=torch.float32)      def _create_anisotropic_kernel(self, shape, energy_MeV):         \"\"\"Create an anisotropic scatter kernel based on energy and sinogram shape.\"\"\"         # Ensure we're working with the last two dimensions for 2D convolution         if len(shape) &gt; 2:             rows, cols = shape[-2], shape[-1]         else:             rows, cols = shape          y, x = np.ogrid[-rows//2:rows//2, -cols//2:cols//2]                  # Adjust these parameters based on your specific CT geometry and energy range         forward_sigma = cols / 8 * (energy_MeV / 0.1)**0.5  # Increases with energy         lateral_sigma = rows / 16          # Create an elliptical Gaussian kernel         kernel = np.exp(-(x**2 / (2 * forward_sigma**2) + y**2 / (2 * lateral_sigma**2)))          # Normalize the kernel         kernel /= kernel.sum()                  # If the sinogram has more than 2 dimensions, expand the kernel         if len(shape) &gt; 2:             for _ in range(len(shape) - 2):                 kernel = np.expand_dims(kernel, axis=0)                  return kernel     def add_beam_hardening(self, material):         print(\"adding beam hardening\")         energy_bins = np.linspace(1, 120, 100)          # Simulate polychromatic spectrum         spectrum = simulate_polychromatic_spectrum(energy_bins)          # Apply beam hardening         hardened_projections = apply_beam_hardening4D(self.sinogram[0], energy_bins, spectrum, material)          hardened_projections = torch.tensor(             np.expand_dims(hardened_projections, axis=0).copy(), dtype=torch.float32         )         self.sinogram = hardened_projections In\u00a0[\u00a0]: Copied! <pre>def apply_noise(noise_simulator, noise_type):\n    if noise_type == \"add_gantry_motion_blur\":\n        noise_simulator.add_gantry_motion_blur(blur_length=5, angle_range=0.5)\n    elif noise_type == \"add_poisson_noise\":\n        noise_simulator.add_poisson_noise(scale_factor=10)\n    elif noise_type == \"add_ring_artifacts\":\n        noise_simulator.add_ring_artifacts(num_rings=5)\n    elif noise_type == \"add_scatter_noise\":\n        noise_simulator.add_scatter_noise(0.5)\n    elif noise_type == \"add_beam_hardening\":\n        noise_simulator.add_beam_hardening(material=\"bone\")\n    elif noise_type == \"add_detector_jitter\":\n        noise_simulator.add_detector_jitter(max_jitter=5)\n    else:\n        raise ValueError(f\"Unknown noise type: {noise_type}\")\n</pre> def apply_noise(noise_simulator, noise_type):     if noise_type == \"add_gantry_motion_blur\":         noise_simulator.add_gantry_motion_blur(blur_length=5, angle_range=0.5)     elif noise_type == \"add_poisson_noise\":         noise_simulator.add_poisson_noise(scale_factor=10)     elif noise_type == \"add_ring_artifacts\":         noise_simulator.add_ring_artifacts(num_rings=5)     elif noise_type == \"add_scatter_noise\":         noise_simulator.add_scatter_noise(0.5)     elif noise_type == \"add_beam_hardening\":         noise_simulator.add_beam_hardening(material=\"bone\")     elif noise_type == \"add_detector_jitter\":         noise_simulator.add_detector_jitter(max_jitter=5)     else:         raise ValueError(f\"Unknown noise type: {noise_type}\") In\u00a0[\u00a0]: Copied! <pre>def save_results(results, input_file, output_path, noise_type, compression=\"gzip\"):\n    base_name = os.path.basename(input_file)\n    name_without_extension = os.path.splitext(base_name)[0]\n    \n    output_file = os.path.join(output_path, f\"{name_without_extension}_{noise_type}_processed.h5\")\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    \n    with h5py.File(output_file, \"w\") as h5f:\n        for i, (sinogram, noisy_sinogram, volume, noisy_volume) in enumerate(results):\n            h5f.create_dataset(f\"sinogram_{i:03d}\", data=sinogram.numpy(), compression=compression)\n            h5f.create_dataset(f\"noisy_sinogram_{i:03d}\", data=noisy_sinogram, compression=compression)\n            h5f.create_dataset(f\"volume_{i:03d}\", data=volume.numpy(), compression=compression)\n            h5f.create_dataset(f\"noisy_volume_{i:03d}\", data=noisy_volume.numpy(), compression=compression)\n    print(f\"Processed data for {noise_type} saved to {output_file}\")\n</pre> def save_results(results, input_file, output_path, noise_type, compression=\"gzip\"):     base_name = os.path.basename(input_file)     name_without_extension = os.path.splitext(base_name)[0]          output_file = os.path.join(output_path, f\"{name_without_extension}_{noise_type}_processed.h5\")     os.makedirs(os.path.dirname(output_file), exist_ok=True)          with h5py.File(output_file, \"w\") as h5f:         for i, (sinogram, noisy_sinogram, volume, noisy_volume) in enumerate(results):             h5f.create_dataset(f\"sinogram_{i:03d}\", data=sinogram.numpy(), compression=compression)             h5f.create_dataset(f\"noisy_sinogram_{i:03d}\", data=noisy_sinogram, compression=compression)             h5f.create_dataset(f\"volume_{i:03d}\", data=volume.numpy(), compression=compression)             h5f.create_dataset(f\"noisy_volume_{i:03d}\", data=noisy_volume.numpy(), compression=compression)     print(f\"Processed data for {noise_type} saved to {output_file}\") In\u00a0[\u00a0]: Copied! <pre>def process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types):\n    with h5py.File(input_file, 'r') as h5f:\n        num_volumes = sum(1 for key in h5f.keys() if key.startswith('array_'))\n        print(f\"Number of volumes in the dataset: {num_volumes}\")\n\n        if num_volumes == 0:\n            raise ValueError(\"No datasets found with 'array_' prefix in the input file.\")\n\n        # Read the first volume to get the shape\n        first_volume = h5f['array_000'][()]\n        volume_shape = first_volume.shape\n        print(f\"Volume shape: {volume_shape}\")\n\n        # Assuming cubic voxels, calculate volume spacing\n        volume_spacing = (1, 1, 1)  # You might want to adjust this if you have specific spacing information\n\n        # Initialize geometry\n        geometry = Geometry()\n        geometry.init_from_parameters(\n            volume_shape=volume_shape,\n            volume_spacing=volume_spacing,\n            detector_shape=detector_shape,\n            detector_spacing=detector_spacing,\n            number_of_projections=num_projections,\n            angular_range=angular_range,\n            trajectory=circular_trajectory_3d,\n            source_isocenter_distance=sid,\n            source_detector_distance=sdd,\n        )\n\n        reco_filter = torch.tensor(\n            filters.ram_lak_3D(\n                geometry.detector_shape,\n                geometry.detector_spacing,\n                geometry.number_of_projections,\n            ),\n            dtype=torch.float32,\n        ).cuda()\n\n        for noise_type in noise_types:\n            results = []\n            for index in range(num_volumes):\n                volume = h5f[f'array_{index:03d}'][()]\n                volume_tensor = torch.tensor(\n                    np.expand_dims(volume, axis=0), dtype=torch.float32\n                ).cuda()\n                sinogram = ConeProjection3D(hardware_interp=True).forward(\n                    volume_tensor, **geometry\n                )\n                sinogram_tensor = sinogram.detach().cpu()\n\n                noise_simulator = SinogramNoiseSimulator(sinogram_tensor)\n                \n                # Apply the current noise type\n                apply_noise(noise_simulator, noise_type)\n\n                noisy_sinogram = noise_simulator.sinogram\n                noisy_sinogram = noisy_sinogram * weights.cosine_weights_3d(geometry)\n\n                x = torch.fft.fft(torch.Tensor(noisy_sinogram).cuda(), dim=-1)\n                x = torch.multiply(x, reco_filter)\n                x = torch.fft.ifft(x, dim=-1).real\n\n                noisy_reco = ConeBackProjection3D(hardware_interp=True).forward(\n                    x.contiguous(), **geometry\n                )\n\n                noisy_reco = F.relu(noisy_reco)\n                noisy_reco = noisy_reco.cpu()\n\n                results.append((sinogram_tensor, noisy_sinogram, volume_tensor.cpu(), noisy_reco))\n                print(f\"Processed volume {index + 1}/{num_volumes} with {noise_type}\")\n\n            save_results(results, input_file, output_path, noise_type)\n</pre> def process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types):     with h5py.File(input_file, 'r') as h5f:         num_volumes = sum(1 for key in h5f.keys() if key.startswith('array_'))         print(f\"Number of volumes in the dataset: {num_volumes}\")          if num_volumes == 0:             raise ValueError(\"No datasets found with 'array_' prefix in the input file.\")          # Read the first volume to get the shape         first_volume = h5f['array_000'][()]         volume_shape = first_volume.shape         print(f\"Volume shape: {volume_shape}\")          # Assuming cubic voxels, calculate volume spacing         volume_spacing = (1, 1, 1)  # You might want to adjust this if you have specific spacing information          # Initialize geometry         geometry = Geometry()         geometry.init_from_parameters(             volume_shape=volume_shape,             volume_spacing=volume_spacing,             detector_shape=detector_shape,             detector_spacing=detector_spacing,             number_of_projections=num_projections,             angular_range=angular_range,             trajectory=circular_trajectory_3d,             source_isocenter_distance=sid,             source_detector_distance=sdd,         )          reco_filter = torch.tensor(             filters.ram_lak_3D(                 geometry.detector_shape,                 geometry.detector_spacing,                 geometry.number_of_projections,             ),             dtype=torch.float32,         ).cuda()          for noise_type in noise_types:             results = []             for index in range(num_volumes):                 volume = h5f[f'array_{index:03d}'][()]                 volume_tensor = torch.tensor(                     np.expand_dims(volume, axis=0), dtype=torch.float32                 ).cuda()                 sinogram = ConeProjection3D(hardware_interp=True).forward(                     volume_tensor, **geometry                 )                 sinogram_tensor = sinogram.detach().cpu()                  noise_simulator = SinogramNoiseSimulator(sinogram_tensor)                                  # Apply the current noise type                 apply_noise(noise_simulator, noise_type)                  noisy_sinogram = noise_simulator.sinogram                 noisy_sinogram = noisy_sinogram * weights.cosine_weights_3d(geometry)                  x = torch.fft.fft(torch.Tensor(noisy_sinogram).cuda(), dim=-1)                 x = torch.multiply(x, reco_filter)                 x = torch.fft.ifft(x, dim=-1).real                  noisy_reco = ConeBackProjection3D(hardware_interp=True).forward(                     x.contiguous(), **geometry                 )                  noisy_reco = F.relu(noisy_reco)                 noisy_reco = noisy_reco.cpu()                  results.append((sinogram_tensor, noisy_sinogram, volume_tensor.cpu(), noisy_reco))                 print(f\"Processed volume {index + 1}/{num_volumes} with {noise_type}\")              save_results(results, input_file, output_path, noise_type) In\u00a0[\u00a0]: Copied! <pre>def main():\n    # Define parameters\n    detector_row = 800\n    detector_col = 800\n    detector_spacer = 1\n    num_projections = 400\n    angular_range = 2 * np.pi\n    sdd = 3000  # Source-Detector distance\n    sid = 2400  # Source-Isocenter distance\n    output_path = r\"D:\\datasets\\stl2\\output_h5\\noisy_data\"\n    input_file = r\"D:\\datasets\\stl2\\output_h5\\batch_003.h5\"\n    # input_file = r\"C:\\Users\\sun\\OneDrive - Fraunhofer\\PhD\\known_operator\\2D-2-3D\\pancreas_ct_data.h5\"\n\n    os.makedirs(output_path, exist_ok=True)\n\n    detector_shape = (detector_row, detector_col)\n    detector_spacing = (detector_spacer, detector_spacer)\n\n    noise_types = [\n        \"add_gantry_motion_blur\",\n        # \"add_poisson_noise\",\n        # \"add_ring_artifacts\",\n        # # \"add_scatter_noise\",\n        # \"add_beam_hardening\",\n        \"add_detector_jitter\"\n    ]\n\n    process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types)\n</pre> def main():     # Define parameters     detector_row = 800     detector_col = 800     detector_spacer = 1     num_projections = 400     angular_range = 2 * np.pi     sdd = 3000  # Source-Detector distance     sid = 2400  # Source-Isocenter distance     output_path = r\"D:\\datasets\\stl2\\output_h5\\noisy_data\"     input_file = r\"D:\\datasets\\stl2\\output_h5\\batch_003.h5\"     # input_file = r\"C:\\Users\\sun\\OneDrive - Fraunhofer\\PhD\\known_operator\\2D-2-3D\\pancreas_ct_data.h5\"      os.makedirs(output_path, exist_ok=True)      detector_shape = (detector_row, detector_col)     detector_spacing = (detector_spacer, detector_spacer)      noise_types = [         \"add_gantry_motion_blur\",         # \"add_poisson_noise\",         # \"add_ring_artifacts\",         # # \"add_scatter_noise\",         # \"add_beam_hardening\",         \"add_detector_jitter\"     ]      process_dataset(input_file, output_path, detector_shape, detector_spacing, num_projections, angular_range, sdd, sid, noise_types) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"stl_2_numpy/","title":"Stl 2 numpy","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport h5py\nimport stltovoxel\nfrom PIL import Image\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport tempfile\nfrom pathlib import Path\nfrom typing import Iterator, Optional\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport itertools\nfrom datetime import datetime\n</pre> import os import numpy as np import h5py import stltovoxel from PIL import Image import multiprocessing from concurrent.futures import ProcessPoolExecutor, as_completed import tempfile from pathlib import Path from typing import Iterator, Optional import logging from logging.handlers import RotatingFileHandler import itertools from datetime import datetime In\u00a0[\u00a0]: Copied! <pre>def setup_logging():\n    logger = logging.getLogger()\n    \n    # \u5982\u679clogger\u5df2\u7ecf\u6709\u5904\u7406\u5668\uff0c\u8bf4\u660e\u5df2\u7ecf\u88ab\u8bbe\u7f6e\u8fc7\uff0c\u76f4\u63a5\u8fd4\u56de\n    if logger.handlers:\n        return logger\n    \n    logger.setLevel(logging.INFO)\n    \n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    \n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n    \n    return logger\n</pre> def setup_logging():     logger = logging.getLogger()          # \u5982\u679clogger\u5df2\u7ecf\u6709\u5904\u7406\u5668\uff0c\u8bf4\u660e\u5df2\u7ecf\u88ab\u8bbe\u7f6e\u8fc7\uff0c\u76f4\u63a5\u8fd4\u56de     if logger.handlers:         return logger          logger.setLevel(logging.INFO)          formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')          console_handler = logging.StreamHandler()     console_handler.setFormatter(formatter)     logger.addHandler(console_handler)          return logger In\u00a0[\u00a0]: Copied! <pre>logger = setup_logging()\n</pre> logger = setup_logging() In\u00a0[\u00a0]: Copied! <pre>def convert_stl_to_png(stl_path: str, resolution: int = 512, pad: int = 0) -&gt; str:\n    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n        png_path = tmp.name\n    stltovoxel.convert_file(stl_path, png_path, resolution=resolution, pad=pad, parallel=True)\n    return png_path\n</pre> def convert_stl_to_png(stl_path: str, resolution: int = 512, pad: int = 0) -&gt; str:     with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:         png_path = tmp.name     stltovoxel.convert_file(stl_path, png_path, resolution=resolution, pad=pad, parallel=True)     return png_path In\u00a0[\u00a0]: Copied! <pre>def load_png_to_numpy(png_path: str, start_num: int, end_num: int) -&gt; np.ndarray:\n    target_size = (512, 512)\n    images = []\n    \n    for i in range(start_num, end_num + 1):\n        filename = f\"{png_path[:-4]}_{i:03d}.png\"\n        if not os.path.exists(filename):\n            logger.warning(f\"File {filename} does not exist. Skipping.\")\n            continue\n        with Image.open(filename) as img:\n            resized_img = img.resize(target_size, Image.LANCZOS)\n            images.append(np.array(resized_img))\n    \n    images_array = np.array(images)\n    \n    if len(images) != 512:\n        padded_array = np.zeros((512, 512, 512), dtype=np.uint8)\n        padded_array[:len(images)] = images_array[:512]\n        return padded_array\n    \n    return images_array.astype(np.uint8)\n</pre> def load_png_to_numpy(png_path: str, start_num: int, end_num: int) -&gt; np.ndarray:     target_size = (512, 512)     images = []          for i in range(start_num, end_num + 1):         filename = f\"{png_path[:-4]}_{i:03d}.png\"         if not os.path.exists(filename):             logger.warning(f\"File {filename} does not exist. Skipping.\")             continue         with Image.open(filename) as img:             resized_img = img.resize(target_size, Image.LANCZOS)             images.append(np.array(resized_img))          images_array = np.array(images)          if len(images) != 512:         padded_array = np.zeros((512, 512, 512), dtype=np.uint8)         padded_array[:len(images)] = images_array[:512]         return padded_array          return images_array.astype(np.uint8) In\u00a0[\u00a0]: Copied! <pre>def process_single_stl(stl_file: str) -&gt; np.ndarray:\n    png_path = convert_stl_to_png(stl_file)\n    array = load_png_to_numpy(png_path, 0, 511)\n    \n    # Delete PNG files\n    for png_file in Path(png_path[:-4]).glob('*.png'):\n        png_file.unlink()\n    \n    return array\n</pre> def process_single_stl(stl_file: str) -&gt; np.ndarray:     png_path = convert_stl_to_png(stl_file)     array = load_png_to_numpy(png_path, 0, 511)          # Delete PNG files     for png_file in Path(png_path[:-4]).glob('*.png'):         png_file.unlink()          return array In\u00a0[\u00a0]: Copied! <pre>def stl_file_generator(input_dir: str, max_files: Optional[int] = None) -&gt; Iterator[Path]:\n    count = 0\n    for stl_file in Path(input_dir).rglob('*.stl'):\n        yield stl_file\n        count += 1\n        if max_files is not None and count &gt;= max_files:\n            break\n</pre> def stl_file_generator(input_dir: str, max_files: Optional[int] = None) -&gt; Iterator[Path]:     count = 0     for stl_file in Path(input_dir).rglob('*.stl'):         yield stl_file         count += 1         if max_files is not None and count &gt;= max_files:             break In\u00a0[\u00a0]: Copied! <pre>def process_stl_files(input_dir: str, output_dir: str, batch_size: int = 1000, max_files: Optional[int] = None):\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    num_workers = max(1, multiprocessing.cpu_count() - 1)\n    stl_files = stl_file_generator(input_dir, max_files)\n\n    batch_num = 0\n    file_count = 0\n\n    while True:\n        h5_filename = Path(output_dir) / f'batch_{batch_num:03d}.h5'\n        \n        with h5py.File(h5_filename, 'w') as h5f:\n            with ProcessPoolExecutor(max_workers=num_workers) as executor:\n                batch_files = list(itertools.islice(stl_files, batch_size))\n                if not batch_files:\n                    break  # No more files to process\n\n                futures = {executor.submit(process_single_stl, str(stl_file)): i \n                           for i, stl_file in enumerate(batch_files)}\n                \n                for future in as_completed(futures):\n                    i = futures[future]\n                    try:\n                        array = future.result()\n                        h5f.create_dataset(f'array_{i:03d}', data=array, compression='gzip')\n                        file_count += 1\n                        logger.info(f\"Processed file {file_count}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing file {file_count + 1}: {e}\")\n\n        logger.info(f\"Batch {batch_num} saved to {h5_filename}\")\n        batch_num += 1\n\n    logger.info(f\"Total processed files: {file_count}\")\n</pre> def process_stl_files(input_dir: str, output_dir: str, batch_size: int = 1000, max_files: Optional[int] = None):     Path(output_dir).mkdir(parents=True, exist_ok=True)      num_workers = max(1, multiprocessing.cpu_count() - 1)     stl_files = stl_file_generator(input_dir, max_files)      batch_num = 0     file_count = 0      while True:         h5_filename = Path(output_dir) / f'batch_{batch_num:03d}.h5'                  with h5py.File(h5_filename, 'w') as h5f:             with ProcessPoolExecutor(max_workers=num_workers) as executor:                 batch_files = list(itertools.islice(stl_files, batch_size))                 if not batch_files:                     break  # No more files to process                  futures = {executor.submit(process_single_stl, str(stl_file)): i                             for i, stl_file in enumerate(batch_files)}                                  for future in as_completed(futures):                     i = futures[future]                     try:                         array = future.result()                         h5f.create_dataset(f'array_{i:03d}', data=array, compression='gzip')                         file_count += 1                         logger.info(f\"Processed file {file_count}\")                     except Exception as e:                         logger.error(f\"Error processing file {file_count + 1}: {e}\")          logger.info(f\"Batch {batch_num} saved to {h5_filename}\")         batch_num += 1      logger.info(f\"Total processed files: {file_count}\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    input_dir = r\"D:\\datasets\\stl2\\abc_0000_stl2_v00\"\n    output_dir = r\"D:\\datasets\\stl2\\output_h5\"\n    batch_size = 10\n    max_files = 100  # Set this to the number of files you want to process, or None to process all files\n    process_stl_files(input_dir, output_dir, batch_size=batch_size, max_files=max_files)\n</pre> if __name__ == \"__main__\":     input_dir = r\"D:\\datasets\\stl2\\abc_0000_stl2_v00\"     output_dir = r\"D:\\datasets\\stl2\\output_h5\"     batch_size = 10     max_files = 100  # Set this to the number of files you want to process, or None to process all files     process_stl_files(input_dir, output_dir, batch_size=batch_size, max_files=max_files)"},{"location":"trainable_filter_demo/","title":"Trainable filter demo","text":"In\u00a0[18]: Copied! <pre>import os\nimport h5py\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass PairedHDF5Dataset(Dataset):\n    def __init__(self, data_dir, groundtruth_dir):\n        super(PairedHDF5Dataset, self).__init__()\n\n        # Get a list of paths\n        self.data_paths = sorted(\n            [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.hdf5')])\n        self.groundtruth_paths = sorted(\n            [os.path.join(groundtruth_dir, file) for file in os.listdir(groundtruth_dir) if file.endswith('.hdf5')])\n\n        assert len(self.data_paths) == len(self.groundtruth_paths), \"Mismatch between data and groundtruth files\"\n\n        # Calculate the cumulative sizes\n        self.cumulative_sizes = []\n        cum_size = 0\n        for path in self.data_paths:\n            with h5py.File(path, 'r') as file:\n                cum_size += len(file['data'])\n                self.cumulative_sizes.append(cum_size)\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, index):\n        # Find the correct file and local index\n        file_idx = next(i for i, cum_size in enumerate(self.cumulative_sizes) if cum_size &gt; index)\n        if file_idx == 0:\n            local_index = index\n        else:\n            local_index = index - self.cumulative_sizes[file_idx - 1]\n\n        with h5py.File(self.data_paths[file_idx], 'r') as data_file, h5py.File(self.groundtruth_paths[file_idx],\n                                                                               'r') as gt_file:\n            data = torch.tensor(np.expand_dims(data_file['data'][local_index], axis=0).squeeze())\n            groundtruth = torch.tensor(np.expand_dims(gt_file['data'][local_index], axis=0).squeeze())\n        return data, groundtruth\n\n\n# Usage:\ntrain_dataset = PairedHDF5Dataset(\n    'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_train',\n    'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_train'\n)\n\nvalidation_dataset = PairedHDF5Dataset(\n    'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_validation',\n    'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_validation'\n)\n\nfrom torch.utils.data import Subset\nimport random\n\n# Randomly sample 200 indices from the dataset\ni = random.sample(range(len(train_dataset)), 100)\nj = random.sample(range(len(validation_dataset)), 10)\n# Create a subset from the train_dataset using the sampled indices\nsubset_dataset = Subset(train_dataset, i)\nvalsubset_dataset = Subset(train_dataset, j)\n\nfrom torch.utils.data import DataLoader\n\ndata_loader = DataLoader(subset_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(valsubset_dataset, batch_size=1, shuffle=True)\n</pre> import os import h5py import numpy as np import torch from torch.utils.data import Dataset   class PairedHDF5Dataset(Dataset):     def __init__(self, data_dir, groundtruth_dir):         super(PairedHDF5Dataset, self).__init__()          # Get a list of paths         self.data_paths = sorted(             [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.hdf5')])         self.groundtruth_paths = sorted(             [os.path.join(groundtruth_dir, file) for file in os.listdir(groundtruth_dir) if file.endswith('.hdf5')])          assert len(self.data_paths) == len(self.groundtruth_paths), \"Mismatch between data and groundtruth files\"          # Calculate the cumulative sizes         self.cumulative_sizes = []         cum_size = 0         for path in self.data_paths:             with h5py.File(path, 'r') as file:                 cum_size += len(file['data'])                 self.cumulative_sizes.append(cum_size)      def __len__(self):         return self.cumulative_sizes[-1]      def __getitem__(self, index):         # Find the correct file and local index         file_idx = next(i for i, cum_size in enumerate(self.cumulative_sizes) if cum_size &gt; index)         if file_idx == 0:             local_index = index         else:             local_index = index - self.cumulative_sizes[file_idx - 1]          with h5py.File(self.data_paths[file_idx], 'r') as data_file, h5py.File(self.groundtruth_paths[file_idx],                                                                                'r') as gt_file:             data = torch.tensor(np.expand_dims(data_file['data'][local_index], axis=0).squeeze())             groundtruth = torch.tensor(np.expand_dims(gt_file['data'][local_index], axis=0).squeeze())         return data, groundtruth   # Usage: train_dataset = PairedHDF5Dataset(     'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_train',     'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_train' )  validation_dataset = PairedHDF5Dataset(     'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\observation_validation',     'C:\\\\Users\\sun\\PycharmProjects\\known_operator\\LoDoPaB-CT\\ground_truth_validation' )  from torch.utils.data import Subset import random  # Randomly sample 200 indices from the dataset i = random.sample(range(len(train_dataset)), 100) j = random.sample(range(len(validation_dataset)), 10) # Create a subset from the train_dataset using the sampled indices subset_dataset = Subset(train_dataset, i) valsubset_dataset = Subset(train_dataset, j)  from torch.utils.data import DataLoader  data_loader = DataLoader(subset_dataset, batch_size=1, shuffle=True) val_loader = DataLoader(valsubset_dataset, batch_size=1, shuffle=True) In\u00a0[19]: Copied! <pre>from pyronn.ct_reconstruction.geometry.geometry import Geometry\nfrom pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d\n\n# Initializing CT geometry parameters:\n\n# Volume parameters:\nvolume_size = 362  # size of the volume/image\nvolume_shape = [volume_size, volume_size]  # shape of the volume as [height, width]\nvolume_spacing = [1, 1]  # spacing between pixels in the volume\n\n# Detector parameters:\ndetector_shape = [513]  # shape of the detector\ndetector_spacing = [1]  # spacing between detector pixels\n\n# Trajectory parameters:\nnumber_of_projections = 1000  # number of projections in the sinogram\nangular_range = -np.pi  # angular range of the trajectory (half-circle in this case)\n\n# Create an instance of the Geometry class and initialize it with the above parameters\ngeometry = Geometry()\ngeometry.init_from_parameters(volume_shape=volume_shape, volume_spacing=volume_spacing,\n                              detector_shape=detector_shape, detector_spacing=detector_spacing,\n                              number_of_projections=number_of_projections, angular_range=angular_range,\n                              trajectory=circular_trajectory_2d)\n\nfrom torch import nn\nfrom pyronn.ct_reconstruction.layers.reconstruction import ParallelBeamReconstruction2D\nfrom traiable_filter import ParReconstruction2D\nfrom pyronn.ct_reconstruction.helpers.filters import filters\n\nmodel = ParReconstruction2D(geometry).cuda()\n</pre> from pyronn.ct_reconstruction.geometry.geometry import Geometry from pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory import circular_trajectory_2d  # Initializing CT geometry parameters:  # Volume parameters: volume_size = 362  # size of the volume/image volume_shape = [volume_size, volume_size]  # shape of the volume as [height, width] volume_spacing = [1, 1]  # spacing between pixels in the volume  # Detector parameters: detector_shape = [513]  # shape of the detector detector_spacing = [1]  # spacing between detector pixels  # Trajectory parameters: number_of_projections = 1000  # number of projections in the sinogram angular_range = -np.pi  # angular range of the trajectory (half-circle in this case)  # Create an instance of the Geometry class and initialize it with the above parameters geometry = Geometry() geometry.init_from_parameters(volume_shape=volume_shape, volume_spacing=volume_spacing,                               detector_shape=detector_shape, detector_spacing=detector_spacing,                               number_of_projections=number_of_projections, angular_range=angular_range,                               trajectory=circular_trajectory_2d)  from torch import nn from pyronn.ct_reconstruction.layers.reconstruction import ParallelBeamReconstruction2D from traiable_filter import ParReconstruction2D from pyronn.ct_reconstruction.helpers.filters import filters  model = ParReconstruction2D(geometry).cuda() In\u00a0[20]: Copied! <pre>import matplotlib.pyplot as plt\n\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(data_loader), epochs=20)\nnum_epochs = 20\n\n\ndef train_model(model, data_loader, optimizer, num_epochs, val_loader=None, save_path=\"best_model.pth\"):\n    train_loss_history = []\n    val_loss_history = []\n\n    best_val_loss = float('inf')  # Initialize with a high value\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        total_batches = len(data_loader)\n\n        for batch_idx, (data, groundtruth) in enumerate(data_loader):\n            data, groundtruth = data.cuda(), groundtruth.cuda()\n\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = nn.MSELoss()(outputs, groundtruth)\n            loss.backward(retain_graph=True)\n            optimizer.step()\n            running_loss += loss.item()\n            scheduler.step()\n\n        avg_train_loss = running_loss / total_batches\n        train_loss_history.append(avg_train_loss)\n\n        # Validation phase\n        if val_loader:\n            model.eval()\n            val_running_loss = 0.0\n            with torch.no_grad():\n                for data, groundtruth in val_loader:\n                    data, groundtruth = data.cuda(), groundtruth.cuda()\n                    outputs = model(data)\n                    loss = nn.MSELoss()(outputs, groundtruth)\n                    val_running_loss += loss.item()\n\n            avg_val_loss = val_running_loss / len(val_loader)\n\n            # Check if this is the best model\n            if avg_val_loss &lt; best_val_loss:\n                best_val_loss = avg_val_loss\n                torch.save(model.state_dict(), save_path)  # Save the best model\n\n            val_loss_history.append(avg_val_loss)\n            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        else:\n            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n\n    print(\"Finished Training\")\n    return train_loss_history, val_loss_history\n\n\nloss_history, val_loss_history = train_model(model, data_loader, optimizer, num_epochs, val_loader)\nplt.figure(figsize=(10, 6))\nplt.plot(loss_history, label='Training Loss')\nplt.plot(val_loss_history, label='Validation Loss', linestyle='--')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss over Epochs')\nplt.legend()\nplt.show()\n</pre> import matplotlib.pyplot as plt  optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01) scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(data_loader), epochs=20) num_epochs = 20   def train_model(model, data_loader, optimizer, num_epochs, val_loader=None, save_path=\"best_model.pth\"):     train_loss_history = []     val_loss_history = []      best_val_loss = float('inf')  # Initialize with a high value      for epoch in range(num_epochs):         model.train()         running_loss = 0.0         total_batches = len(data_loader)          for batch_idx, (data, groundtruth) in enumerate(data_loader):             data, groundtruth = data.cuda(), groundtruth.cuda()              optimizer.zero_grad()             outputs = model(data)             loss = nn.MSELoss()(outputs, groundtruth)             loss.backward(retain_graph=True)             optimizer.step()             running_loss += loss.item()             scheduler.step()          avg_train_loss = running_loss / total_batches         train_loss_history.append(avg_train_loss)          # Validation phase         if val_loader:             model.eval()             val_running_loss = 0.0             with torch.no_grad():                 for data, groundtruth in val_loader:                     data, groundtruth = data.cuda(), groundtruth.cuda()                     outputs = model(data)                     loss = nn.MSELoss()(outputs, groundtruth)                     val_running_loss += loss.item()              avg_val_loss = val_running_loss / len(val_loader)              # Check if this is the best model             if avg_val_loss &lt; best_val_loss:                 best_val_loss = avg_val_loss                 torch.save(model.state_dict(), save_path)  # Save the best model              val_loss_history.append(avg_val_loss)             print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")         else:             print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")      print(\"Finished Training\")     return train_loss_history, val_loss_history   loss_history, val_loss_history = train_model(model, data_loader, optimizer, num_epochs, val_loader) plt.figure(figsize=(10, 6)) plt.plot(loss_history, label='Training Loss') plt.plot(val_loss_history, label='Validation Loss', linestyle='--') plt.xlabel('Epochs') plt.ylabel('Loss') plt.title('Training and Validation Loss over Epochs') plt.legend() plt.show() <pre>Epoch [1/20], Train Loss: 0.0099, Val Loss: 0.0085\nEpoch [2/20], Train Loss: 0.0079, Val Loss: 0.0066\nEpoch [3/20], Train Loss: 0.0054, Val Loss: 0.0041\nEpoch [4/20], Train Loss: 0.0031, Val Loss: 0.0023\nEpoch [5/20], Train Loss: 0.0019, Val Loss: 0.0015\nEpoch [6/20], Train Loss: 0.0013, Val Loss: 0.0011\nEpoch [7/20], Train Loss: 0.0010, Val Loss: 0.0008\nEpoch [8/20], Train Loss: 0.0008, Val Loss: 0.0007\nEpoch [9/20], Train Loss: 0.0007, Val Loss: 0.0006\nEpoch [10/20], Train Loss: 0.0006, Val Loss: 0.0006\nEpoch [11/20], Train Loss: 0.0006, Val Loss: 0.0005\nEpoch [12/20], Train Loss: 0.0005, Val Loss: 0.0005\nEpoch [13/20], Train Loss: 0.0005, Val Loss: 0.0005\nEpoch [14/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [15/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [16/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [17/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [18/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [19/20], Train Loss: 0.0005, Val Loss: 0.0004\nEpoch [20/20], Train Loss: 0.0005, Val Loss: 0.0004\nFinished Training\n</pre> In\u00a0[21]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming this is the frequency domain representation of your filter\nfilter_freq_domain = model.filter.filter_1d.detach().cpu().numpy().squeeze()\n\n# Get the frequencies for positive and negative values\nfrequencies = np.fft.fftshift(np.fft.fftfreq(len(filter_freq_domain)))\n\n# Shift the zero-frequency component to the center\nfilter_freq_domain_centered = np.fft.fftshift(filter_freq_domain)\n\n# Now plot the magnitude response\nplt.figure(figsize=(10, 5))\nplt.plot(frequencies, filter_freq_domain_centered)\nplt.title('Magnitude Response')\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Magnitude')\nplt.grid(True)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Assuming this is the frequency domain representation of your filter filter_freq_domain = model.filter.filter_1d.detach().cpu().numpy().squeeze()  # Get the frequencies for positive and negative values frequencies = np.fft.fftshift(np.fft.fftfreq(len(filter_freq_domain)))  # Shift the zero-frequency component to the center filter_freq_domain_centered = np.fft.fftshift(filter_freq_domain)  # Now plot the magnitude response plt.figure(figsize=(10, 5)) plt.plot(frequencies, filter_freq_domain_centered) plt.title('Magnitude Response') plt.xlabel('Frequency (Hz)') plt.ylabel('Magnitude') plt.grid(True) plt.show() In\u00a0[34]: Copied! <pre>from pyronn.ct_reconstruction.helpers.filters.filters import ram_lak, shepp_logan, cosine, hamming, hann\n\n\ndef plot_filters(num_detectors):\n    # Generate filters\n    ram_lak1 = ram_lak(num_detectors, 1)\n    shepp_logan1 = shepp_logan(num_detectors, 1)\n    cosine1 = cosine(num_detectors, 1)\n    hamming1 = hamming(num_detectors, 1)\n    hann1 = hann(num_detectors, 1)\n\n    # Get the frequencies for positive and negative values\n    frequencies = np.fft.fftshift(np.fft.fftfreq(num_detectors))\n\n    # Now plot the magnitude response\n    plt.figure(figsize=(10, 5))\n    plt.plot(frequencies, np.fft.fftshift(ram_lak1), label='Ram-Lak')\n    plt.plot(frequencies, np.fft.fftshift(shepp_logan1), label='Shepp-Logan')\n    plt.plot(frequencies, np.fft.fftshift(cosine1), label='Cosine')\n    plt.plot(frequencies, np.fft.fftshift(hamming1), label='Hamming')\n    plt.plot(frequencies, np.fft.fftshift(hann1), label='Hann')\n\n    plt.title('Magnitude Response')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Magnitude')\n    plt.legend()  # Add legend to differentiate filters\n    plt.grid(True)\n    plt.show()\n\n\n# Call the function with the desired number of detectors\nplot_filters(513)\n</pre> from pyronn.ct_reconstruction.helpers.filters.filters import ram_lak, shepp_logan, cosine, hamming, hann   def plot_filters(num_detectors):     # Generate filters     ram_lak1 = ram_lak(num_detectors, 1)     shepp_logan1 = shepp_logan(num_detectors, 1)     cosine1 = cosine(num_detectors, 1)     hamming1 = hamming(num_detectors, 1)     hann1 = hann(num_detectors, 1)      # Get the frequencies for positive and negative values     frequencies = np.fft.fftshift(np.fft.fftfreq(num_detectors))      # Now plot the magnitude response     plt.figure(figsize=(10, 5))     plt.plot(frequencies, np.fft.fftshift(ram_lak1), label='Ram-Lak')     plt.plot(frequencies, np.fft.fftshift(shepp_logan1), label='Shepp-Logan')     plt.plot(frequencies, np.fft.fftshift(cosine1), label='Cosine')     plt.plot(frequencies, np.fft.fftshift(hamming1), label='Hamming')     plt.plot(frequencies, np.fft.fftshift(hann1), label='Hann')      plt.title('Magnitude Response')     plt.xlabel('Frequency (Hz)')     plt.ylabel('Magnitude')     plt.legend()  # Add legend to differentiate filters     plt.grid(True)     plt.show()   # Call the function with the desired number of detectors plot_filters(513) In\u00a0[23]: Copied! <pre>def load_data_from_hdf5(filename, dataset_name=\"data\"):\n    with h5py.File(filename, 'r') as f:\n        # Assuming the dataset is 3D and you want the first image\n        data = f[dataset_name][-1, :, :]\n    return data\n\n\nphantom = load_data_from_hdf5(\"../LoDoPaB-CT/ground_truth_test/ground_truth_test_000.hdf5\")\nphantom = torch.tensor(np.expand_dims(phantom, axis=0).copy(), dtype=torch.float32).cuda()\nsinogram = load_data_from_hdf5(\"../LoDoPaB-CT/observation_test/observation_test_000.hdf5\")\nsinogram = torch.tensor(np.expand_dims(sinogram, axis=0).copy(), dtype=torch.float32).cuda()\n</pre> def load_data_from_hdf5(filename, dataset_name=\"data\"):     with h5py.File(filename, 'r') as f:         # Assuming the dataset is 3D and you want the first image         data = f[dataset_name][-1, :, :]     return data   phantom = load_data_from_hdf5(\"../LoDoPaB-CT/ground_truth_test/ground_truth_test_000.hdf5\") phantom = torch.tensor(np.expand_dims(phantom, axis=0).copy(), dtype=torch.float32).cuda() sinogram = load_data_from_hdf5(\"../LoDoPaB-CT/observation_test/observation_test_000.hdf5\") sinogram = torch.tensor(np.expand_dims(sinogram, axis=0).copy(), dtype=torch.float32).cuda() In\u00a0[24]: Copied! <pre>if os.path.exists(\"best_model.pth\"):\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n</pre> if os.path.exists(\"best_model.pth\"):     model.load_state_dict(torch.load(\"best_model.pth\"))  In\u00a0[25]: Copied! <pre>model.eval()\nreconstruction = model(sinogram)\n</pre> model.eval() reconstruction = model(sinogram) In\u00a0[26]: Copied! <pre>reco_filter = filters.hann_2D(geometry.detector_shape,\n                              geometry.detector_spacing,\n                              geometry.number_of_projections)\n\nFBP = ParallelBeamReconstruction2D(geometry=geometry, filter=reco_filter, tainable_filter=False).cuda()\nFBP.eval()\nreconstruction_FBP = FBP(sinogram, **geometry)\n</pre> reco_filter = filters.hann_2D(geometry.detector_shape,                               geometry.detector_spacing,                               geometry.number_of_projections)  FBP = ParallelBeamReconstruction2D(geometry=geometry, filter=reco_filter, tainable_filter=False).cuda() FBP.eval() reconstruction_FBP = FBP(sinogram, **geometry) In\u00a0[27]: Copied! <pre>def normalize_image(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize the pixel values of an image to the range [0, 1].\"\"\"\n    min_val = image.min()\n    max_val = image.max()\n\n    return (image - min_val) / (max_val - min_val)\n\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(normalize_image(phantom.squeeze().cpu()), cmap='gray')\nplt.title('Ground Truth')\n\nplt.subplot(1, 3, 2)\nplt.imshow(normalize_image(reconstruction.detach().cpu().numpy().squeeze()), cmap='gray')\nplt.title('FBP_trained_filter')\n\nplt.subplot(1, 3, 3)\nplt.imshow(normalize_image(reconstruction_FBP.cpu().detach().numpy().squeeze()), cmap='gray')\nplt.title('FBP_hann')\n\nplt.tight_layout()\nplt.show()\n</pre> def normalize_image(image: np.ndarray) -&gt; np.ndarray:     \"\"\"Normalize the pixel values of an image to the range [0, 1].\"\"\"     min_val = image.min()     max_val = image.max()      return (image - min_val) / (max_val - min_val)   plt.figure(figsize=(15, 5))  plt.subplot(1, 3, 1) plt.imshow(normalize_image(phantom.squeeze().cpu()), cmap='gray') plt.title('Ground Truth')  plt.subplot(1, 3, 2) plt.imshow(normalize_image(reconstruction.detach().cpu().numpy().squeeze()), cmap='gray') plt.title('FBP_trained_filter')  plt.subplot(1, 3, 3) plt.imshow(normalize_image(reconstruction_FBP.cpu().detach().numpy().squeeze()), cmap='gray') plt.title('FBP_hann')  plt.tight_layout() plt.show() In\u00a0[28]: Copied! <pre>difference_learn = normalize_image(reconstruction.squeeze().cpu().detach().numpy()) - normalize_image(\n    phantom.squeeze().cpu().numpy())\ndifference_fbp = normalize_image(reconstruction_FBP.squeeze().cpu().detach().numpy()) - normalize_image(\n    phantom.squeeze().cpu().numpy())\n\nplt.figure(figsize=(16, 5))\n\n# Display the difference image between IR and phantom.\nplt.subplot(1, 2, 1)\nplt.imshow(difference_learn, cmap='gray')\nplt.title('Difference (L vs. Phantom)')\nplt.colorbar()\n\n# Display the difference image between FBP and phantom.\nplt.subplot(1, 2, 2)\nplt.imshow(difference_fbp, cmap='gray')\nplt.title('Difference (FBP vs. Phantom)')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n# Calculate the MSE for each reconstruction method.\nmse_ir = nn.MSELoss()(reconstruction, phantom).item()\nmse_fbp = nn.MSELoss()(reconstruction_FBP, phantom).item()\n\nprint(f\"Mean Squared Error (MSE) between L and Phantom: {mse_ir:.4f}\")\nprint(f\"Mean Squared Error (MSE) between FBP and Phantom: {mse_fbp:.4f}\")\n</pre> difference_learn = normalize_image(reconstruction.squeeze().cpu().detach().numpy()) - normalize_image(     phantom.squeeze().cpu().numpy()) difference_fbp = normalize_image(reconstruction_FBP.squeeze().cpu().detach().numpy()) - normalize_image(     phantom.squeeze().cpu().numpy())  plt.figure(figsize=(16, 5))  # Display the difference image between IR and phantom. plt.subplot(1, 2, 1) plt.imshow(difference_learn, cmap='gray') plt.title('Difference (L vs. Phantom)') plt.colorbar()  # Display the difference image between FBP and phantom. plt.subplot(1, 2, 2) plt.imshow(difference_fbp, cmap='gray') plt.title('Difference (FBP vs. Phantom)') plt.colorbar()  plt.tight_layout() plt.show()  # Calculate the MSE for each reconstruction method. mse_ir = nn.MSELoss()(reconstruction, phantom).item() mse_fbp = nn.MSELoss()(reconstruction_FBP, phantom).item()  print(f\"Mean Squared Error (MSE) between L and Phantom: {mse_ir:.4f}\") print(f\"Mean Squared Error (MSE) between FBP and Phantom: {mse_fbp:.4f}\") <pre>Mean Squared Error (MSE) between L and Phantom: 0.0003\nMean Squared Error (MSE) between FBP and Phantom: 0.0412\n</pre>"},{"location":"usage/","title":"Usage \ud83c\udfd7\ufe0f\ud83d\udcbb","text":"<p>The methodology of Pyro-NN is contained within the <code>ct_reconstruction</code> folder, which is organized into four essential parts:</p>"},{"location":"usage/#1-geometry","title":"1. \ud83d\udd27 Geometry","text":"<p>Defines the scanning parameters and trajectory. </p> <ul> <li>Initialization from parameters: This is possible if you know all your scanning parameters and if the scanning trajectory was circular. </li> <li>\u26a0\ufe0f Small Tip: Sometimes, parameters in the header can be incorrectly filled. Be aware\u2014errors may still occur!</li> </ul>"},{"location":"usage/#2-layers","title":"2. \ud83d\udee0\ufe0f Layers","text":"<p>Defines the 2D and 3D forward/backward projectors. </p> <ul> <li>Geometry setup: To initialize these layers, the geometry of the scan must be defined.</li> <li> <p>Input of all layers: The input is the Image-Tensor (depending on the dimensionality) and a geometry dictionary, returned when the geometry is initialized.</p> </li> <li> <p>For 2D: Includes implementations for Parallel Beam and Fan Beam.</p> </li> <li>For 3D: Implements Cone Beam.</li> </ul>"},{"location":"usage/#3-helpers","title":"3. \ud83d\udd0d Helpers","text":"<p>Provides pre-implemented filters, weights, trajectories, and phantoms. </p> <ul> <li>Implemented Filters: </li> <li>Ramp Filters, Ram Lak, Shepp Logan, Cosine, Hamming, Hann \ud83c\udf9b\ufe0f</li> <li>Implemented Weights: </li> <li>Cosine, Parker \u2696\ufe0f</li> <li>Implemented Trajectories: </li> <li>Circular and arbitrary paths \ud83c\udf00</li> </ul>"},{"location":"usage/#4-cores","title":"4. \u2699\ufe0f Cores","text":"<p>Contains the kernels and the PyTorch connection.</p> <pre><code>        Each part plays a crucial role in making Pyro-NN an efficient \n        and powerful framework for differentiable reconstruction. \ud83d\ude80\n</code></pre>"},{"location":"reference/__init__/","title":"Reference for <code>pyronn/__init__.py</code>","text":""},{"location":"reference/__init__/#pyronn.default_config","title":"pyronn.default_config","text":"<pre><code>default_config()\n</code></pre> Source code in <code>pyronn/__init__.py</code> <pre><code>def default_config():\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    config = {'backend': 'torch'}\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config, f)\n</code></pre>"},{"location":"reference/__init__/#pyronn.read_backend","title":"pyronn.read_backend","text":"<pre><code>read_backend()\n</code></pre> Source code in <code>pyronn/__init__.py</code> <pre><code>def read_backend():\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    if not os.path.exists(CONFIG_FILE):\n        default_config()\n    with open(CONFIG_FILE, 'r') as f:\n        config = json.load(f)\n    return config['backend']\n</code></pre>"},{"location":"reference/__init__/#pyronn.set_backend","title":"pyronn.set_backend","text":"<pre><code>set_backend(value)\n</code></pre> Source code in <code>pyronn/__init__.py</code> <pre><code>def set_backend(value):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    config = {'backend': value}\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config, f)\n</code></pre>"},{"location":"reference/ct_reconstruction/__init__/","title":"Reference for <code>pyronn/ct_reconstruction/__init__.py</code>","text":""},{"location":"reference/ct_reconstruction/__init__/#pyronn.ct_reconstruction.default_config","title":"pyronn.ct_reconstruction.default_config","text":"<pre><code>default_config()\n</code></pre> Source code in <code>pyronn/ct_reconstruction/__init__.py</code> <pre><code>def default_config():\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    config = {'backend': 'torch'}\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config, f)\n</code></pre>"},{"location":"reference/ct_reconstruction/__init__/#pyronn.ct_reconstruction.read_backend","title":"pyronn.ct_reconstruction.read_backend","text":"<pre><code>read_backend()\n</code></pre> Source code in <code>pyronn/ct_reconstruction/__init__.py</code> <pre><code>def read_backend():\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    if not os.path.exists(CONFIG_FILE):\n        default_config()\n    with open(CONFIG_FILE, 'r') as f:\n        config = json.load(f)\n    return config['backend']\n</code></pre>"},{"location":"reference/ct_reconstruction/__init__/#pyronn.ct_reconstruction.set_backend","title":"pyronn.ct_reconstruction.set_backend","text":"<pre><code>set_backend(value)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/__init__.py</code> <pre><code>def set_backend(value):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    config = {'backend': value}\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config, f)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/","title":"Reference for <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code>","text":""},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryBase","title":"pyronn.ct_reconstruction.geometry.geometry_base.GeometryBase","text":"<pre><code>GeometryBase(volume_shape, volume_spacing, detector_shape, detector_spacing, number_of_projections, angular_range, source_detector_distance, source_isocenter_distance, *args, **kwargs)\n</code></pre> <p>The Base Class for the different Geometry classes. Provides commonly used members.</p> <pre><code>volume_shape:               The volume size in Z, Y, X order.\nvolume_spacing:             The spacing between voxels in Z, Y, X order.\ndetector_shape:             Shape of the detector in Y, X order.\ndetector_spacing:           The spacing between detector voxels in Y, X order.\nnumber_of_projections:      Number of equidistant projections.\nangular_range:              The covered angular range.\nsource_detector_distance:   The source to detector distance (sdd).\nsource_isocenter_distance:  The source to isocenter distance (sid).\n</code></pre> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def __init__(self,\n             volume_shape,\n             volume_spacing,\n             detector_shape,\n             detector_spacing,\n             number_of_projections,\n             angular_range,\n             source_detector_distance,\n             source_isocenter_distance,\n             *args, **kwargs):\n    \"\"\"\n        Constructor of Base Geometry Class, should only get called by sub classes.\n    Args:\n        volume_shape:               The volume size in Z, Y, X order.\n        volume_spacing:             The spacing between voxels in Z, Y, X order.\n        detector_shape:             Shape of the detector in Y, X order.\n        detector_spacing:           The spacing between detector voxels in Y, X order.\n        number_of_projections:      Number of equidistant projections.\n        angular_range:              The covered angular range.\n        source_detector_distance:   The source to detector distance (sdd).\n        source_isocenter_distance:  The source to isocenter distance (sid).\n    \"\"\"\n    self.np_dtype = np.float32  # datatype for np.arrays make sure everything will be float32\n    # self.gpu_device = True\n    # Volume Parameters:\n    self.volume_shape = np.array(volume_shape)\n    self.volume_spacing = np.array(volume_spacing, dtype=self.np_dtype)\n    self.volume_origin = -(self.volume_shape - 1) / 2.0 * self.volume_spacing\n\n    # Detector Parameters:\n    self.detector_shape = np.array(detector_shape)\n    self.detector_spacing = np.array(detector_spacing, dtype=self.np_dtype)\n    self.detector_origin = -(self.detector_shape - 1) / 2.0 * self.detector_spacing\n\n    # Trajectory Parameters:\n    self.number_of_projections = number_of_projections\n    if isinstance(angular_range, list):\n        self.angular_range = angular_range\n    else:\n        self.angular_range = [0, angular_range]\n\n    self.sinogram_shape = np.array([self.number_of_projections, *self.detector_shape])\n\n    self.source_detector_distance = source_detector_distance\n    self.source_isocenter_distance = source_isocenter_distance\n    self.fan_angle = None\n    self.cone_angle = None\n    self.projection_multiplier = None\n    self.step_size = None\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryBase.get_dict","title":"get_dict","text":"<pre><code>get_dict()\n</code></pre> <p>Get the geometry as a dict.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def get_dict(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Get the geometry as a dict.\n    '''\n    info = {}\n    for i in dir(self):\n        if i[:2] != '__': info[i] = getattr(self, i)\n    return info\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryBase.set_trajectory","title":"set_trajectory","text":"<pre><code>set_trajectory(trajectory)\n</code></pre> <pre><code>Sets the member trajectory.\n</code></pre> <p>Args:     trajectory: np.array defining the trajectory.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def set_trajectory(self, trajectory):\n    \"\"\"\n        Sets the member trajectory.\n    Args:\n        trajectory: np.array defining the trajectory.\n    \"\"\"\n    self.trajectory = np.array(trajectory, self.np_dtype)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryBase.update","title":"update","text":"<pre><code>update(dict)\n</code></pre> <pre><code>Change the geometry.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>new geometry values. Choose your target properties by setting them as the keys.</p> required Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def update(self, dict):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Change the geometry.\n\n    args:\n        dict: new geometry values. Choose your target properties by setting them as the keys.\n    '''\n    changed = []\n    for key in dict:\n        if key in dir(self):\n            setattr(self, key, dict[key])\n            self.key = dict[key]\n            changed.append(key)\n        else:\n            print(f'{key} is not a property of geometry! Please check it!')\n    if changed:\n        print(f'The following properties has been changed: {changed}')\n        if 'trajectory' not in changed:\n            print(f'Please confirm whether you need to modify the trajectory.')\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryParallel2D","title":"pyronn.ct_reconstruction.geometry.geometry_base.GeometryParallel2D","text":"<pre><code>GeometryParallel2D(volume_shape, volume_spacing, detector_shape, detector_spacing, number_of_projections, angular_range, *args, **kwargs)\n</code></pre> <p>               Bases: <code>GeometryBase</code></p> <p>2D Parallel specialization of Geometry.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def __init__(self,\n             volume_shape, volume_spacing,\n             detector_shape, detector_spacing,\n             number_of_projections, angular_range, *args, **kwargs):\n    # init base selfmetry class with 2 dimensional members:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super().__init__(volume_shape, volume_spacing,\n                     detector_shape, detector_spacing,\n                     number_of_projections, angular_range,\n                     None, None, *args, **kwargs)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryFan2D","title":"pyronn.ct_reconstruction.geometry.geometry_base.GeometryFan2D","text":"<pre><code>GeometryFan2D(volume_shape, volume_spacing, detector_shape, detector_spacing, number_of_projections, angular_range, source_detector_distance, source_isocenter_distance, *args, **kwargs)\n</code></pre> <p>               Bases: <code>GeometryBase</code></p> <p>2D Fan specialization of Geometry.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def __init__(self,\n             volume_shape, volume_spacing,\n             detector_shape, detector_spacing,\n             number_of_projections, angular_range,\n             source_detector_distance, source_isocenter_distance, *args, **kwargs):\n    # init base Geometry class with 2 dimensional members:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super().__init__(volume_shape, volume_spacing,\n                     detector_shape, detector_spacing,\n                     number_of_projections, angular_range,\n                     source_detector_distance, source_isocenter_distance, *args, **kwargs)\n\n    # defined by geometry so calculate for convenience use\n    self.fan_angle = np.arctan(((self.detector_shape[0] - 1) / 2.0 * self.detector_spacing[0]) / self.source_detector_distance)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_base/#pyronn.ct_reconstruction.geometry.geometry_base.GeometryCone3D","title":"pyronn.ct_reconstruction.geometry.geometry_base.GeometryCone3D","text":"<pre><code>GeometryCone3D(volume_shape, volume_spacing, detector_shape, detector_spacing, number_of_projections, angular_range, source_detector_distance, source_isocenter_distance, *args, **kwargs)\n</code></pre> <p>               Bases: <code>GeometryBase</code></p> <p>3D Cone specialization of Geometry.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_base.py</code> <pre><code>def __init__(self,\n             volume_shape, volume_spacing,\n             detector_shape, detector_spacing,\n             number_of_projections, angular_range,\n             source_detector_distance, source_isocenter_distance, *args, **kwargs):\n    # init base Geometry class with 3 dimensional members:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super().__init__(volume_shape, volume_spacing,\n                     detector_shape, detector_spacing,\n                     number_of_projections, angular_range,\n                     source_detector_distance, source_isocenter_distance, *args, **kwargs)\n\n    # defined by geometry so calculate for convenience use\n    self.fan_angle = np.arctan(((self.detector_shape[1] - 1) / 2.0 * self.detector_spacing[1]) / self.source_detector_distance)\n    self.cone_angle = np.arctan(((self.detector_shape[0] - 1) / 2.0 * self.detector_spacing[0]) / self.source_detector_distance)\n\n    # Containing the constant part of the distance weight and discretization invariant\n    self.projection_multiplier = self.source_isocenter_distance * self.source_detector_distance * detector_spacing[-1] * np.pi / self.number_of_projections\n    # TODO: need to be changed or not?\n    self.step_size = 0.2\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_specific/","title":"Reference for <code>pyronn/ct_reconstruction/geometry/geometry_specific.py</code>","text":""},{"location":"reference/ct_reconstruction/geometry/geometry_specific/#pyronn.ct_reconstruction.geometry.geometry_specific.SpecificGeometry","title":"pyronn.ct_reconstruction.geometry.geometry_specific.SpecificGeometry","text":"<pre><code>SpecificGeometry(geo_info_dict, traj_func)\n</code></pre> <p>               Bases: <code>ABC</code></p> <pre><code>geo_info_dict: All required information for creating a geometry.\n</code></pre> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_specific.py</code> <pre><code>def __init__(self, geo_info_dict, traj_func):\n    \"\"\"\n    Generate a specific geometry\n    Args:\n        geo_info_dict: All required information for creating a geometry.\n    \"\"\"\n    self.geometry_info = geo_info_dict\n    self.geometry = self.set_geo()\n    temp_info = {**geo_info_dict, **self.geometry.get_dict()}\n    self.trajectory = traj_func(**temp_info)\n    self.geometry.set_trajectory(self.trajectory)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_specific/#pyronn.ct_reconstruction.geometry.geometry_specific.SpecificGeometry.generate_specific_phantom","title":"generate_specific_phantom","text":"<pre><code>generate_specific_phantom(phantom_func, *args, **kwargs)\n</code></pre> <p>Generates a phantom created by the given function and its corresponding sinogram.</p> <p>The method first creates a phantom based on the volume shape specified in the geometry attribute of the class. It then computes the sinogram by applying a forward projection. The projection is calculated based on the parameters defined in the geometry attribute, including the detector shape, spacing, and the source-detector configuration.</p> <p>Returns:</p> Type Description <p>Tuple[np.array, np.array]: A tuple containing two numpy arrays. The first array is the generated</p> <p>3D mask of the phantom, and the second array is the corresponding 3D sinogram obtained through</p> <p>the cone beam forward projection.</p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_specific.py</code> <pre><code>def generate_specific_phantom(self, phantom_func, *args, **kwargs):\n    \"\"\"\n    Generates a phantom created by the given function and its corresponding sinogram.\n\n    The method first creates a phantom based on the volume shape specified in the\n    geometry attribute of the class. It then computes the sinogram by applying a forward projection.\n    The projection is calculated based on the parameters defined in the\n    geometry attribute, including the detector shape, spacing, and the source-detector configuration.\n\n    Returns:\n        Tuple[np.array, np.array]: A tuple containing two numpy arrays. The first array is the generated\n        3D mask of the phantom, and the second array is the corresponding 3D sinogram obtained through\n        the cone beam forward projection.\n    \"\"\"\n    phantom = phantom_func(self.geometry_info['volume_shape'], *args, **kwargs)\n    phantom = np.expand_dims(phantom, axis=0)\n    mask = (phantom != 0)\n    sinogram = self.create_sinogram(phantom)\n    return mask, sinogram, phantom, self.geometry\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_specific/#pyronn.ct_reconstruction.geometry.geometry_specific.CircularGeometrys3D","title":"pyronn.ct_reconstruction.geometry.geometry_specific.CircularGeometrys3D","text":"<pre><code>CircularGeometrys3D(geo_dict_info)\n</code></pre> <p>               Bases: <code>SpecificGeometry</code></p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_specific.py</code> <pre><code>def __init__(self, geo_dict_info):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super().__init__(geo_dict_info, circular_trajectory_3d)\n</code></pre>"},{"location":"reference/ct_reconstruction/geometry/geometry_specific/#pyronn.ct_reconstruction.geometry.geometry_specific.ArbitraryGeometrys3D","title":"pyronn.ct_reconstruction.geometry.geometry_specific.ArbitraryGeometrys3D","text":"<pre><code>ArbitraryGeometrys3D(geo_dict_info)\n</code></pre> <p>               Bases: <code>SpecificGeometry</code></p> Source code in <code>pyronn/ct_reconstruction/geometry/geometry_specific.py</code> <pre><code>def __init__(self, geo_dict_info):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super().__init__(geo_dict_info, arbitrary_projection_matrix)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ramp","title":"pyronn.ct_reconstruction.helpers.filters.filters.ramp","text":"<pre><code>ramp(detector_width: int) -&gt; np.array\n</code></pre> <p>create a 1d ramp filter.</p> <p>:param detector_width: width of detector(filter) :return: filter</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ramp(detector_width:int)-&gt;np.array:\n    \"\"\"\n    create a 1d ramp filter.\n\n    :param detector_width: width of detector(filter)\n    :return: filter\n    \"\"\"\n    filter_array = np.zeros(detector_width)\n    frequency_spacing = 0.5 / (detector_width / 2.0)\n    for i in range(0, filter_array.shape[0]):\n        if i &lt;= filter_array.shape[0] / 2.0:\n            filter_array[i] = i * frequency_spacing\n        elif i &gt; filter_array.shape[0] / 2.0:\n            filter_array[i] = 0.5 - (((i - filter_array.shape[0] / 2.0)) * frequency_spacing)\n    return filter_array.astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ramp_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.ramp_2D","text":"<pre><code>ramp_2D(detector_shape: Tuple[int, int], number_of_projections: int) -&gt; np.array\n</code></pre> <p>create a 2d ramp filter.</p> <p>:param detector_shape: shape of detector :param number_of_projections: number of projections :return: a 2d ramp filter</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ramp_2D(detector_shape:Tuple[int,int], number_of_projections:int)-&gt;np.array:\n    \"\"\"\n    create a 2d ramp filter.\n\n    :param detector_shape: shape of detector\n    :param number_of_projections: number of projections\n    :return: a 2d ramp filter\n    \"\"\"\n    detector_width = detector_shape[-1]\n\n    filter = [\n        np.reshape(\n            ramp(detector_width),\n            (1, detector_width)\n        )\n        for i in range(0, number_of_projections)\n    ]\n\n    filter = np.concatenate(filter)\n\n    return filter\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ramp_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.ramp_3D","text":"<pre><code>ramp_3D(detector_shape: Tuple[int, int, int], number_of_projections: int) -&gt; np.array\n</code></pre> <p>create a 3d ramp filter</p> <p>:param detector_shape: shape of detector :param number_of_projections: number of projections :return: a 3d ramp filter</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ramp_3D(detector_shape:Tuple[int,int,int], number_of_projections:int)-&gt;np.array:\n    \"\"\"\n    create a 3d ramp filter\n\n    :param detector_shape: shape of detector\n    :param number_of_projections: number of projections\n    :return: a 3d ramp filter\n    \"\"\"\n    detector_width = detector_shape[-1]\n\n    filter = [\n        np.reshape(\n            ramp(detector_width),\n            (1, 1, detector_width)\n        )\n        for i in range(0, number_of_projections)\n    ]\n\n    filter = np.concatenate(filter)\n\n    return filter\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ram_lak","title":"pyronn.ct_reconstruction.helpers.filters.filters.ram_lak","text":"<pre><code>ram_lak(num_detectors: int, detector_spacing: float) -&gt; np.array\n</code></pre> <p>Generate the RAM-LAK (Ramp) filter in the frequency domain.</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ram_lak(num_detectors: int, detector_spacing: float) -&gt; np.array:\n    \"\"\"Generate the RAM-LAK (Ramp) filter in the frequency domain.\"\"\"\n    frequencies = np.fft.fftfreq(num_detectors)\n    ramp = 1.0 / (detector_spacing * detector_spacing)\n    filter = ramp * np.abs(frequencies)\n\n    return filter.astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ram_lak_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.ram_lak_2D","text":"<pre><code>ram_lak_2D(detector_shape: Tuple[int, int], detector_spacing: Tuple[float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ram_lak_2D(\n    detector_shape: Tuple[int, int],\n    detector_spacing: Tuple[float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = ram_lak(detector_width, detector_spacing_width)\n    filter_2D = np.tile(filter_1D, (number_of_projections, 1))\n\n    return filter_2D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.ram_lak_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.ram_lak_3D","text":"<pre><code>ram_lak_3D(detector_shape: Tuple[int, int, int], detector_spacing: Tuple[float, float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def ram_lak_3D(\n    detector_shape: Tuple[int, int, int],\n    detector_spacing: Tuple[float, float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = ram_lak(detector_width, detector_spacing_width)\n    filter_3D = np.tile(\n        filter_1D.reshape(1, 1, detector_width), (number_of_projections, 1, 1)\n    )\n\n    return filter_3D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan","title":"pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan","text":"<pre><code>shepp_logan(num_detectors: int, detector_spacing: float) -&gt; np.array\n</code></pre> <p>Generate the Shepp-Logan filter in the frequency domain.</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def shepp_logan(num_detectors: int, detector_spacing: float) -&gt; np.array:\n    \"\"\"Generate the Shepp-Logan filter in the frequency domain.\"\"\"\n    frequencies = np.fft.fftfreq(num_detectors)\n    ramp = 1.0 / (detector_spacing * detector_spacing)\n    filter = ramp * np.abs(frequencies)\n    sinc_filter = np.where(\n        frequencies == 0, 1.0, np.sin(np.pi * frequencies) / (np.pi * frequencies)\n    )\n\n    return (filter * sinc_filter).astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan_2D","text":"<pre><code>shepp_logan_2D(detector_shape: Tuple[int, int], detector_spacing: Tuple[float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def shepp_logan_2D(\n    detector_shape: Tuple[int, int],\n    detector_spacing: Tuple[float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = shepp_logan(detector_width, detector_spacing_width)\n    filter_2D = np.tile(filter_1D, (number_of_projections, 1))\n\n    return filter_2D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.shepp_logan_3D","text":"<pre><code>shepp_logan_3D(detector_shape: Tuple[int, int, int], detector_spacing: Tuple[float, float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def shepp_logan_3D(\n    detector_shape: Tuple[int, int, int],\n    detector_spacing: Tuple[float, float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = shepp_logan(detector_width, detector_spacing_width)\n    filter_3D = np.tile(\n        filter_1D.reshape(1, 1, detector_width), (number_of_projections, 1, 1)\n    )\n\n    return filter_3D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.cosine","title":"pyronn.ct_reconstruction.helpers.filters.filters.cosine","text":"<pre><code>cosine(num_detectors: int, detector_spacing: float) -&gt; np.array\n</code></pre> <p>Generate the Cosine filter in the frequency domain.</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def cosine(num_detectors: int, detector_spacing: float) -&gt; np.array:\n    \"\"\"Generate the Cosine filter in the frequency domain.\"\"\"\n    frequencies = np.fft.fftfreq(num_detectors)\n    ramp = 1.0 / (detector_spacing * detector_spacing)\n    filter = ramp * np.abs(frequencies) * np.cos(np.pi * frequencies / 2)\n\n    return filter.astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.cosine_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.cosine_2D","text":"<pre><code>cosine_2D(detector_shape: Tuple[int, int], detector_spacing: Tuple[float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def cosine_2D(\n    detector_shape: Tuple[int, int],\n    detector_spacing: Tuple[float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = cosine(detector_width, detector_spacing_width)\n    filter_2D = np.tile(filter_1D, (number_of_projections, 1))\n\n    return filter_2D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.cosine_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.cosine_3D","text":"<pre><code>cosine_3D(detector_shape: Tuple[int, int, int], detector_spacing: Tuple[float, float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def cosine_3D(\n    detector_shape: Tuple[int, int, int],\n    detector_spacing: Tuple[float, float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = cosine(detector_width, detector_spacing_width)\n    filter_3D = np.tile(\n        filter_1D.reshape(1, 1, detector_width), (number_of_projections, 1, 1)\n    )\n\n    return filter_3D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hamming","title":"pyronn.ct_reconstruction.helpers.filters.filters.hamming","text":"<pre><code>hamming(num_detectors: int, detector_spacing: float) -&gt; np.array\n</code></pre> <p>Generate the Hamming filter in the frequency domain.</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hamming(num_detectors: int, detector_spacing: float) -&gt; np.array:\n    \"\"\"Generate the Hamming filter in the frequency domain.\"\"\"\n    frequencies = np.fft.fftfreq(num_detectors)\n    ramp = 1.0 / (detector_spacing * detector_spacing)\n    filter = ramp * np.abs(frequencies) * (0.54 + 0.46 * np.cos(np.pi * frequencies))\n\n    return filter.astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hamming_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.hamming_2D","text":"<pre><code>hamming_2D(detector_shape: Tuple[int, int], detector_spacing: Tuple[float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hamming_2D(\n    detector_shape: Tuple[int, int],\n    detector_spacing: Tuple[float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = hamming(detector_width, detector_spacing_width)\n    filter_2D = np.tile(filter_1D, (number_of_projections, 1))\n\n    return filter_2D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hamming_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.hamming_3D","text":"<pre><code>hamming_3D(detector_shape: Tuple[int, int, int], detector_spacing: Tuple[float, float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hamming_3D(\n    detector_shape: Tuple[int, int, int],\n    detector_spacing: Tuple[float, float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = hamming(detector_width, detector_spacing_width)\n    filter_3D = np.tile(\n        filter_1D.reshape(1, 1, detector_width), (number_of_projections, 1, 1)\n    )\n\n    return filter_3D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hann","title":"pyronn.ct_reconstruction.helpers.filters.filters.hann","text":"<pre><code>hann(num_detectors: int, detector_spacing: float) -&gt; np.array\n</code></pre> <p>Generate the Hann filter in the frequency domain.</p> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hann(num_detectors: int, detector_spacing: float) -&gt; np.array:\n    \"\"\"Generate the Hann filter in the frequency domain.\"\"\"\n    frequencies = np.fft.fftfreq(num_detectors)\n    ramp = 1.0 / (detector_spacing * detector_spacing)\n    filter = ramp * np.abs(frequencies) * (0.5 + 0.5 * np.cos(np.pi * frequencies))\n\n    return filter.astype(np.float32)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hann_2D","title":"pyronn.ct_reconstruction.helpers.filters.filters.hann_2D","text":"<pre><code>hann_2D(detector_shape: Tuple[int, int], detector_spacing: Tuple[float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hann_2D(\n    detector_shape: Tuple[int, int],\n    detector_spacing: Tuple[float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = hann(detector_width, detector_spacing_width)\n    filter_2D = np.tile(filter_1D, (number_of_projections, 1))\n\n    return filter_2D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/filters/#pyronn.ct_reconstruction.helpers.filters.filters.hann_3D","title":"pyronn.ct_reconstruction.helpers.filters.filters.hann_3D","text":"<pre><code>hann_3D(detector_shape: Tuple[int, int, int], detector_spacing: Tuple[float, float, float], number_of_projections: int) -&gt; np.array\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/filters.py</code> <pre><code>def hann_3D(\n    detector_shape: Tuple[int, int, int],\n    detector_spacing: Tuple[float, float, float],\n    number_of_projections: int,\n) -&gt; np.array:\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    detector_width = detector_shape[-1]\n    detector_spacing_width = detector_spacing[-1]\n\n    filter_1D = hann(detector_width, detector_spacing_width)\n    filter_3D = np.tile(\n        filter_1D.reshape(1, 1, detector_width), (number_of_projections, 1, 1)\n    )\n\n    return filter_3D\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/weights/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/filters/weights.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/filters/weights/#pyronn.ct_reconstruction.helpers.filters.weights.cosine_weights_3d","title":"pyronn.ct_reconstruction.helpers.filters.weights.cosine_weights_3d","text":"<pre><code>cosine_weights_3d(geometry)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/weights.py</code> <pre><code>def cosine_weights_3d(geometry):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    cu = -(geometry.detector_shape[-1] - 1) / 2 * geometry.detector_spacing[-1]\n    cv = -(geometry.detector_shape[-2] - 1) / 2 * geometry.detector_spacing[-2]\n    sd2 = geometry.source_detector_distance ** 2\n\n    w = np.zeros((geometry.detector_shape[-2], geometry.detector_shape[-1]), dtype=np.float32)\n\n    for v in range(0, geometry.detector_shape[-2]):\n        dv = (v * geometry.detector_spacing[-2] + cv) ** 2\n        for u in range(0, geometry.detector_shape[-1]):\n            du = (u * geometry.detector_spacing[-1] + cu) ** 2\n            w[v, u] = geometry.source_detector_distance / np.sqrt(sd2 + dv + du)\n\n    return np.flip(w)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/weights/#pyronn.ct_reconstruction.helpers.filters.weights.parker_weights_3d","title":"pyronn.ct_reconstruction.helpers.filters.weights.parker_weights_3d","text":"<pre><code>parker_weights_3d(geometry)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/weights.py</code> <pre><code>def parker_weights_3d(geometry):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    weights = np.flip(parker_weights_2d(geometry), axis=1)\n    weights = np.array(np.expand_dims(weights, axis=1), dtype=np.float32)\n    return weights\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/weights/#pyronn.ct_reconstruction.helpers.filters.weights.parker_weights_2d","title":"pyronn.ct_reconstruction.helpers.filters.weights.parker_weights_2d","text":"<pre><code>parker_weights_2d(geometry)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/weights.py</code> <pre><code>def parker_weights_2d(geometry):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    number_of_projections = geometry.number_of_projections\n    angular_range = geometry.angular_range[1] - geometry.angular_range[0]\n    detector_shape = geometry.detector_shape\n    detector_spacing = geometry.detector_spacing\n    # detector_origin = geometry.detector_origin\n    source_detector_distance = geometry.source_detector_distance\n    fan_angle = geometry.fan_angle\n\n    weights = np.ones((number_of_projections, detector_shape[-1]))\n    angular_increment = angular_range / number_of_projections\n    beta = 0\n    beta = ((np.pi + 2*fan_angle) - angular_range) / 2.0 # adds offset\n\n    for beta_idx in range(weights.shape[0]):\n        for gamma_idx in range(weights.shape[1]):\n                # calculate correct pos on detector and current angle\n                gamma_angle = gamma_idx * detector_spacing[-1]# + detector_origin[-1]\n                gamma_angle = np.arctan(gamma_angle / source_detector_distance)\n\n                # check if rays sampled twice and create weight volume\n                if 0 &lt;= beta and beta &lt;= 2*(fan_angle - gamma_angle):\n                    val = np.sin( ((np.pi/4.0) * beta) / (fan_angle - gamma_angle) ) ** 2\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n                elif 2*(fan_angle - gamma_angle) &lt; beta and beta &lt; np.pi - 2*gamma_angle:\n                    weights[beta_idx, gamma_idx] = 1.0\n\n                elif np.pi - 2*gamma_angle &lt;= beta and beta &lt;= np.pi + 2*fan_angle:\n                    val = np.sin((np.pi/4.0) * ((np.pi + 2*fan_angle - beta) / (gamma_angle + fan_angle))) ** 2\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n                else:\n                    weights[beta_idx, gamma_idx] = 0\n\n        beta += angular_increment\n\n    # additional scaling factor\n    scale_factor = (angular_range + angular_range  / number_of_projections) / np.pi\n\n    return weights * scale_factor\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/filters/weights/#pyronn.ct_reconstruction.helpers.filters.weights.riess_weights_2d","title":"pyronn.ct_reconstruction.helpers.filters.weights.riess_weights_2d","text":"<pre><code>riess_weights_2d(geometry)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/filters/weights.py</code> <pre><code>def riess_weights_2d(geometry):\n\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    delta_x = geometry.angular_range - np.pi # overscan\n\n    def eta(beta, gamma_angle):\n        return np.sin( (np.pi/2.0) * (np.pi+delta_x-beta) / (delta_x-2*gamma_angle) ) ** 2\n\n    def zeta(beta, gamma_angle):\n        return np.sin( (np.pi/2.0) * beta / (delta_x+2*gamma_angle) ) ** 2\n\n    weights = np.ones((geometry.number_of_projections, geometry.detector_shape[-1]))\n    angular_increment = geometry.angular_range / geometry.number_of_projections\n    beta = 0\n\n    for beta_idx in range(weights.shape[0]):\n        for gamma_idx in range(weights.shape[1]):\n                # calculate correct pos on detector and current angle\n                gamma_angle = gamma_idx * geometry.detector_spacing[-1] + geometry.detector_origin[-1]\n                gamma_angle = np.arctan(gamma_angle / geometry.source_detector_distance)\n\n                if np.pi + 2*gamma_angle &lt;= beta and beta &lt;= np.pi + delta_x:\n                    val = eta(beta, gamma_angle)\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n                if np.pi + 2*(delta_x - gamma_angle) &lt;= beta and beta &lt;= np.pi + delta_x:\n                    val = 2 - eta(beta, gamma_angle)\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n                if 0 &lt;= beta and beta &lt;= 2*gamma_angle + delta_x:\n                    val = zeta(beta, gamma_angle)\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n                if 0 &lt;= beta and beta &lt;= -delta_x - 2*gamma_angle:\n                    val = 2 - zeta(beta, gamma_angle)\n                    if not np.isnan(val):\n                        weights[beta_idx, gamma_idx] = val\n\n        beta += angular_increment\n\n    # additional scaling factor\n    scale_factor = geometry.angular_range / np.pi\n    return weights * scale_factor\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/misc/general_utils/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/misc/general_utils.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/misc/general_utils/#pyronn.ct_reconstruction.helpers.misc.general_utils.fibonacci_sphere","title":"pyronn.ct_reconstruction.helpers.misc.general_utils.fibonacci_sphere","text":"<pre><code>fibonacci_sphere(n)\n</code></pre> <p>Calculation of the fibonacci distribution on a unit sphere with n samples. :param n: Number of samples on the sphere :return: The entered coordinates seperated to x,y,z components</p> Source code in <code>pyronn/ct_reconstruction/helpers/misc/general_utils.py</code> <pre><code>def fibonacci_sphere(n):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Calculation of the fibonacci distribution on a unit sphere with n samples.\n    :param n: Number of samples on the sphere\n    :return: The entered coordinates seperated to x,y,z components\n    '''\n    goldenRatio = (1 + 5**0.5)/2\n    i = np.arange(0, n)\n    theta = (2 *np.pi * i / goldenRatio)  %(2*np.pi) # to radian  # in range [0\u00b0,360\u00b0]\n    phi = np.arccos(1 - 2*(i+0.5)/n)  #/2  # in range [1\u00b0,179\u00b0]\n    x,z, y = np.cos(theta) * np.sin(phi), np.sin(theta) * np.sin(phi), np.cos(phi)\n    return np.vstack([x,y,z]).T\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/misc/general_utils/#pyronn.ct_reconstruction.helpers.misc.general_utils.rotation_matrix_from_points","title":"pyronn.ct_reconstruction.helpers.misc.general_utils.rotation_matrix_from_points","text":"<pre><code>rotation_matrix_from_points(p1, p2)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/misc/general_utils.py</code> <pre><code>def rotation_matrix_from_points(p1, p2):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    p1 = np.array(p1) / np.linalg.norm(p1)\n    p2 = np.array(p2) / np.linalg.norm(p2)\n\n    axis = np.cross(p1, p2)\n    axis_length = np.linalg.norm(axis)\n    if axis_length &lt; 1e-5:\n        if np.dot(p1, p2) &gt; 0:\n            return np.eye(3)\n        else:\n            axis = np.array([p1[1], -p1[0], 0])\n            if np.linalg.norm(axis) == 0:\n                axis = np.array([p1[2], 0, -p1[0]])\n            axis = axis / np.linalg.norm(axis)\n            theta = np.pi\n    else:\n        axis = axis / axis_length\n\n    cos_theta = np.dot(p1, p2)\n    sin_theta = np.sqrt(1 - cos_theta ** 2)\n\n    K = np.array([[0, -axis[2], axis[1]],\n                  [axis[2], 0, -axis[0]],\n                  [-axis[1], axis[0], 0]])\n    R = np.eye(3) + sin_theta * K + (1 - cos_theta) * np.dot(K, K)\n\n    return R\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/misc/general_utils/#pyronn.ct_reconstruction.helpers.misc.general_utils.fft_and_ifft","title":"pyronn.ct_reconstruction.helpers.misc.general_utils.fft_and_ifft","text":"<pre><code>fft_and_ifft(sinogram, filter)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/misc/general_utils.py</code> <pre><code>def fft_and_ifft(sinogram, filter):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    if pyronn.read_backend() == 'torch':\n        import torch\n        if not isinstance(sinogram, torch.Tensor):\n            sinogram = torch.tensor(sinogram).cuda()\n        if not isinstance(filter, torch.Tensor):\n            filter = torch.tensor(filter).cuda()\n\n        x = torch.fft.fft(sinogram, dim=-1, norm='ortho')\n        x = torch.multiply(x, filter)\n        x = torch.fft.ifft(x, dim=-1, norm='ortho').real\n        return x\n    elif pyronn.read_backend() == 'tensorflow':\n        import tensorflow as tf\n        sino_freq = tf.signal.fft(tf.cast(sinogram, dtype=tf.complex64))\n        sino_filtered_freq = tf.multiply(sino_freq, tf.cast(filter, dtype=tf.complex64))\n        sinogram_filtered = tf.math.real(tf.signal.ifft(sino_filtered_freq))\n        return sinogram_filtered\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_2d/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_2d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.circle","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.circle","text":"<pre><code>circle(shape, pos, radius, value=1.0)\n</code></pre> <pre><code>Creates a simple circle primitive.\n</code></pre> <p>Args:     shape:      Shape (in [Y, X])     pos:        Center (in [Y, X]) from upper left corner     radius:     Radius     value:      Value</p> <p>Returns:</p> Type Description <p>np.array filled with circle</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_2d.py</code> <pre><code>def circle(shape, pos, radius, value=1.0):\n    \"\"\"\n        Creates a simple circle primitive.\n    Args:\n        shape:      Shape (in [Y, X])\n        pos:        Center (in [Y, X]) from upper left corner\n        radius:     Radius\n        value:      Value\n\n    Returns:\n        np.array filled with circle\n    \"\"\"\n    # create meshgrid of coords\n    xx, yy = np.mgrid[:shape[0], :shape[1]].astype(dtype=np.float32)\n\n    # calc squared distance to pos\n    circle = (xx - pos[1]) ** 2 + (yy - pos[0]) ** 2\n\n    return (circle &lt;= radius ** 2) *  (np.float32)(value)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_2d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.ellipse","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.ellipse","text":"<pre><code>ellipse(shape, pos, half_axes, value=1.0, phi=0.0)\n</code></pre> <pre><code>Creates a simple ellipse primitive.\n</code></pre> <p>Args:     shape:          Shape (in [Y, X])     pos:            Center (in [Y, X]) from upper left corner     half_axes:      Half axes of the ellipse (in [b, a])     value:          Value     phi:            Rotation Angle in radians</p> <p>Returns:</p> Type Description <p>np.array filled with ellipse</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_2d.py</code> <pre><code>def ellipse(shape, pos, half_axes, value=1.0, phi=0.0):\n    \"\"\"\n        Creates a simple ellipse primitive.\n    Args:\n        shape:          Shape (in [Y, X])\n        pos:            Center (in [Y, X]) from upper left corner\n        half_axes:      Half axes of the ellipse (in [b, a])\n        value:          Value\n        phi:            Rotation Angle in radians\n\n    Returns:\n        np.array filled with ellipse\n    \"\"\"\n    # create meshgrid of coords\n    xx, yy = np.mgrid[:shape[0], :shape[1]].astype(dtype=np.float32)\n\n    # move to pos\n    xc = (xx - pos[1])\n    yc = (yy - pos[0])\n\n    # rotate\n    xx = xc * np.cos(phi) + yc * np.sin(phi)\n    yy = yc * np.cos(phi) - xc * np.sin(phi)\n\n    a = half_axes[1]\n    b = half_axes[0]\n\n    # calc squared distance to pos\n    ellipse_points = (xx ** 2) / (a ** 2) + (yy ** 2) / (b ** 2)\n\n    return (ellipse_points &lt;= 1) * value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_2d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.rect","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_2d.rect","text":"<pre><code>rect(shape, pos, size, value=1.0)\n</code></pre> <pre><code>Creates a simple rect primitive.\n</code></pre> <p>Args:     shape:      Shape (in [Y, X])     pos:        Pos (upper left corner) (in [Y, X]) from upper left corner     size:       Size  (in [Y, X])     value:      Value</p> <p>Returns:</p> Type Description <p>np.array filled with rectangle</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_2d.py</code> <pre><code>def rect(shape, pos, size, value=1.0):\n    \"\"\"\n        Creates a simple rect primitive.\n    Args:\n        shape:      Shape (in [Y, X])\n        pos:        Pos (upper left corner) (in [Y, X]) from upper left corner\n        size:       Size  (in [Y, X])\n        value:      Value\n\n    Returns:\n        np.array filled with rectangle\n    \"\"\"\n    # create array and populate it with value\n    rectangle = np.zeros(shape, dtype=np.float32)\n    rectangle[pos[0]:pos[0] + size[0], pos[1]:pos[1] + size[1]] = value\n\n    return rectangle\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_sphere","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_sphere","text":"<pre><code>place_sphere(grid, pos, radius, value=1.0)\n</code></pre> <p>Updates an existing 3D grid by placing a spherical object within it.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>The existing 3D numpy array (meshgrid) to update.</p> required <code>pos</code> <p>Center of the sphere (in [Z, Y, X]).</p> required <code>radius</code> <p>Radius of the sphere.</p> required <code>value</code> <p>Value to fill the sphere with.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>None; the function updates the grid in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_sphere(grid, pos, radius, value=1.0):\n    \"\"\"\n    Updates an existing 3D grid by placing a spherical object within it.\n\n    Args:\n        grid:   The existing 3D numpy array (meshgrid) to update.\n        pos:    Center of the sphere (in [Z, Y, X]).\n        radius: Radius of the sphere.\n        value:  Value to fill the sphere with.\n\n    Returns:\n        None; the function updates the grid in place.\n    \"\"\"\n\n    # Ensure the grid is a numpy array\n    grid = np.asarray(grid)\n\n    # Grid shape\n    shape = grid.shape\n\n    # Create meshgrid of coords based on the grid's shape\n    xx, yy, zz = np.mgrid[0:shape[0], 0:shape[1], 0:shape[2]]\n\n    # Calculate squared distance to pos\n    circle = (xx - pos[2])**2 + (yy - pos[1])**2 + (zz - pos[0])**2\n\n    # Update grid in place where the condition is met\n    grid[circle &lt;= radius**2] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cube","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cube","text":"<pre><code>place_cube(grid, pos, size, value=1.0)\n</code></pre> <p>Updates an existing 3D grid by placing a cube object within it.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>The existing 3D numpy array (meshgrid) to update.</p> required <code>pos</code> <p>Position of the cube's upper left corner (in [Z, Y, X]).</p> required <code>size</code> <p>Size of the cube (in [Z, Y, X]).</p> required <code>value</code> <p>Value to fill the cube with.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>None; the function updates the grid in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_cube(grid, pos, size, value=1.0):\n    \"\"\"\n    Updates an existing 3D grid by placing a cube object within it.\n\n    Args:\n        grid:   The existing 3D numpy array (meshgrid) to update.\n        pos:    Position of the cube's upper left corner (in [Z, Y, X]).\n        size:   Size of the cube (in [Z, Y, X]).\n        value:  Value to fill the cube with.\n\n    Returns:\n        None; the function updates the grid in place.\n    \"\"\"\n    # Ensure pos and size are within the grid bounds\n    for i in range(3):\n        if pos[i] &lt; 0 or pos[i] + size[i] &gt; grid.shape[i]:\n            raise ValueError(f\"Cube at position {pos} with size {size} exceeds grid bounds.\")\n\n    # Update the grid in place\n    grid[pos[0]:pos[0]+size[0], pos[1]:pos[1]+size[1], pos[2]:pos[2]+size[2]] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.rotate_point_around_z","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.rotate_point_around_z","text":"<pre><code>rotate_point_around_z(x, y, z, angle)\n</code></pre> <p>Rotate a point around the Z-axis by a given angle.</p> <p>Parameters:</p> Name Type Description Default <code>x,</code> <code>(y, z)</code> <p>Coordinates of the point.</p> required <code>angle</code> <p>Rotation angle in radians.</p> required <p>Returns:</p> Type Description <p>Rotated coordinates (x', y', z').</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def rotate_point_around_z(x, y, z, angle):\n    \"\"\"\n    Rotate a point around the Z-axis by a given angle.\n\n    Args:\n        x, y, z: Coordinates of the point.\n        angle: Rotation angle in radians.\n\n    Returns:\n        Rotated coordinates (x', y', z').\n    \"\"\"\n    cos_angle = np.cos(angle)\n    sin_angle = np.sin(angle)\n    x_rotated = x * cos_angle - y * sin_angle\n    y_rotated = x * sin_angle + y * cos_angle\n    return x_rotated, y_rotated, z\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cube_with_rotation","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cube_with_rotation","text":"<pre><code>place_cube_with_rotation(grid, pos, size, value=1.0, angle=None)\n</code></pre> <p>Updates an existing 3D grid by placing a cube object within it, potentially rotated around the Z-axis.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>The existing 3D numpy array (meshgrid) to update.</p> required <code>pos</code> <p>Position of the cube's upper left corner (in [Z, Y, X]).</p> required <code>size</code> <p>Size of the cube (in [Z, Y, X]).</p> required <code>value</code> <p>Value to fill the cube with.</p> <code>1.0</code> <code>angle</code> <p>Rotation angle in radians around the Z-axis. If None, a random angle is chosen.</p> <code>None</code> <p>Returns:</p> Type Description <p>None; the function updates the grid in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_cube_with_rotation(grid, pos, size, value=1.0, angle=None):\n    \"\"\"\n    Updates an existing 3D grid by placing a cube object within it, potentially rotated around the Z-axis.\n\n    Args:\n        grid:   The existing 3D numpy array (meshgrid) to update.\n        pos:    Position of the cube's upper left corner (in [Z, Y, X]).\n        size:   Size of the cube (in [Z, Y, X]).\n        value:  Value to fill the cube with.\n        angle:  Rotation angle in radians around the Z-axis. If None, a random angle is chosen.\n\n    Returns:\n        None; the function updates the grid in place.\n    \"\"\"\n    if angle is None:\n        angle = np.random.uniform(0, 2*np.pi)  # Random angle if not specified\n\n    xx,yy,zz = np.mgrid[0:grid.shape[0], 0:grid.shape[1], 0:grid.shape[2]]\n\n    # Center of the cube\n    center = np.array(pos) + np.array(size) / 2\n\n    # Rotate grid points around the Z-axis in the opposite direction\n    try:\n        xx_rotated, yy_rotated, _ = rotate_point_around_z(xx - center[2], yy - center[1], 0, -angle)\n        # Adjust back to grid coordinates\n        xx_rotated += center[2]\n        yy_rotated += center[1]\n         # Check if rotated points are inside the unrotated cube's bounds\n        inside_cube = (\n            (xx_rotated &gt;= pos[2]) &amp; (xx_rotated &lt;= pos[2] + size[2]) &amp;\n            (yy_rotated &gt;= pos[1]) &amp; (yy_rotated &lt;= pos[1] + size[1]) &amp;\n            (zz &gt;= pos[0]) &amp; (zz &lt;= pos[0] + size[0])\n        )\n\n        # Update grid where the condition is met\n        grid[inside_cube] = value\n    except ValueError:\n        # Calculate start and end indices for each dimension, ensuring they are within the grid bounds\n        start_indices = [max(0, p) for p in pos]\n        end_indices = [min(pos[i] + size[i], grid.shape[i]) for i in range(3)]\n\n        # Update the grid within the calculated indices\n        grid[\n            start_indices[0]:end_indices[0],\n            start_indices[1]:end_indices[1],\n            start_indices[2]:end_indices[2]\n        ] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_ellipsoid_with_rotation","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_ellipsoid_with_rotation","text":"<pre><code>place_ellipsoid_with_rotation(grid, pos, radii, value=1.0, angle=None)\n</code></pre> <p>Updates an existing 3D grid by placing a randomly rotated ellipsoid object within it.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>The existing 3D numpy array (meshgrid) to update.</p> required <code>pos</code> <p>Center of the ellipsoid (in [Z, Y, X]).</p> required <code>radii</code> <p>Radii of the ellipsoid (in [Z, Y, X]).</p> required <code>value</code> <p>Value to fill the ellipsoid with.</p> <code>1.0</code> <code>angle</code> <p>Rotation angle in radians around the Z-axis. If None, a random angle is chosen.</p> <code>None</code> <p>Returns:</p> Type Description <p>None; the function updates the grid in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_ellipsoid_with_rotation(grid, pos, radii, value=1.0, angle=None):\n    \"\"\"\n    Updates an existing 3D grid by placing a randomly rotated ellipsoid object within it.\n\n    Args:\n        grid:   The existing 3D numpy array (meshgrid) to update.\n        pos:    Center of the ellipsoid (in [Z, Y, X]).\n        radii:  Radii of the ellipsoid (in [Z, Y, X]).\n        value:  Value to fill the ellipsoid with.\n        angle:  Rotation angle in radians around the Z-axis. If None, a random angle is chosen.\n\n    Returns:\n        None; the function updates the grid in place.\n    \"\"\"\n    if angle is None:\n        angle = np.random.uniform(0, 2*np.pi)  # Random angle if not specified\n\n    zz, yy, xx = np.mgrid[0:grid.shape[0], 0:grid.shape[1], 0:grid.shape[2]]\n\n    # Rotate grid points around the Z-axis in the opposite direction\n    xx_rotated, yy_rotated, _ = rotate_point_around_z(xx - pos[2], yy - pos[1], 0, -angle)\n\n    # Adjust back to original positions\n    xx_rotated += pos[2]\n    yy_rotated += pos[1]\n\n    # Check if rotated points are inside the ellipsoid\n    inside_ellipsoid = (\n        ((xx_rotated - pos[2])**2 / radii[2]**2) +\n        ((yy_rotated - pos[1])**2 / radii[1]**2) +\n        ((zz - pos[0])**2 / radii[0]**2)\n    ) &lt;= 1\n\n    # Update grid where the condition is met\n    grid[inside_ellipsoid] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cylinder_with_rotation","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_cylinder_with_rotation","text":"<pre><code>place_cylinder_with_rotation(grid, pos, height, radius, value=1.0, angle=None)\n</code></pre> <p>Updates an existing 3D grid by placing a randomly rotated cylinder object within it. Args:     grid: The existing 3D numpy array (meshgrid) to update.     pos: Center of the base of the cylinder (in [Z, Y, X]).     height: Height of the cylinder along the Z-axis.     radius: Radius of the cylinder in the XY plane.     value: Value to fill the cylinder with.     angle: Rotation angle in radians around the Z-axis. If None, a random angle is chosen. Returns:     None; the function updates the grid in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_cylinder_with_rotation(grid, pos, height, radius, value=1.0, angle=None):\n    \"\"\"\n    Updates an existing 3D grid by placing a randomly rotated cylinder object within it.\n    Args:\n        grid: The existing 3D numpy array (meshgrid) to update.\n        pos: Center of the base of the cylinder (in [Z, Y, X]).\n        height: Height of the cylinder along the Z-axis.\n        radius: Radius of the cylinder in the XY plane.\n        value: Value to fill the cylinder with.\n        angle: Rotation angle in radians around the Z-axis. If None, a random angle is chosen.\n    Returns:\n        None; the function updates the grid in place.\n    \"\"\"\n    if angle is None:\n        angle = np.random.uniform(0, 2*np.pi)  # Random angle if not specified\n\n    # Create meshgrid\n    zz, yy, xx = np.mgrid[0:grid.shape[0], 0:grid.shape[1], 0:grid.shape[2]]\n\n    # Rotate grid points around the Z-axis in the opposite direction to simulate cylinder rotation\n    xx_rotated, yy_rotated, _ = rotate_point_around_z(xx - pos[2], yy - pos[1], zz, -angle)\n\n    # Adjust back to original positions\n    xx_rotated += pos[2]\n    yy_rotated += pos[1]\n\n    # Check if rotated points are within the cylinder\n    inside_cylinder = ((xx_rotated - pos[2])**2 + (yy_rotated - pos[1])**2 &lt;= radius**2) &amp;                      (zz &gt;= pos[0]) &amp; (zz &lt;= pos[0] + height)\n\n    # Update grid where the condition is met\n    grid[inside_cylinder] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_pyramid","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.place_pyramid","text":"<pre><code>place_pyramid(grid, base_center, base_size, height, value=1.0)\n</code></pre> <p>Places an axis-aligned pyramid into a 3D grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>3D numpy array representing the grid.</p> required <code>base_center</code> <p>Center of the pyramid's base in the grid (z, y, x).</p> required <code>base_size</code> <p>Side length of the pyramid's square base.</p> required <code>height</code> <p>Height of the pyramid.</p> required <code>value</code> <p>Value to fill the pyramid with.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>None; the grid is modified in place.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def place_pyramid(grid, base_center, base_size, height, value=1.0):\n    \"\"\"\n    Places an axis-aligned pyramid into a 3D grid.\n\n    Args:\n        grid: 3D numpy array representing the grid.\n        base_center: Center of the pyramid's base in the grid (z, y, x).\n        base_size: Side length of the pyramid's square base.\n        height: Height of the pyramid.\n        value: Value to fill the pyramid with.\n\n    Returns:\n        None; the grid is modified in place.\n    \"\"\"\n    zz, yy, xx = np.mgrid[0:grid.shape[0], 0:grid.shape[1], 0:grid.shape[2]]\n\n    # Calculate distances from the base center in the XY plane\n    dx = np.abs(xx - base_center[2])\n    dy = np.abs(yy - base_center[1])\n\n    # Calculate the maximum allowable distance in the XY plane at each Z level\n    max_dist = base_size / 2 * (1 - zz / height)\n\n    # Determine points inside the pyramid\n    inside_pyramid = (dx &lt;= max_dist) &amp; (dy &lt;= max_dist) &amp; (zz &lt;= height)\n\n    # Update the grid\n    grid[inside_pyramid] = value\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.visualize_grid","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.visualize_grid","text":"<pre><code>visualize_grid(grid)\n</code></pre> <p>Visualizes a 3D grid using matplotlib, plotting points where the grid value is non-zero.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <p>The 3D numpy array to visualize.</p> required Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def visualize_grid(grid):\n    \"\"\"\n    Visualizes a 3D grid using matplotlib, plotting points where the grid value is non-zero.\n\n    Args:\n        grid: The 3D numpy array to visualize.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Get the coordinates of points where the grid value is non-zero\n    z, y, x = grid.nonzero()\n\n    # Use scatter plot for these points\n    ax.scatter(x, y, z, c='red', marker='o')\n\n    # Set labels and title\n    ax.set_xlabel('X Axis')\n    ax.set_ylabel('Y Axis')\n    ax.set_zlabel('Z Axis')\n    ax.set_title('3D Grid Visualization')\n\n    # plt.show()\n    plt.savefig(f'random_object.png', dpi=150, transparent=False, bbox_inches='tight')\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/primitives_3d/#pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.generate_3D_primitives","title":"pyronn.ct_reconstruction.helpers.phantoms.primitives_3d.generate_3D_primitives","text":"<pre><code>generate_3D_primitives(volume_shape, number_of_primitives)\n</code></pre> <p>Generates a 3D phantom composed of a specified number of random geometric primitives. The primitives are added to the phantom with random positions, orientations, sizes, and intensities. The method returns both the phantom and its sinogram as PyTorch tensors.</p> <p>Parameters: - volume_shape: shape of the phantom - number_of_primitives (int, optional): The number of geometric primitives to include in the phantom. Defaults to 6.</p> <p>Returns: - numpy.array: The 3D phantom.</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/primitives_3d.py</code> <pre><code>def generate_3D_primitives(volume_shape, number_of_primitives):\n    \"\"\"\n    Generates a 3D phantom composed of a specified number of random geometric primitives. The primitives are added\n    to the phantom with random positions, orientations, sizes, and intensities. The method returns both\n    the phantom and its sinogram as PyTorch tensors.\n\n    Parameters:\n    - volume_shape: shape of the phantom\n    - number_of_primitives (int, optional): The number of geometric primitives to include in the phantom.\n    Defaults to 6.\n\n    Returns:\n    - numpy.array: The 3D phantom.\n    \"\"\"\n    grid = np.zeros(volume_shape)\n\n    for i in range(number_of_primitives):\n        object_type = random.choice([\"ellipsoid\", \"sphere\", \"cube\", \"pyramid\", \"cylinder\", \"rectangle\"])\n        pos = np.random.randint(0, volume_shape[0], 3)\n        intensitiy_value = np.random.uniform(0.4, 1.0, 1)\n        print(f\"{i}th Random choice was {object_type}, placed at {pos} with intensity {intensitiy_value}.\")\n\n        if object_type == \"ellipsoid\":\n            ellipsoid_radii = np.random.randint(1, int(volume_shape[0] / 5),\n                                                3)  # Radii along Z, Y, X axes\n            place_ellipsoid_with_rotation(grid, pos, ellipsoid_radii, value=intensitiy_value)\n        elif object_type == \"sphere\":\n            radius = np.random.randint(1, int(volume_shape[0] / 5), 1)  # Radius\n            place_sphere(grid, pos, radius, value=intensitiy_value)\n        elif object_type == 'rectangle':\n            cube_size = np.random.randint(1, int(volume_shape[0] / 5), 3)  # Size of the cube\n            place_cube_with_rotation(grid, pos, cube_size, value=intensitiy_value)\n        elif object_type == 'cube':\n            cube_size = np.random.randint(1, int(volume_shape[0] / 5), 1)  # Size of the cube\n            place_cube_with_rotation(grid, pos, (cube_size[0], cube_size[0], cube_size[0]), value=intensitiy_value)\n        elif object_type == 'pyramid':\n            base_size = np.random.randint(1, int(volume_shape[0] / 5), 1)  # Length of the base's side\n            height = np.random.randint(1, int(volume_shape[0] / 5), 1)  # Height of the pyramid\n            # Place the pyramid in the grid\n            place_pyramid(grid, pos, base_size, height, intensitiy_value)\n        else:  # 'cylinder'\n            cylinder_height = np.random.randint(1, int(volume_shape[0] / 5), 1)\n            cylinder_radius = np.random.randint(1, int(volume_shape[0] / 5), 1)\n            place_cylinder_with_rotation(grid, pos, cylinder_height, cylinder_radius, value=intensitiy_value)\n    return grid\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/shepp_logan/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/phantoms/shepp_logan.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/phantoms/shepp_logan/#pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan","title":"pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan","text":"<pre><code>shepp_logan(shape)\n</code></pre> <pre><code>Creates the Shepp Logan Phantom.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>shape</code> <p>Shape (in [Y, X]) of Shepp Logan phantom to create.</p> required <p>Returns:</p> Type Description <p>Shepp Logan of shape as np.array</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/shepp_logan.py</code> <pre><code>def shepp_logan(shape):\n    \"\"\"\n        Creates the Shepp Logan Phantom.\n\n    Args:\n        shape: Shape (in [Y, X]) of Shepp Logan phantom to create.\n\n    Returns:\n        Shepp Logan of shape as np.array\n\n    \"\"\"\n    # Definition of Shepp Logan Phantom\n    # Ellipse\tCenter\t         Major Axis    Minor Axis    Phi     Gray Level\n    # a\t        (0,0)\t         0.69\t       0.92\t         0\t     2\n    # b\t        (0,-0.0184)\t     0.6624\t       0.874\t     0\t     -0.98\n    # c\t        (0.22,0)\t     0.11\t       0.31\t         -18\u00b0\t -0.02\n    # d\t        (-0.22,0)\t     0.16\t       0.41\t         18\u00b0\t -0.02\n    # e\t        (0,0.35)\t     0.21\t       0.25\t         0\t     0.01\n    # f\t        (0,0.1)\t         0.046\t       0.046\t     0\t     0.01\n    # g\t        (0,-0.1)\t     0.046\t       0.046\t     0\t     0.01\n    # h\t        (-0.08,-0.605)\t 0.046\t       0.023\t     0\t     0.01\n    # i\t        (0,-0.605)\t     0.023\t       0.023\t     0\t     0.01\n    # j\t        (0.06,-0.605)\t 0.023\t       0.046\t     0\t     0.01\n    shepp_logan = np.zeros(shape)\n\n    # create meshgrid of coords\n    yy_base, xx_base = np.mgrid[:shape[0], :shape[1]]\n\n    # center at 0, 0 and normalize\n    xx_base = (xx_base - (shape[1] - 1) / 2) / ((shape[1] - 1) / 2)\n    yy_base = (yy_base - (shape[0] - 1) / 2) / ((shape[0] - 1) / 2)\n\n    # definition of ellipses as np.array:\n    el_params = np.array([[0     ,0       ,0.69     ,0.92 ,0              ,2     ],\n                          [0     ,-0.0184 ,0.6624 ,0.874 ,0              ,-0.98 ],\n                          [0.22  ,0       ,0.11     ,0.31 ,np.radians(-18) ,-0.02 ],\n                          [-0.22 ,0       ,0.16     ,0.41 ,np.radians( 18) ,-0.02 ],\n                          [0     ,0.35   ,0.21     ,0.25 ,0              ,0.01  ],\n                          [0     ,0.1     ,0.046 ,0.046 ,0              ,0.01  ],\n                          [0     ,-0.1    ,0.046 ,0.046 ,0              ,0.01  ],\n                          [-0.08 ,-0.605  ,0.046 ,0.023 ,0              ,0.01  ],\n                          [0     ,-0.605  ,0.023 ,0.023 ,0              ,0.01  ],\n                          [0.06  ,-0.605  ,0.023 ,0.046 ,0              ,0.01  ]])\n\n    # create ellipses and sum up\n    for i in range(el_params.shape[0]):\n        # get params:\n        x_pos = el_params[i][0]\n        y_pos = el_params[i][1]\n        a     = el_params[i][2]\n        b     = el_params[i][3]\n        phi   = el_params[i][4]\n        value = el_params[i][5]\n\n        # move to pos\n        xc = (xx_base - x_pos)\n        yc = (yy_base - y_pos)\n\n        # rotate\n        xx = xc * np.cos(phi) + yc * np.sin(phi)\n        yy = yc * np.cos(phi) - xc * np.sin(phi)\n\n        # calc squared distance to pos\n        ellipse_points = (xx ** 2) / (a ** 2) + (yy ** 2) / (b ** 2)\n\n        # sum up\n        shepp_logan = shepp_logan + (ellipse_points &lt;= 1) * value\n\n    return np.flip(shepp_logan, axis=0)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/shepp_logan/#pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan_enhanced","title":"pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan_enhanced","text":"<pre><code>shepp_logan_enhanced(shape)\n</code></pre> <pre><code>Creates a contrast enhanced Shepp Logan Phantom.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>shape</code> <p>Shape (in [Y, X]) of phantom to create.</p> required <p>Returns:</p> Type Description <p>Phantom of shape as np.array</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/shepp_logan.py</code> <pre><code>def shepp_logan_enhanced(shape):\n    \"\"\"\n        Creates a contrast enhanced Shepp Logan Phantom.\n\n    Args:\n        shape: Shape (in [Y, X]) of phantom to create.\n\n    Returns:\n        Phantom of shape as np.array\n\n    \"\"\"\n    shepp_logan = np.zeros(shape, dtype=np.float32)\n\n    # create meshgrid of coords\n    yy_base, xx_base = np.mgrid[:shape[0], :shape[1]]\n\n    # center at 0, 0 and normalize\n    xx_base = (xx_base - (shape[1] - 1) / 2) / ((shape[1] - 1) / 2)\n    yy_base = (yy_base - (shape[0] - 1) / 2) / ((shape[0] - 1) / 2)\n\n    # definition of ellipses with enhanced contrast values as np.array:\n    el_params = np.array([[0     ,0       ,0.69     ,0.92 ,0              ,1    ],\n                          [0     ,-0.0184 ,0.6624 ,0.874 ,0              ,-0.8 ],\n                          [0.22  ,0       ,0.11     ,0.31 ,np.radians(-18) ,-0.2 ],\n                          [-0.22 ,0       ,0.16     ,0.41 ,np.radians( 18) ,-0.2 ],\n                          [0     ,0.35   ,0.21     ,0.25 ,0              ,0.1  ],\n                          [0     ,0.1     ,0.046 ,0.046 ,0              ,0.1  ],\n                          [0     ,-0.1    ,0.046 ,0.046 ,0              ,0.1  ],\n                          [-0.08 ,-0.605  ,0.046 ,0.023 ,0              ,0.1  ],\n                          [0     ,-0.605  ,0.023 ,0.023 ,0              ,0.1  ],\n                          [0.06  ,-0.605  ,0.023 ,0.046 ,0              ,0.1  ]])\n\n    # create ellipses and sum up\n    for i in range(el_params.shape[0]):\n        # get params:\n        x_pos = el_params[i][0]\n        y_pos = el_params[i][1]\n        a     = el_params[i][2]\n        b     = el_params[i][3]\n        phi   = el_params[i][4]\n        value = el_params[i][5]\n\n        # move to pos\n        xc = (xx_base - x_pos)\n        yc = (yy_base - y_pos)\n\n        # rotate\n        xx = xc * np.cos(phi) + yc * np.sin(phi)\n        yy = yc * np.cos(phi) - xc * np.sin(phi)\n\n        # calc squared distance to pos\n        ellipse_points = (xx ** 2) / (a ** 2) + (yy ** 2) / (b ** 2)\n\n        # sum up\n        shepp_logan = shepp_logan + (ellipse_points &lt;= 1) * value\n\n    return np.flip(shepp_logan, axis=0)\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/phantoms/shepp_logan/#pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan_3d","title":"pyronn.ct_reconstruction.helpers.phantoms.shepp_logan.shepp_logan_3d","text":"<pre><code>shepp_logan_3d(shape)\n</code></pre> <pre><code>Creates a Shepp Logan like 3d Phantom. Definition adopted from CONRAD.\n</code></pre> <p>Args:     shape: Shape (in [Y, X]) of Shepp Logan phantom to create.</p> <p>Returns:</p> Type Description <p>Phantom of shape as np.array</p> Source code in <code>pyronn/ct_reconstruction/helpers/phantoms/shepp_logan.py</code> <pre><code>def shepp_logan_3d(shape):\n    \"\"\"\n        Creates a Shepp Logan like 3d Phantom. Definition adopted from CONRAD.\n    Args:\n        shape: Shape (in [Y, X]) of Shepp Logan phantom to create.\n\n    Returns:\n        Phantom of shape as np.array\n    \"\"\"\n    shepp_logan = np.zeros(shape, dtype=np.float32)\n\n    # create meshgrid of coords\n    zz_base, yy_base, xx_base = np.mgrid[:shape[0], :shape[1], :shape[2]]\n\n    # center at 0, 0 and normalize\n    xx_base = (xx_base - (shape[2] - 1) / 2) / ((shape[2] - 1) / 2)\n    yy_base = (yy_base - (shape[1] - 1) / 2) / ((shape[1] - 1) / 2)\n    zz_base = (zz_base - (shape[0] - 1) / 2) / ((shape[0] - 1) / 2)\n\n    # definition of ellipsoids as np.array:\n    #                       delta_x, delta_y, delta_z,        a,       b,       c,            phi,  theta,  psi,     rho\n    el_params = np.array([[       0,       0,       0,     0.69,    0.92,    0.81,              0,      0,    0,     1  ],\n                          [       0, -0.0184,       0,   0.6624,   0.874,    0.78,              0,      0,    0,   -0.8 ],\n                          [    0.22,       0,       0,     0.11,    0.31,    0.22,   -(np.pi)/10.,      0,    0,   -0.2 ],\n                          [   -0.22,       0,       0,     0.16,    0.41,    0.28,    (np.pi)/10.,      0,    0,   -0.2 ],\n                          [       0,    0.35,   -0.15,     0.21,    0.25,    0.41,              0,      0,    0,    0.1 ],\n                          [       0,     0.1,    0.25,    0.046,   0.046,    0.05,              0,      0,    0,    0.1 ],\n                          [       0,    -0.1,    0.25,    0.046,   0.046,    0.05,              0,      0,    0,    0.1 ],\n                          [   -0.08,  -0.605,       0,    0.046,   0.023,    0.05,              0,      0,    0,    0.1 ],\n                          [       0,  -0.605,       0,    0.023,   0.023,    0.02,              0,      0,    0,    0.1 ],\n                          [    0.06,  -0.605,       0,    0.023,   0.046,    0.02,              0,      0,    0,    0.1 ]])\n\n    # create ellipses and sum up\n    for i in range(el_params.shape[0]):\n        # get params:\n        x_pos  = el_params[i][0]\n        y_pos  = el_params[i][1]\n        z_pos  = el_params[i][2]\n        a_axis = el_params[i][3]\n        b_axis = el_params[i][4]\n        c_axis = el_params[i][5]\n        phi    = el_params[i][6]\n        value  = el_params[i][9]\n\n        # move to pos\n        xc = (xx_base - x_pos)\n        yc = (yy_base - y_pos)\n        zc = (zz_base - z_pos)\n\n        # Rotation\n        c = np.cos(phi)\n        s = np.sin(phi)\n        Rz_phi   = np.array([[ c, -s,  0 ],\n                             [ s,  c,  0 ],\n                             [ 0,  0,  1 ]])\n        c = np.cos(0)\n        s = np.sin(0)\n        Ry_theta = np.array([[ c,  0,  s ],\n                             [ 0,  1,  0 ],\n                             [-s,  0,  c ]])\n        c = np.cos(0)\n        s = np.sin(0)\n        Rz_psi   = np.array([[ c, -s,  0 ],\n                             [ s,  c,  0 ],\n                             [ 0,  0,  1 ]])\n\n        # R = Rz(phi) * Ry(theta) * Rz(psi)\n        R = np.dot(np.dot(Rz_phi, Ry_theta), Rz_psi).T\n\n        xx = xc * R[0, 0] + yc * R[0, 1] + zc * R[0, 2]\n        yy = xc * R[1, 0] + yc * R[1, 1] + zc * R[1, 2]\n        zz = xc * R[2, 0] + yc * R[2, 1] + zc * R[2, 2]\n\n        # calc squared distance to pos\n        ellipse_points = (xx ** 2) / (a_axis ** 2) + (yy ** 2) / (b_axis ** 2) + (zz ** 2) / (c_axis ** 2)\n\n        # sum up\n        shepp_logan = shepp_logan + (ellipse_points &lt;= 1) * value\n\n    return shepp_logan\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/trajectories/arbitrary_trajectory/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/trajectories/arbitrary_trajectory.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/trajectories/arbitrary_trajectory/#pyronn.ct_reconstruction.helpers.trajectories.arbitrary_trajectory.arbitrary_projection_matrix","title":"pyronn.ct_reconstruction.helpers.trajectories.arbitrary_trajectory.arbitrary_projection_matrix","text":"<pre><code>arbitrary_projection_matrix(headers, voxel_size=[0.45, 0.45], swap_detector_axis=False, **kwargs)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/trajectories/arbitrary_trajectory.py</code> <pre><code>def arbitrary_projection_matrix(headers,voxel_size = [0.45,0.45], swap_detector_axis=False, **kwargs):\n    #Source: Auto-calibration of cone beam geometries from arbitrary rotating markers using a vector geometry formulation of projection matrices by Graetz, Jonas\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    number_of_projections = len(headers)\n    # init empty\n    projection_matrices = np.zeros((number_of_projections, 3, 4))\n\n    detector_shape = np.array(\n            [headers[0].number_vertical_pixels, headers[0].number_horizontal_pixels])\n\n    # Shift into left upper corner of the detector\n    detector_left_corner_trans = np.eye(3) \n    detector_left_corner_trans[0, 2] = + (float(headers[0].number_vertical_pixels) - 1.) / 2.\n    detector_left_corner_trans[1, 2] = + (float( headers[0].number_horizontal_pixels) - 1.) / 2.\n    detector_left_corner_trans[0, 0] *= 1\n    detector_left_corner_trans[1, 1] *= -1\n    detector_left_corner_trans[2, 2] = 1.\n    traj_type = 'circ' if np.array_equal(np.array(headers[0].agv_source_position),np.array([0,0,0])) else 'free'\n    print(traj_type)\n    #Initial stuff for circular trajectory:\n    if traj_type == 'circ':\n        init_source_position = [0, 0, headers[0].focus_object_distance_in_mm]\n        init_detector_position = [0, 0, headers[0].focus_object_distance_in_mm - headers[0].focus_detector_distance_in_mm]\n        init_detector_line_direction = [0,1,0]\n        init_detector_column_direction = [1,0,0]\n        angular_range = headers[0].scan_range_in_rad\n        if angular_range == 0:\n            angular_range = 2 * np.pi\n        current_angle = 0 \n        angular_increment = angular_range/number_of_projections\n\n    for p, header in enumerate(headers): \n        if traj_type == 'free':\n            det_h = np.array(header.agv_detector_line_direction)\n            det_v = -1* np.array(header.agv_detector_col_direction)\n            source_center_in_voxel = (np.array(header.agv_source_position)/1000)/voxel_size[0] # in mm\n            detector_center_in_voxel  = (np.array(header.agv_detector_center_position)/1000)/voxel_size[0] # in mm\n        else:\n            # rotation about x axis =&gt; Column direction of the detector\n            R_x_axis = np.eye(3, 3)\n            R_x_axis = np.array([1, 0, 0,\n                                           0, np.cos(-current_angle), -np.sin(-current_angle),\n                                           0, np.sin(-current_angle), np.cos(-current_angle)]).reshape((3, 3))\n            source_center_in_voxel = np.dot(R_x_axis,init_source_position)/voxel_size[0]\n            detector_center_in_voxel = np.dot(R_x_axis,init_detector_position)/voxel_size[0]\n            det_h = np.dot(R_x_axis,init_detector_line_direction)\n            det_v = np.dot(R_x_axis,init_detector_column_direction)\n            current_angle += angular_increment\n\n        #[H|V|d-s]\n        h_v_sdd = np.column_stack((det_h, det_v, (detector_center_in_voxel - source_center_in_voxel) ))\n        h_v_sdd_invers = np.linalg.inv(h_v_sdd)\n        # [H|V|d-s]^-1 * -s\n        back_part = h_v_sdd_invers @ (-source_center_in_voxel)\n        proj_matrix = np.column_stack((h_v_sdd_invers,back_part))\n        projection_matrices[p] =  detector_left_corner_trans @ proj_matrix\n\n        # post processing to get the same oriented outputvolume like ezrt commandline reco: =&gt; tested, no changes needed to get the same orientation as Firefly ART\n        # flip Z-Axis: Z = -Z\n        if swap_detector_axis:\n            projection_matrices[p][0:3, 2] = projection_matrices[p][0:3, 2] * -1.0\n\n        # change orientation of current matrix from XYZ to YXZ: exchange the first two columns\n        # projection_matrices[p][0:3, 0:2] = np.flip(projection_matrices[p][0:3, 0:2], axis=1)\n        # change orientation of current matrix from YXZ to YZX: exchange the last two columns\n        # projection_matrices[p][0:3, 1:3] = np.flip(projection_matrices[p][0:3, 1:3], axis=1)\n    return projection_matrices\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/trajectories/arbitrary_trajectory/#pyronn.ct_reconstruction.helpers.trajectories.arbitrary_trajectory.fibonacci_sphere_projecton_matrix","title":"pyronn.ct_reconstruction.helpers.trajectories.arbitrary_trajectory.fibonacci_sphere_projecton_matrix","text":"<pre><code>fibonacci_sphere_projecton_matrix(number_of_projections, source_detector_distance, detector_spacing, source_isocenter_distance, detector_origin, swap_axis=False, *args, **kwargs)\n</code></pre> Source code in <code>pyronn/ct_reconstruction/helpers/trajectories/arbitrary_trajectory.py</code> <pre><code>def fibonacci_sphere_projecton_matrix(number_of_projections, source_detector_distance,\n                                      detector_spacing, source_isocenter_distance, detector_origin,\n                                      swap_axis=False, *args, **kwargs):\n    # init empty\n    # assert len(pts) == number_of_projections\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    pts = fibonacci_sphere(number_of_projections)\n    projection_matrices = np.zeros((len(pts), 3, 4))\n    x_axis = np.array([1., 0., 0.])\n    y_axis = np.array([0., 1., 0.])\n    z_axis = np.array([0., 0., 1.])\n\n    u_dir = y_axis\n    if swap_axis:\n        v_dir = x_axis\n    else:\n        v_dir = -x_axis\n\n    intrinsic_params_mat = np.eye(3, 3)\n    for i in range(2):\n        intrinsic_params_mat[i, i] = source_detector_distance / detector_spacing[1 - i]\n\n    # calc and set detector origin\n    # we need t_x and t_y, and follow the [z,y,x] convention\n\n    intrinsic_params_mat[0, 2] = detector_origin[-1] / detector_spacing[-1] * -1\n    intrinsic_params_mat[1, 2] = detector_origin[-2] / detector_spacing[-2] * -1\n\n    for p in range(len(pts)):\n        extrinsic_params_mat = np.eye(4, 4)\n\n        R_to_plane = np.eye(4, 4)\n        R_to_plane[0:3, 0:3] = np.array([z_axis, np.cross(z_axis, x_axis), -x_axis])\n\n\n        axis_align_R = np.eye(4, 4)\n        axis_align_R[0:3, 0] = u_dir\n        axis_align_R[0:3, 1] = v_dir\n        axis_align_R[0:3, 2] = np.cross(u_dir, v_dir)\n        axis_align_R = axis_align_R.T\n\n        translation = np.eye(4, 4)\n        translation[0:4, 3] = np.array([0, 0, source_isocenter_distance, 1])\n\n        R_to_pts = np.eye(4, 4)\n        R_to_pts[0:3, 0:3] = rotation_matrix_from_points(pts[p],\n                                                         np.array([0, 0, source_isocenter_distance])\n                                                         )\n\n        extrinsic_params_mat = np.dot(np.dot(np.dot(translation, axis_align_R), R_to_pts), R_to_plane)\n        extrinsic_params_mat = extrinsic_params_mat / extrinsic_params_mat[3, 3]\n\n        projection_matrices[p][0:3, 0:3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 0:3])\n        projection_matrices[p][0:3, 3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 3])\n\n    return projection_matrices\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/trajectories/circular_trajectory/","title":"Reference for <code>pyronn/ct_reconstruction/helpers/trajectories/circular_trajectory.py</code>","text":""},{"location":"reference/ct_reconstruction/helpers/trajectories/circular_trajectory/#pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.circular_trajectory_2d","title":"pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.circular_trajectory_2d","text":"<pre><code>circular_trajectory_2d(number_of_projections, angular_range, swap_detector_axis)\n</code></pre> <pre><code>Generates the central ray vectors defining a circular trajectory for use with the 2d projection layers.\n</code></pre> <p>Args:     geometry: 2d Geometry class including angular_range and number_of_projections Returns:     Central ray vectors as np.array.</p> Source code in <code>pyronn/ct_reconstruction/helpers/trajectories/circular_trajectory.py</code> <pre><code>def circular_trajectory_2d(number_of_projections, angular_range, swap_detector_axis):\n    \"\"\"\n        Generates the central ray vectors defining a circular trajectory for use with the 2d projection layers.\n    Args:\n        geometry: 2d Geometry class including angular_range and number_of_projections\n    Returns:\n        Central ray vectors as np.array.\n    \"\"\"\n    rays = np.zeros([number_of_projections, 2])\n    angular_increment = (angular_range[1] - angular_range[0]) / number_of_projections\n    for i in range(number_of_projections):\n        if swap_detector_axis:\n            rays[i] = [np.cos(angular_range[0] + i * angular_increment), -np.sin(angular_range[0] + i * angular_increment)]\n        else:\n            rays[i] = [np.cos(angular_range[0] + i * angular_increment), np.sin(angular_range[0] + i * angular_increment)]\n    return rays\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/trajectories/circular_trajectory/#pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.circular_trajectory_3d","title":"pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.circular_trajectory_3d","text":"<pre><code>circular_trajectory_3d(number_of_projections, angular_range, detector_spacing, detector_origin, source_isocenter_distance, source_detector_distance, swap_detector_axis, **kwargs)\n</code></pre> <pre><code>Generates the projection matrices defining a circular trajectory around the z-axis\nfor use with the 3d projection layers.\nAdapted from CONRAD Source code https://github.com/akmaier/CONRAD.\n</code></pre> <p>Args:     geometry: 3d Geometry class including angular_range, number_of_projections, source_detector_distance,     detector_spacing, volume_origin, volume_shape and volume_spacing. Returns:     Projection matrices with shape (num_projections, 3, 4) as np.array.</p> Source code in <code>pyronn/ct_reconstruction/helpers/trajectories/circular_trajectory.py</code> <pre><code>def circular_trajectory_3d(number_of_projections, angular_range, detector_spacing, detector_origin, source_isocenter_distance, source_detector_distance, swap_detector_axis, **kwargs):\n    \"\"\"\n        Generates the projection matrices defining a circular trajectory around the z-axis\n        for use with the 3d projection layers.\n        Adapted from CONRAD Source code https://github.com/akmaier/CONRAD.\n    Args:\n        geometry: 3d Geometry class including angular_range, number_of_projections, source_detector_distance,\n        detector_spacing, volume_origin, volume_shape and volume_spacing.\n    Returns:\n        Projection matrices with shape (num_projections, 3, 4) as np.array.\n    \"\"\"\n\n    # init empty\n    projection_matrices = np.zeros((number_of_projections, 3, 4))\n\n    # axes for later use\n    x_axis = np.array([1.0, 0.0, 0.0])\n    y_axis = np.array([0.0, 1.0, 0.0])\n    z_axis = np.array([0.0, 0.0, 1.0])\n\n    # defining u and v directions by: main coord axes\n    u_dir = y_axis\n    if swap_detector_axis:\n        v_dir = x_axis\n    else:\n        v_dir = -x_axis\n\n    # configure intrinsic camera parameters\n    intrinsic_params_mat = np.eye(3, 3)\n    for i in range(2):\n        intrinsic_params_mat[i, i] = source_detector_distance / detector_spacing[1-i]\n\n    # calc and set detector origin\n    # we need t_x and t_y, and follow the [z,y,x] convention\n\n    intrinsic_params_mat[0,2] = detector_origin[-1] / detector_spacing[-1] *-1\n    intrinsic_params_mat[1,2] = detector_origin[-2] / detector_spacing[-2] *-1\n    # intrinsic_params_mat[0:2, 2] = detector_origin / detector_spacing *-1\n    # configure extrinisc pararams and create projection_matrices\n    current_angle = angular_range[0]\n    angular_increment = (angular_range[1] - angular_range[0]) / number_of_projections\n    for p in range(number_of_projections):\n        # calculate extrinsic params\n        extrinsic_params_mat = np.eye(4, 4)\n\n        # rotation of axes from world system to plane of rotation system\n        R_to_plane = np.eye(4, 4)\n        R_to_plane[0:3, 0:3] = np.array([z_axis, np.cross(z_axis, x_axis), -x_axis])\n\n        # rotation for u and v direction\n        axis_align_R = np.eye(4, 4)\n        # v_dir = z_axis\n        axis_align_R[0:3, 0] = u_dir\n        axis_align_R[0:3, 1] = v_dir\n        axis_align_R[0:3, 2] = np.cross(u_dir, v_dir)\n        axis_align_R = axis_align_R.T\n\n        # rotation about x axis\n        R_x_axis = np.eye(4, 4)\n        R_x_axis[0:3, 0:3] = np.array([1, 0, 0,\n                                       0, np.cos(-current_angle), -np.sin(-current_angle),\n                                       0, np.sin(-current_angle), np.cos(-current_angle)]).reshape((3, 3))\n\n        # R_x_axis = np.eye(4, 4)\n        # R_x_axis[0:3, 0:3] = np.array([\n        #                                np.cos(-current_angle), -np.sin(-current_angle),0,\n        #                                np.sin(-current_angle), np.cos(-current_angle), 0,\n        #                                 0, 0, 1]).reshape((3, 3))\n\n        # translation of camera\n        translation = np.eye(4, 4)\n        translation[0:4, 3] = np.array([0, 0, -source_isocenter_distance, 1])\n\n        # combine the above into 4x4 extrinsic params matrix\n        extrinsic_params_mat = np.dot(np.dot(np.dot(axis_align_R, translation), R_x_axis), R_to_plane.T)\n        extrinsic_params_mat = extrinsic_params_mat / extrinsic_params_mat[3, 3]\n\n        # calculate projection matrix\n        projection_matrices[p][0:3, 0:3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 0:3])\n        projection_matrices[p][0:3, 3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 3])\n\n        # next angle\n        current_angle += angular_increment\n\n    return projection_matrices\n</code></pre>"},{"location":"reference/ct_reconstruction/helpers/trajectories/circular_trajectory/#pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.multi_circular_trajectory_3d","title":"pyronn.ct_reconstruction.helpers.trajectories.circular_trajectory.multi_circular_trajectory_3d","text":"<pre><code>multi_circular_trajectory_3d(number_of_projections, longtitude, angular_range, detector_spacing, detector_origin, source_isocenter_distance, source_detector_distance, swap_detector_axis, **kwargs)\n</code></pre> <pre><code>Generates the projection matrices defining a circular trajectory around the z-axis\nfor use with the 3d projection layers.\nAdapted from CONRAD Source code https://github.com/akmaier/CONRAD.\n</code></pre> <p>Args:     geometry: 3d Geometry class including angular_range, number_of_projections, source_detector_distance,     detector_spacing, volume_origin, volume_shape and volume_spacing. Returns:     Projection matrices with shape (num_projections, 3, 4) as np.array.</p> Source code in <code>pyronn/ct_reconstruction/helpers/trajectories/circular_trajectory.py</code> <pre><code>def multi_circular_trajectory_3d(number_of_projections, longtitude, angular_range, detector_spacing, detector_origin, source_isocenter_distance, source_detector_distance, swap_detector_axis, **kwargs):\n    \"\"\"\n        Generates the projection matrices defining a circular trajectory around the z-axis\n        for use with the 3d projection layers.\n        Adapted from CONRAD Source code https://github.com/akmaier/CONRAD.\n    Args:\n        geometry: 3d Geometry class including angular_range, number_of_projections, source_detector_distance,\n        detector_spacing, volume_origin, volume_shape and volume_spacing.\n    Returns:\n        Projection matrices with shape (num_projections, 3, 4) as np.array.\n    \"\"\"\n\n    # init empty\n    projection_matrices = np.zeros((number_of_projections, 3, 4))\n\n    # axes for later use\n    x_axis = np.array([1.0, 0.0, 0.0])\n    y_axis = np.array([0.0, 1.0, 0.0])\n    z_axis = np.array([0.0, 0.0, 1.0])\n\n    # defining u and v directions by: main coord axes\n    u_dir = y_axis\n    if swap_detector_axis:\n        v_dir = x_axis\n    else:\n        v_dir = -x_axis\n\n    # configure intrinsic camera parameters\n    intrinsic_params_mat = np.eye(3, 3)\n    for i in range(2):\n        intrinsic_params_mat[i, i] = source_detector_distance / detector_spacing[1-i]\n\n    # calc and set detector origin\n    # we need t_x and t_y, and follow the [z,y,x] convention\n\n    intrinsic_params_mat[0,2] = detector_origin[-1] / detector_spacing[-1] *-1\n    intrinsic_params_mat[1,2] = detector_origin[-2] / detector_spacing[-2] *-1\n    # intrinsic_params_mat[0:2, 2] = detector_origin / detector_spacing *-1\n    # configure extrinisc pararams and create projection_matrices\n    current_angle = angular_range[0]\n    angular_increment = (angular_range[1] - angular_range[0]) / number_of_projections\n    for p in range(number_of_projections):\n        # calculate extrinsic params\n        extrinsic_params_mat = np.eye(4, 4)\n\n        # # rotation of axes from world system to plane of rotation system\n        R_to_plane = np.eye(4, 4)\n        R_to_plane[0:3, 0:3] = np.array([z_axis, np.cross(z_axis, x_axis), -x_axis])\n\n        # rotation for u and v direction\n        axis_align_R = np.eye(4, 4)\n        axis_align_R[0:3, 0] = u_dir\n        axis_align_R[0:3, 1] = v_dir\n        axis_align_R[0:3, 2] = np.cross(u_dir, v_dir)\n        axis_align_R = axis_align_R.T\n\n        # rotation about x axis\n        R_x_axis = np.eye(4, 4)\n        R_x_axis[0:3, 0:3] = np.array([1, 0, 0,\n                                       0, np.cos(-current_angle), -np.sin(-current_angle),\n                                       0, np.sin(-current_angle), np.cos(-current_angle)]).reshape((3, 3))\n\n        # rotation about z axis\n        R_z_axis = np.eye(4, 4)\n        R_z_axis[0:3, 0:3] = np.array([\n                                       np.cos(-longtitude), -np.sin(-longtitude),0,\n                                       np.sin(-longtitude), np.cos(-longtitude), 0,\n                                        0, 0, 1]).reshape((3, 3))\n\n        # translation of camera\n        translation = np.eye(4, 4)\n        translation[0:4, 3] = np.array([0, 0, -source_isocenter_distance, 1])\n\n        # combine the above into 4x4 extrinsic params matrix\n        extrinsic_params_mat = np.dot(np.dot(np.dot(np.dot(axis_align_R, translation), R_x_axis), R_z_axis), R_to_plane.T)\n        extrinsic_params_mat = extrinsic_params_mat / extrinsic_params_mat[3, 3]\n\n        # calculate projection matrix\n        projection_matrices[p][0:3, 0:3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 0:3])\n        projection_matrices[p][0:3, 3] = np.dot(intrinsic_params_mat, extrinsic_params_mat[0:3, 3])\n\n        # next angle\n        current_angle += angular_increment\n\n    return projection_matrices\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/backprojection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/backprojection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/backprojection_2d/#pyronn.ct_reconstruction.layers.backprojection_2d.ParallelBackProjectionFor2D","title":"pyronn.ct_reconstruction.layers.backprojection_2d.ParallelBackProjectionFor2D","text":""},{"location":"reference/ct_reconstruction/layers/backprojection_2d/#pyronn.ct_reconstruction.layers.backprojection_2d.ParallelBackProjectionFor2D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Reconstruction for the 2D parallel beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detection_size) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 2D parallel beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/backprojection_2d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Reconstruction for the 2D parallel beam CT.\n\n    args:\n        input: (1, number_of_projections, detection_size) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 2D parallel beam CT.\n    '''\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.backprojection_2d import ParallelBackProjection2D\n\n        if not isinstance(input, torch.Tensor):\n            sinogram = torch.tensor(input.copy(), dtype=torch.float32)\n        else:\n            sinogram = torch.clone(input)\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n\n                    sinogram = sinogram.cuda()\n                    tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n\n        reco = ParallelBackProjection2D().forward(sinogram.contiguous(), **tensor_geometry)\n        if for_train:\n            return reco\n\n        if reco.device.type == 'cuda':\n            return reco.detach().cpu().numpy()\n        return reco.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d import parallel_backprojection2d\n            return parallel_backprojection2d(input, geometry)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/backprojection_2d/#pyronn.ct_reconstruction.layers.backprojection_2d.FanBackProjectionFor2D","title":"pyronn.ct_reconstruction.layers.backprojection_2d.FanBackProjectionFor2D","text":""},{"location":"reference/ct_reconstruction/layers/backprojection_2d/#pyronn.ct_reconstruction.layers.backprojection_2d.FanBackProjectionFor2D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Reconstruction for the 2D fan beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detection_size) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 2D fan beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/backprojection_2d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Reconstruction for the 2D fan beam CT.\n\n    args:\n        input: (1, number_of_projections, detection_size) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 2D fan beam CT.\n    '''\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.backprojection_2d import FanBackProjection2D\n\n        if not isinstance(input, torch.Tensor):\n            sinogram = torch.tensor(input.copy(), dtype=torch.float32).cuda()\n        else:\n            sinogram = torch.clone(input).cuda()\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n                else:\n                    tmp_tensor = torch.Tensor([param])\n\n                tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n        reco = FanBackProjection2D().forward(sinogram.contiguous(), **tensor_geometry)\n        if for_train:\n            return reco\n\n        if reco.device.type == 'cuda':\n            return reco.detach().cpu().numpy()\n        return reco.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d import fan_backprojection2d\n            return fan_backprojection2d(input, geometry)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/backprojection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/backprojection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/backprojection_3d/#pyronn.ct_reconstruction.layers.backprojection_3d.ConeBackProjectionFor3D","title":"pyronn.ct_reconstruction.layers.backprojection_3d.ConeBackProjectionFor3D","text":""},{"location":"reference/ct_reconstruction/layers/backprojection_3d/#pyronn.ct_reconstruction.layers.backprojection_3d.ConeBackProjectionFor3D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Reconstruction for the 3D cone beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detector_height, detector_width) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 2D parallel beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/backprojection_3d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Reconstruction for the 3D cone beam CT.\n\n    args:\n        input: (1, number_of_projections, detector_height, detector_width) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 2D parallel beam CT.\n    '''\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.backprojection_3d import ConeBackProjection3D\n\n        if not isinstance(input, torch.Tensor):\n            sinogram = torch.tensor(input.copy(), dtype=torch.float32).cuda()\n        else:\n            sinogram = torch.clone(input).cuda()\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n                else:\n                    tmp_tensor = torch.Tensor([param])\n\n                tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n\n        reco = ConeBackProjection3D().forward(sinogram.contiguous(), **tensor_geometry)\n        if for_train:\n            return reco\n\n        if reco.device.type == 'cuda':\n            return reco.detach().cpu().numpy()\n        return reco.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.backprojection_3d import cone_backprojection3d\n            return cone_backprojection3d(input, geometry)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/projection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/projection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/projection_2d/#pyronn.ct_reconstruction.layers.projection_2d.ParallelProjectionFor2D","title":"pyronn.ct_reconstruction.layers.projection_2d.ParallelProjectionFor2D","text":""},{"location":"reference/ct_reconstruction/layers/projection_2d/#pyronn.ct_reconstruction.layers.projection_2d.ParallelProjectionFor2D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Projection for the 2D parallel beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detection_size) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 2D parallel beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/projection_2d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Projection for the 2D parallel beam CT.\n\n    args:\n        input: (1, number_of_projections, detection_size) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 2D parallel beam CT.\n    '''\n\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.projection_2d import ParallelProjection2D\n\n        if not isinstance(input, torch.Tensor):\n            phantom = torch.tensor(input.copy(), dtype=torch.float32)\n        else:\n            phantom = torch.clone(input).cuda()\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n                else:\n                    tmp_tensor = torch.Tensor([param])\n\n                tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n        sinogram =  ParallelProjection2D().forward(phantom, **tensor_geometry)\n        if for_train:\n            return sinogram\n\n        if sinogram.device.type == 'cuda':\n            return sinogram.detach().cpu().numpy()\n        return sinogram.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.projection_2d import parallel_projection2d\n            return parallel_projection2d(input, geometry)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/projection_2d/#pyronn.ct_reconstruction.layers.projection_2d.FanProjectionFor2D","title":"pyronn.ct_reconstruction.layers.projection_2d.FanProjectionFor2D","text":""},{"location":"reference/ct_reconstruction/layers/projection_2d/#pyronn.ct_reconstruction.layers.projection_2d.FanProjectionFor2D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Projection for the 2D fan beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detection_size) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 2D fan beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/projection_2d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Projection for the 2D fan beam CT.\n\n    args:\n        input: (1, number_of_projections, detection_size) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 2D fan beam CT.\n    '''\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.projection_2d import FanProjection2D\n\n        if not isinstance(input, torch.Tensor):\n            phantom = torch.tensor(input.copy(), dtype=torch.float32).cuda()\n        else:\n            phantom = torch.clone(input).cuda()\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n                else:\n                    tmp_tensor = torch.Tensor([param])\n\n                tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n\n        sinogram = FanProjection2D().forward(phantom, **tensor_geometry)\n        if for_train:\n            return sinogram\n\n        if sinogram.device.type == 'cuda':\n            return sinogram.detach().cpu().numpy()\n        return sinogram.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.projection_2d import fan_projection2d\n            return fan_projection2d(input, geometry)\n        else: raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/projection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/projection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/projection_3d/#pyronn.ct_reconstruction.layers.projection_3d.Projection3D","title":"pyronn.ct_reconstruction.layers.projection_3d.Projection3D","text":"<pre><code>Projection3D()\n</code></pre> Source code in <code>pyronn/ct_reconstruction/layers/projection_3d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    self.backend = pyronn.read_backend()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/projection_3d/#pyronn.ct_reconstruction.layers.projection_3d.ConeProjectionFor3D","title":"pyronn.ct_reconstruction.layers.projection_3d.ConeProjectionFor3D","text":"<pre><code>ConeProjectionFor3D()\n</code></pre> <p>               Bases: <code>Projection3D</code></p> Source code in <code>pyronn/ct_reconstruction/layers/projection_3d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    self.backend = pyronn.read_backend()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/projection_3d/#pyronn.ct_reconstruction.layers.projection_3d.ConeProjectionFor3D.forward","title":"forward","text":"<pre><code>forward(input, geometry, for_train=False, debug=False)\n</code></pre> <p>Projection for the 3D cone beam CT.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>(1, number_of_projections, detection_size) numpy array or torch.Tensor.</p> required <code>geometry</code> <p>The projection geometry used for projection.</p> required <code>for_train</code> <p>Set the return value data type if the backend is torch. You can get a numpy.array by setting this</p> <code>False</code> return <p>The reconstruction result of 3D cone beam CT.</p> Source code in <code>pyronn/ct_reconstruction/layers/projection_3d.py</code> <pre><code>def forward(self, input, geometry, for_train=False, debug=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Projection for the 3D cone beam CT.\n\n    args:\n        input: (1, number_of_projections, detection_size) numpy array or torch.Tensor.\n        geometry: The projection geometry used for projection.\n        for_train: Set the return value data type if the backend is torch. You can get a numpy.array by setting this\n        value False, otherwise you will get a torch.Tensor.\n\n    return:\n        The reconstruction result of 3D cone beam CT.\n    '''\n    try:\n        import torch\n        from pyronn.ct_reconstruction.layers.torch.projection_3d import ConeProjection3D\n\n        if not isinstance(input, torch.Tensor):\n            phantom = torch.tensor(input.copy(), dtype=torch.float32).cuda()\n        else:\n            phantom = torch.clone(input).cuda()\n\n        tensor_geometry = {}\n        geo_dict = vars(geometry)\n        for k in geo_dict:\n            param = geo_dict[k]\n            try:\n                if hasattr(param, '__len__'):\n                    tmp_tensor = torch.Tensor(param)\n                else:\n                    tmp_tensor = torch.Tensor([param])\n\n\n                tensor_geometry[k] = tmp_tensor.cuda()\n            except Exception as e:\n                if isinstance(e, TypeError):\n                    if debug: print('Attribute &lt;' + k + '&gt; could not be transformed to torch.Tensor')\n                else:\n                    raise e\n\n        sinogram = ConeProjection3D().forward(phantom, **tensor_geometry)\n        if for_train:\n            return sinogram\n\n        if sinogram.device.type == 'cuda':\n            return sinogram.detach().cpu().numpy()\n        return sinogram.cpu().numpy()\n\n    except Exception as e:\n        if isinstance(e, ModuleNotFoundError):\n            from pyronn.ct_reconstruction.layers.tensorflow.projection_3d import cone_projection3d\n            return cone_projection3d(input, geometry)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_2d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d.parallel_backprojection2d","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d.parallel_backprojection2d","text":"<pre><code>parallel_backprojection2d(sinogram, geometry)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:     Input volume to project.     geometry:   Corresponding GeometryParallel2D Object defining parameters. Returns:         Initialized lme_custom_ops.parallel_backprojection2d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_2d.py</code> <pre><code>def parallel_backprojection2d(sinogram, geometry):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:     Input volume to project.\n        geometry:   Corresponding GeometryParallel2D Object defining parameters.\n    Returns:\n            Initialized lme_custom_ops.parallel_backprojection2d layer.\n    \"\"\"\n    batch = np.shape(sinogram)[0]\n    return pyronn_layers.parallel_backprojection2d(sinogram,\n                                                   volume_shape=geometry.volume_shape,\n                                                    volume_origin   =np.broadcast_to(geometry.volume_origin,[batch,*np.shape(geometry.volume_origin)]),\n                                                    detector_origin =np.broadcast_to(geometry.detector_origin,[batch,*np.shape(geometry.detector_origin)]),\n                                                    volume_spacing  =np.broadcast_to(geometry.volume_spacing,[batch,*np.shape(geometry.volume_spacing)]),\n                                                    detector_spacing=np.broadcast_to(geometry.detector_spacing,[batch,*np.shape(geometry.detector_spacing)]),\n                                                    ray_vectors     =np.broadcast_to(geometry.trajectory,[batch,*np.shape(geometry.trajectory)]))\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_2d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d._backproject_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d._backproject_grad","text":"<pre><code>_backproject_grad(op, grad)\n</code></pre> <p>Compute the gradient of the backprojector op by invoking the forward projector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_2d.py</code> <pre><code>@ops.RegisterGradient(\"FanBackprojection2D\")\ndef _backproject_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Compute the gradient of the backprojector op by invoking the forward projector.\n    '''\n    proj = pyronn_layers.fan_projection2d(\n        volume=grad,\n        projection_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        detector_origin=op.inputs[3],\n        volume_spacing=op.inputs[4],\n        detector_spacing=op.inputs[5],\n        source_2_isocenter_distance=op.inputs[6],\n        source_2_detector_distance=op.inputs[7],\n        central_ray_vectors=op.inputs[8],\n    )\n    return [proj, tf.stop_gradient(op.inputs[1]),  tf.stop_gradient(op.inputs[2]),  tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]),\n            tf.stop_gradient(op.inputs[5]),  tf.stop_gradient(op.inputs[6]),  tf.stop_gradient(op.inputs[7]), tf.stop_gradient(op.inputs[8])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_2d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d.fan_backprojection2d","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d.fan_backprojection2d","text":"<pre><code>fan_backprojection2d(sinogram, geometry)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:     Input volume to project.     geometry:   Corresponding GeometryFan2D Object defining parameters. Returns:         Initialized lme_custom_ops.fan_backprojection2d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_2d.py</code> <pre><code>def fan_backprojection2d(sinogram, geometry):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:     Input volume to project.\n        geometry:   Corresponding GeometryFan2D Object defining parameters.\n    Returns:\n            Initialized lme_custom_ops.fan_backprojection2d layer.\n    \"\"\"\n    batch = np.shape(sinogram)[0]\n    return pyronn_layers.fan_backprojection2d(sinogram,\n                                              volume_shape=geometry.volume_shape,\n                                              volume_origin=np.broadcast_to(geometry.volume_origin, [batch, *np.shape(geometry.volume_origin)]),\n                                              detector_origin=np.broadcast_to(geometry.detector_origin, [batch, *np.shape(geometry.detector_origin)]),\n                                              volume_spacing=np.broadcast_to(geometry.volume_spacing, [batch, *np.shape(geometry.volume_spacing)]),\n                                              detector_spacing=np.broadcast_to(geometry.detector_spacing, [batch, *np.shape(geometry.detector_spacing)]),\n                                              source_2_isocenter_distance=np.broadcast_to(geometry.source_isocenter_distance, [batch, *np.shape(geometry.source_isocenter_distance)]),\n                                              source_2_detector_distance=np.broadcast_to(geometry.source_detector_distance, [batch, *np.shape(geometry.source_detector_distance)]),\n                                              central_ray_vectors=np.broadcast_to(geometry.trajectory, [batch, *np.shape(geometry.trajectory)]))\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_2d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d._backproject_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_2d._backproject_grad","text":"<pre><code>_backproject_grad(op, grad)\n</code></pre> <p>Compute the gradient of the backprojector op by invoking the forward projector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_2d.py</code> <pre><code>@ops.RegisterGradient(\"FanBackprojection2D\")\ndef _backproject_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Compute the gradient of the backprojector op by invoking the forward projector.\n    '''\n    proj = pyronn_layers.fan_projection2d(\n        volume=grad,\n        projection_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        detector_origin=op.inputs[3],\n        volume_spacing=op.inputs[4],\n        detector_spacing=op.inputs[5],\n        source_2_isocenter_distance=op.inputs[6],\n        source_2_detector_distance=op.inputs[7],\n        central_ray_vectors=op.inputs[8],\n    )\n    return [proj, tf.stop_gradient(op.inputs[1]),  tf.stop_gradient(op.inputs[2]),  tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]),\n            tf.stop_gradient(op.inputs[5]),  tf.stop_gradient(op.inputs[6]),  tf.stop_gradient(op.inputs[7]), tf.stop_gradient(op.inputs[8])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_3d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_3d.cone_backprojection3d","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_3d.cone_backprojection3d","text":"<pre><code>cone_backprojection3d(sinogram, geometry, hardware_interp=True)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:             Input volume to project.     geometry:           Corresponding GeometryCone3D Object defining parameters.     hardware_interp:    Controls if interpolation is done by GPU  Returns:         Initialized lme_custom_ops.cone_backprojection3d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_3d.py</code> <pre><code>def cone_backprojection3d(sinogram, geometry, hardware_interp=True):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:             Input volume to project.\n        geometry:           Corresponding GeometryCone3D Object defining parameters.\n        hardware_interp:    Controls if interpolation is done by GPU \n    Returns:\n            Initialized lme_custom_ops.cone_backprojection3d layer.\n    \"\"\"\n    batch = np.shape(sinogram)[0]\n    step_size=1.0\n    return pyronn_layers.cone_backprojection3d(sinogram,\n                                               volume_shape=geometry.volume_shape,\n                                               volume_origin=np.broadcast_to(geometry.volume_origin, [batch, *np.shape(geometry.volume_origin)]),\n                                               volume_spacing=np.broadcast_to(geometry.volume_spacing, [batch, *np.shape(geometry.volume_spacing)]),\n                                               projection_matrices=np.broadcast_to(geometry.trajectory, [batch, *np.shape(geometry.trajectory)]),\n                                               hardware_interp=hardware_interp,\n                                               step_size=np.broadcast_to(step_size, [batch, *np.shape(step_size)]),\n                                               projection_multiplier=np.broadcast_to(geometry.projection_multiplier, [batch, *np.shape(geometry.projection_multiplier)]))\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/backprojection_3d/#pyronn.ct_reconstruction.layers.tensorflow.backprojection_3d._backproject_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.backprojection_3d._backproject_grad","text":"<pre><code>_backproject_grad(op, grad)\n</code></pre> <p>Compute the gradient of the backprojector op by invoking the forward projector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/backprojection_3d.py</code> <pre><code>@ops.RegisterGradient(\"ConeBackprojection3D\")\ndef _backproject_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Compute the gradient of the backprojector op by invoking the forward projector.\n    '''\n    proj = pyronn_layers.cone_projection3d(\n        volume=grad,\n        projection_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        volume_spacing=op.inputs[3],\n        projection_matrices=op.inputs[4],\n        hardware_interp=op.get_attr(\"hardware_interp\"),\n        step_size=op.inputs[5],\n        projection_multiplier=op.inputs[6],\n    )\n    return [proj, tf.stop_gradient(op.inputs[1]), tf.stop_gradient(op.inputs[2]), tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]), tf.stop_gradient(op.inputs[5]), tf.stop_gradient(op.inputs[6])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/tensorflow/projection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_2d/#pyronn.ct_reconstruction.layers.tensorflow.projection_2d.parallel_projection2d","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_2d.parallel_projection2d","text":"<pre><code>parallel_projection2d(volume, geometry)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:     Input volume to project.     geometry:   Corresponding GeometryParallel2D Object defining parameters. Returns:         Initialized lme_custom_ops.parallel_projection2d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_2d.py</code> <pre><code>def parallel_projection2d(volume, geometry):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:     Input volume to project.\n        geometry:   Corresponding GeometryParallel2D Object defining parameters.\n    Returns:\n            Initialized lme_custom_ops.parallel_projection2d layer.\n    \"\"\"\n    batch = np.shape(volume)[0]\n    return pyronn_layers.parallel_projection2d(volume,\n                                               projection_shape=geometry.sinogram_shape,\n                                               volume_origin=np.broadcast_to(geometry.volume_origin, [batch, *np.shape(geometry.volume_origin)]),\n                                               detector_origin=np.broadcast_to(geometry.detector_origin, [batch, *np.shape(geometry.detector_origin)]),\n                                               volume_spacing=np.broadcast_to(geometry.volume_spacing, [batch, *np.shape(geometry.volume_spacing)]),\n                                               detector_spacing=np.broadcast_to(geometry.detector_spacing, [batch, *np.shape(geometry.detector_spacing)]),\n                                               ray_vectors=np.broadcast_to(geometry.trajectory, [batch, *np.shape(geometry.trajectory)]))\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_2d/#pyronn.ct_reconstruction.layers.tensorflow.projection_2d._project_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_2d._project_grad","text":"<pre><code>_project_grad(op, grad)\n</code></pre> <p>Compute the gradient of the projection op by invoking the backprojector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_2d.py</code> <pre><code>@ops.RegisterGradient(\"FanProjection2D\")\ndef _project_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Compute the gradient of the projection op by invoking the backprojector.\n    '''\n    reco = pyronn_layers.fan_backprojection2d(\n        sinogram=grad,\n        volume_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        detector_origin=op.inputs[3],\n        volume_spacing=op.inputs[4],\n        detector_spacing=op.inputs[5],\n        source_2_isocenter_distance=op.inputs[6],\n        source_2_detector_distance=op.inputs[7],\n        central_ray_vectors=op.inputs[8],\n    )\n    return [reco, tf.stop_gradient(op.inputs[1]), tf.stop_gradient(op.inputs[2]), tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]), tf.stop_gradient(op.inputs[5]), tf.stop_gradient(op.inputs[6]),\n            tf.stop_gradient(op.inputs[7]), tf.stop_gradient(op.inputs[8])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_2d/#pyronn.ct_reconstruction.layers.tensorflow.projection_2d.fan_projection2d","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_2d.fan_projection2d","text":"<pre><code>fan_projection2d(volume, geometry)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:     Input volume to project.     geometry:   Corresponding GeometryFan2D Object defining parameters. Returns:         Initialized lme_custom_ops.fan_projection2d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_2d.py</code> <pre><code>def fan_projection2d(volume, geometry):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:     Input volume to project.\n        geometry:   Corresponding GeometryFan2D Object defining parameters.\n    Returns:\n            Initialized lme_custom_ops.fan_projection2d layer.\n    \"\"\"\n    batch = np.shape(volume)[0]\n    return pyronn_layers.fan_projection2d(volume,\n                                          projection_shape=geometry.sinogram_shape,\n                                          volume_origin=np.broadcast_to(geometry.volume_origin, [batch, *np.shape(geometry.volume_origin)]),\n                                          detector_origin=np.broadcast_to(geometry.detector_origin, [batch, *np.shape(geometry.detector_origin)]),\n                                          volume_spacing=np.broadcast_to(geometry.volume_spacing, [batch, *np.shape(geometry.volume_spacing)]),\n                                          detector_spacing=np.broadcast_to(geometry.detector_spacing, [batch, *np.shape(geometry.detector_spacing)]),\n                                          source_2_isocenter_distance=np.broadcast_to(geometry.source_isocenter_distance, [batch, *np.shape(geometry.source_isocenter_distance)]),\n                                          source_2_detector_distance=np.broadcast_to(geometry.source_detector_distance, [batch, *np.shape(geometry.source_detector_distance)]),\n                                          central_ray_vectors=np.broadcast_to(geometry.trajectory, [batch, *np.shape(geometry.trajectory)]))\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_2d/#pyronn.ct_reconstruction.layers.tensorflow.projection_2d._project_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_2d._project_grad","text":"<pre><code>_project_grad(op, grad)\n</code></pre> <p>Compute the gradient of the projection op by invoking the backprojector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_2d.py</code> <pre><code>@ops.RegisterGradient(\"FanProjection2D\")\ndef _project_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n        Compute the gradient of the projection op by invoking the backprojector.\n    '''\n    reco = pyronn_layers.fan_backprojection2d(\n        sinogram=grad,\n        volume_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        detector_origin=op.inputs[3],\n        volume_spacing=op.inputs[4],\n        detector_spacing=op.inputs[5],\n        source_2_isocenter_distance=op.inputs[6],\n        source_2_detector_distance=op.inputs[7],\n        central_ray_vectors=op.inputs[8],\n    )\n    return [reco, tf.stop_gradient(op.inputs[1]), tf.stop_gradient(op.inputs[2]), tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]), tf.stop_gradient(op.inputs[5]), tf.stop_gradient(op.inputs[6]),\n            tf.stop_gradient(op.inputs[7]), tf.stop_gradient(op.inputs[8])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/tensorflow/projection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_3d/#pyronn.ct_reconstruction.layers.tensorflow.projection_3d.cone_projection3d","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_3d.cone_projection3d","text":"<pre><code>cone_projection3d(volume, geometry, hardware_interp=True, step_size=1.0)\n</code></pre> <p>Wrapper function for making the layer call. Args:     volume:             Input volume to project.     geometry:           Corresponding GeometryCone3D Object defining parameters.     hardware_interp:    Controls if interpolation is done by GPU.     step_size:          step_size along ray direction in voxel. Returns:         Initialized lme_custom_ops.cone_projection3d layer.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_3d.py</code> <pre><code>def cone_projection3d(volume, geometry, hardware_interp=True, step_size=1.0):\n    \"\"\"\n    Wrapper function for making the layer call.\n    Args:\n        volume:             Input volume to project.\n        geometry:           Corresponding GeometryCone3D Object defining parameters.\n        hardware_interp:    Controls if interpolation is done by GPU.\n        step_size:          step_size along ray direction in voxel.\n    Returns:\n            Initialized lme_custom_ops.cone_projection3d layer.\n    \"\"\"\n    batch = np.shape(volume)[0]\n    return pyronn_layers.cone_projection3d(volume,\n                                           projection_shape=geometry.sinogram_shape,\n                                           volume_origin=np.broadcast_to(geometry.volume_origin, [batch, *np.shape(geometry.volume_origin)]),\n                                           volume_spacing=np.broadcast_to(geometry.volume_spacing, [batch, *np.shape(geometry.volume_spacing)]),\n                                           projection_matrices=np.broadcast_to(geometry.trajectory, [batch, *np.shape(geometry.trajectory)]),\n                                           step_size=np.broadcast_to(step_size, [batch, *np.shape(step_size)]),\n                                           projection_multiplier=np.broadcast_to(geometry.projection_multiplier, [batch, *np.shape(geometry.projection_multiplier)]),\n                                           hardware_interp=hardware_interp)\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/tensorflow/projection_3d/#pyronn.ct_reconstruction.layers.tensorflow.projection_3d._project_grad","title":"pyronn.ct_reconstruction.layers.tensorflow.projection_3d._project_grad","text":"<pre><code>_project_grad(op, grad)\n</code></pre> <p>Compute the gradient of the projection op by invoking the backprojector.</p> Source code in <code>pyronn/ct_reconstruction/layers/tensorflow/projection_3d.py</code> <pre><code>@ops.RegisterGradient(\"ConeProjection3D\")\ndef _project_grad(op, grad):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    '''\n    Compute the gradient of the projection op by invoking the backprojector.\n    '''\n    reco = pyronn_layers.cone_backprojection3d(\n        sinogram=grad,\n        volume_shape=op.inputs[0].shape[1:],\n        volume_origin=op.inputs[2],\n        volume_spacing=op.inputs[3],\n        projection_matrices=op.inputs[4],\n        step_size=op.inputs[5],\n        projection_multiplier=op.inputs[6],\n        hardware_interp=op.get_attr(\"hardware_interp\")\n    )\n    return [reco, tf.stop_gradient(op.inputs[1]), tf.stop_gradient(op.inputs[2]), tf.stop_gradient(op.inputs[3]), tf.stop_gradient(op.inputs[4]), tf.stop_gradient(op.inputs[5]), tf.stop_gradient(op.inputs[6])]\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/torch/backprojection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/torch/backprojection_2d/#pyronn.ct_reconstruction.layers.torch.backprojection_2d.ParallelBackProjection2DFunction","title":"pyronn.ct_reconstruction.layers.torch.backprojection_2d.ParallelBackProjection2DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_2d/#pyronn.ct_reconstruction.layers.torch.backprojection_2d.ParallelBackProjection2D","title":"pyronn.ct_reconstruction.layers.torch.backprojection_2d.ParallelBackProjection2D","text":"<pre><code>ParallelBackProjection2D()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/backprojection_2d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(ParallelBackProjection2D, self).__init__()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_2d/#pyronn.ct_reconstruction.layers.torch.backprojection_2d.FanBackProjection2DFunction","title":"pyronn.ct_reconstruction.layers.torch.backprojection_2d.FanBackProjection2DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_2d/#pyronn.ct_reconstruction.layers.torch.backprojection_2d.FanBackProjection2D","title":"pyronn.ct_reconstruction.layers.torch.backprojection_2d.FanBackProjection2D","text":"<pre><code>FanBackProjection2D()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/backprojection_2d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(FanBackProjection2D, self).__init__()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/torch/backprojection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/torch/backprojection_3d/#pyronn.ct_reconstruction.layers.torch.backprojection_3d.ConeBackProjection3DFunction","title":"pyronn.ct_reconstruction.layers.torch.backprojection_3d.ConeBackProjection3DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/backprojection_3d/#pyronn.ct_reconstruction.layers.torch.backprojection_3d.ConeBackProjection3D","title":"pyronn.ct_reconstruction.layers.torch.backprojection_3d.ConeBackProjection3D","text":"<pre><code>ConeBackProjection3D(hardware_interp=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/backprojection_3d.py</code> <pre><code>def __init__(self, hardware_interp=False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(ConeBackProjection3D, self).__init__()\n    self.hardware_interp = torch.Tensor([hardware_interp]).cpu()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/torch/projection_2d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.ParallelProjection2DFunction","title":"pyronn.ct_reconstruction.layers.torch.projection_2d.ParallelProjection2DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.ParallelProjection2DFunction.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx, input: Tensor, sinogram_shape: Tensor, volume_origin: Tensor, detector_origin: Tensor, volume_spacing: Tensor, detector_spacing: Tensor, trajectory) -&gt; Tensor\n</code></pre> <p>Forward operator of 2D parallel projection Args:          input:              volume to be projected         sinogram_shape:     number_of_projections x detector_width         volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)         ...</p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_2d.py</code> <pre><code>@staticmethod\ndef forward(ctx, input:Tensor, sinogram_shape:Tensor, volume_origin:Tensor, detector_origin:Tensor, volume_spacing:Tensor, detector_spacing:Tensor, trajectory)-&gt;Tensor:\n    \"\"\"\n    Forward operator of 2D parallel projection\n    Args: \n            input:              volume to be projected\n            sinogram_shape:     number_of_projections x detector_width\n            volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)\n            ...\n    \"\"\"\n    outputs = pyronn_layers.parallel_projection2d(input,sinogram_shape, volume_origin,detector_origin,volume_spacing,detector_spacing,trajectory)\n\n    ctx.volume_shape        = torch.tensor(input.shape[1:]).cuda()\n    ctx.volume_origin       = volume_origin\n    ctx.detector_origin     = detector_origin\n    ctx.volume_spacing      = volume_spacing\n    ctx.detector_spacing    = detector_spacing\n    ctx.trajectory          = trajectory\n\n    return outputs\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.ParallelProjection2D","title":"pyronn.ct_reconstruction.layers.torch.projection_2d.ParallelProjection2D","text":"<pre><code>ParallelProjection2D()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_2d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(ParallelProjection2D, self).__init__()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.FanProjection2DFunction","title":"pyronn.ct_reconstruction.layers.torch.projection_2d.FanProjection2DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.FanProjection2DFunction.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx, input: Tensor, sinogram_shape: Tensor, volume_origin: Tensor, detector_origin: Tensor, volume_spacing: Tensor, detector_spacing: Tensor, source_isocenter_distance: Tensor, source_detector_distance: Tensor, trajectory: Tensor) -&gt; Tensor\n</code></pre> <p>Forward operator of 2D fan projection Args:          input:              volume to be projected         sinogram_shape:     number_of_projections x detector_width         volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)         ...</p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_2d.py</code> <pre><code>@staticmethod\ndef forward(ctx, input:Tensor, sinogram_shape:Tensor, volume_origin:Tensor, detector_origin:Tensor, volume_spacing:Tensor, detector_spacing:Tensor, source_isocenter_distance:Tensor, source_detector_distance:Tensor, trajectory:Tensor)-&gt;Tensor:\n    \"\"\"\n    Forward operator of 2D fan projection\n    Args: \n            input:              volume to be projected\n            sinogram_shape:     number_of_projections x detector_width\n            volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)\n            ...\n    \"\"\"\n    outputs = pyronn_layers.fan_projection2d(input,sinogram_shape, volume_origin,detector_origin,volume_spacing,detector_spacing,source_isocenter_distance,source_detector_distance,trajectory)\n\n    ctx.volume_shape        = torch.tensor(input.shape[1:]).cuda()\n    ctx.volume_origin       = volume_origin\n    ctx.detector_origin     = detector_origin\n    ctx.volume_spacing      = volume_spacing\n    ctx.detector_spacing    = detector_spacing\n    ctx.source_isocenter_distance      = source_isocenter_distance\n    ctx.source_detector_distance    = source_detector_distance\n    ctx.trajectory          = trajectory\n\n    return outputs\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_2d/#pyronn.ct_reconstruction.layers.torch.projection_2d.FanProjection2D","title":"pyronn.ct_reconstruction.layers.torch.projection_2d.FanProjection2D","text":"<pre><code>FanProjection2D()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_2d.py</code> <pre><code>def __init__(self):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(FanProjection2D, self).__init__()\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_3d/","title":"Reference for <code>pyronn/ct_reconstruction/layers/torch/projection_3d.py</code>","text":""},{"location":"reference/ct_reconstruction/layers/torch/projection_3d/#pyronn.ct_reconstruction.layers.torch.projection_3d.ConeProjection3DFunction","title":"pyronn.ct_reconstruction.layers.torch.projection_3d.ConeProjection3DFunction","text":"<p>               Bases: <code>Function</code></p>"},{"location":"reference/ct_reconstruction/layers/torch/projection_3d/#pyronn.ct_reconstruction.layers.torch.projection_3d.ConeProjection3DFunction.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx, input: Tensor, sinogram_shape: Tensor, volume_origin: Tensor, volume_spacing: Tensor, trajectory: Tensor, projection_multiplier: Tensor, step_size: Tensor, hardware_interp: Tensor) -&gt; Tensor\n</code></pre> <p>Forward operator of 2D fan projection Args:          input:              volume to be projected         sinogram_shape:     number_of_projections x detector_width         volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)         ...</p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_3d.py</code> <pre><code>@staticmethod\ndef forward(ctx, input:Tensor, sinogram_shape:Tensor, volume_origin:Tensor, volume_spacing:Tensor, trajectory:Tensor,\n                 projection_multiplier:Tensor, step_size:Tensor, hardware_interp:Tensor)-&gt;Tensor:\n    \"\"\"\n    Forward operator of 2D fan projection\n    Args: \n            input:              volume to be projected\n            sinogram_shape:     number_of_projections x detector_width\n            volume_origin:      origin of the world coordinate system w.r.t. the volume array (tensor)\n            ...\n    \"\"\"\n    outputs = pyronn_layers.cone_projection3d(input,sinogram_shape, volume_origin,volume_spacing,trajectory, step_size, hardware_interp)\n\n    ctx.volume_shape            = torch.tensor(input.shape[1:]).cuda()\n    ctx.volume_origin           = volume_origin\n    ctx.volume_spacing          = volume_spacing\n    ctx.trajectory              = trajectory\n    ctx.projection_multiplier   = projection_multiplier\n    ctx.hardware_interp         = hardware_interp\n\n    return outputs\n</code></pre>"},{"location":"reference/ct_reconstruction/layers/torch/projection_3d/#pyronn.ct_reconstruction.layers.torch.projection_3d.ConeProjection3D","title":"pyronn.ct_reconstruction.layers.torch.projection_3d.ConeProjection3D","text":"<pre><code>ConeProjection3D(hardware_interp=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>pyronn/ct_reconstruction/layers/torch/projection_3d.py</code> <pre><code>def __init__(self, hardware_interp = False):\n    # TODO: Document this function on your own. Could not be documented by the model.\n    # TODO: Document this function on your own. Could not be documented by the model.\n    super(ConeProjection3D, self).__init__()\n    self.hardware_interp = torch.Tensor([hardware_interp]).cpu()\n</code></pre>"},{"location":"scripts/gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>import re\nimport subprocess\nfrom collections import defaultdict\nfrom pathlib import Path\n</pre> import re import subprocess from collections import defaultdict from pathlib import Path In\u00a0[\u00a0]: Copied! <pre># Constants\nFILE = Path(__file__).resolve()\nPACKAGE_DIR = FILE.parents[2] / \"pyronn\"\nREFERENCE_DIR = PACKAGE_DIR.parent / \"docs/reference\"\n</pre> # Constants FILE = Path(__file__).resolve() PACKAGE_DIR = FILE.parents[2] / \"pyronn\" REFERENCE_DIR = PACKAGE_DIR.parent / \"docs/reference\" In\u00a0[\u00a0]: Copied! <pre>def extract_classes_and_functions(filepath: Path) -&gt; tuple:\n    \"\"\"Extracts class and function names from a given Python file.\"\"\"\n    content = filepath.read_text(encoding=\"utf-8\")\n    class_pattern = r\"(?:^|\\n)class\\s(\\w+)(?:\\(|:)\"\n    func_pattern = r\"(?:^|\\n)def\\s(\\w+)\\(\"\n\n    classes = re.findall(class_pattern, content)\n    functions = re.findall(func_pattern, content)\n\n    return classes, functions\n</pre> def extract_classes_and_functions(filepath: Path) -&gt; tuple:     \"\"\"Extracts class and function names from a given Python file.\"\"\"     content = filepath.read_text(encoding=\"utf-8\")     class_pattern = r\"(?:^|\\n)class\\s(\\w+)(?:\\(|:)\"     func_pattern = r\"(?:^|\\n)def\\s(\\w+)\\(\"      classes = re.findall(class_pattern, content)     functions = re.findall(func_pattern, content)      return classes, functions In\u00a0[\u00a0]: Copied! <pre>def create_markdown(\n    py_filepath: Path, module_path: str, classes: list, functions: list\n):\n    \"\"\"Creates a Markdown file containing the API reference for the given Python module.\"\"\"\n    md_filepath = py_filepath.with_suffix(\".md\")\n    exists = md_filepath.exists()\n\n    # Extract module docstring\n    py_filepath = Path(str(py_filepath).replace(str(REFERENCE_DIR), str(PACKAGE_DIR)))\n    content = py_filepath.read_text(encoding=\"utf-8\")\n    module_docstring = \"\"\n    if content.startswith('\"\"\"') or content.startswith(\"'''\"):\n        end_index = content.find(content[:3], 3)\n        if end_index != -1:\n            module_docstring = content[3:end_index].strip()\n\n    # Read existing content and keep header content between first two ---\n    header_content = f\"{module_docstring}\\n\\n\" if module_docstring else \"\"\n\n    module_name = module_path.replace(\".__init__\", \"\")\n    module_path = module_path.replace(\".\", \"/\")\n    title_content = f\"# Reference for `{module_path}.py`\\n\\n\"\n    md_content = [\"&lt;br&gt;\\n\"] + [\n        f\"## ::: {module_name}.{class_name}\\n\\n&lt;br&gt;&lt;br&gt;&lt;hr&gt;&lt;br&gt;\\n\"\n        for class_name in classes\n    ]\n    md_content.extend(\n        f\"## ::: {module_name}.{func_name}\\n\\n&lt;br&gt;&lt;br&gt;&lt;hr&gt;&lt;br&gt;\\n\"\n        for func_name in functions\n    )\n    md_content[-1] = md_content[-1].replace(\n        \"&lt;hr&gt;&lt;br&gt;\", \"\"\n    )  # remove last horizontal line\n    md_content = title_content + header_content + \"\\n\".join(md_content)\n    if not md_content.endswith(\"\\n\"):\n        md_content += \"\\n\"\n\n    md_filepath.parent.mkdir(parents=True, exist_ok=True)\n    md_filepath.write_text(md_content)\n\n    if not exists:\n        # Add new markdown file to the git staging area\n        print(f\"Created new file '{md_filepath}'\")\n        subprocess.run(\n            [\"git\", \"add\", \"-f\", str(md_filepath)], check=True, cwd=PACKAGE_DIR\n        )\n\n    return md_filepath.relative_to(PACKAGE_DIR.parent)\n</pre> def create_markdown(     py_filepath: Path, module_path: str, classes: list, functions: list ):     \"\"\"Creates a Markdown file containing the API reference for the given Python module.\"\"\"     md_filepath = py_filepath.with_suffix(\".md\")     exists = md_filepath.exists()      # Extract module docstring     py_filepath = Path(str(py_filepath).replace(str(REFERENCE_DIR), str(PACKAGE_DIR)))     content = py_filepath.read_text(encoding=\"utf-8\")     module_docstring = \"\"     if content.startswith('\"\"\"') or content.startswith(\"'''\"):         end_index = content.find(content[:3], 3)         if end_index != -1:             module_docstring = content[3:end_index].strip()      # Read existing content and keep header content between first two ---     header_content = f\"{module_docstring}\\n\\n\" if module_docstring else \"\"      module_name = module_path.replace(\".__init__\", \"\")     module_path = module_path.replace(\".\", \"/\")     title_content = f\"# Reference for `{module_path}.py`\\n\\n\"     md_content = [\"\\n\"] + [         f\"## ::: {module_name}.{class_name}\\n\\n\\n\"         for class_name in classes     ]     md_content.extend(         f\"## ::: {module_name}.{func_name}\\n\\n\\n\"         for func_name in functions     )     md_content[-1] = md_content[-1].replace(         \"\", \"\"     )  # remove last horizontal line     md_content = title_content + header_content + \"\\n\".join(md_content)     if not md_content.endswith(\"\\n\"):         md_content += \"\\n\"      md_filepath.parent.mkdir(parents=True, exist_ok=True)     md_filepath.write_text(md_content)      if not exists:         # Add new markdown file to the git staging area         print(f\"Created new file '{md_filepath}'\")         subprocess.run(             [\"git\", \"add\", \"-f\", str(md_filepath)], check=True, cwd=PACKAGE_DIR         )      return md_filepath.relative_to(PACKAGE_DIR.parent) In\u00a0[\u00a0]: Copied! <pre>def nested_dict() -&gt; defaultdict:\n    \"\"\"Creates and returns a nested defaultdict.\"\"\"\n    return defaultdict(nested_dict)\n</pre> def nested_dict() -&gt; defaultdict:     \"\"\"Creates and returns a nested defaultdict.\"\"\"     return defaultdict(nested_dict) In\u00a0[\u00a0]: Copied! <pre>def sort_nested_dict(d: dict) -&gt; dict:\n    \"\"\"Sorts a nested dictionary recursively.\"\"\"\n    return {\n        key: sort_nested_dict(value) if isinstance(value, dict) else value\n        for key, value in sorted(d.items())\n    }\n</pre> def sort_nested_dict(d: dict) -&gt; dict:     \"\"\"Sorts a nested dictionary recursively.\"\"\"     return {         key: sort_nested_dict(value) if isinstance(value, dict) else value         for key, value in sorted(d.items())     } In\u00a0[\u00a0]: Copied! <pre>def create_nav_menu_yaml(nav_items: list, save: bool = False):\n    \"\"\"Creates a YAML file for the navigation menu based on the provided list of items.\"\"\"\n    nav_tree = nested_dict()\n\n    for item_str in nav_items:\n        item = Path(item_str)\n        parts = item.parts\n        current_level = nav_tree[\"Reference\"]\n        for part in parts[\n            2:-1\n        ]:  # skip the first two parts (docs and reference) and the last part (filename)\n            current_level = current_level[part.capitalize()]\n\n        md_file_name = parts[-1].replace(\".md\", \"\").capitalize()\n        current_level[md_file_name] = str(item).replace(\"docs/\", \"\")\n\n    nav_tree_sorted = sort_nested_dict(nav_tree)\n\n    def _dict_to_yaml(d, level=0):\n        \"\"\"Converts a nested dictionary to a YAML-formatted string with indentation.\"\"\"\n        yaml_str = \"\"\n        indent = \"  \" * level\n        for k, v in d.items():\n            if isinstance(v, dict):\n                yaml_str += f\"{indent}- {k}:\\n{_dict_to_yaml(v, level + 1)}\"\n            else:\n                yaml_str += f\"{indent}- {k}: {str(v)}\\n\"\n        return yaml_str\n\n    # Print updated YAML reference section\n    print(\n        \"Scan complete, new mkdocs.yaml reference section is:\\n\\n\",\n        _dict_to_yaml(nav_tree_sorted),\n    )\n\n    # Save new YAML reference section\n    if save:\n        (PACKAGE_DIR.parent / \"nav_menu_updated.yml\").write_text(\n            _dict_to_yaml(nav_tree_sorted)\n        )\n</pre> def create_nav_menu_yaml(nav_items: list, save: bool = False):     \"\"\"Creates a YAML file for the navigation menu based on the provided list of items.\"\"\"     nav_tree = nested_dict()      for item_str in nav_items:         item = Path(item_str)         parts = item.parts         current_level = nav_tree[\"Reference\"]         for part in parts[             2:-1         ]:  # skip the first two parts (docs and reference) and the last part (filename)             current_level = current_level[part.capitalize()]          md_file_name = parts[-1].replace(\".md\", \"\").capitalize()         current_level[md_file_name] = str(item).replace(\"docs/\", \"\")      nav_tree_sorted = sort_nested_dict(nav_tree)      def _dict_to_yaml(d, level=0):         \"\"\"Converts a nested dictionary to a YAML-formatted string with indentation.\"\"\"         yaml_str = \"\"         indent = \"  \" * level         for k, v in d.items():             if isinstance(v, dict):                 yaml_str += f\"{indent}- {k}:\\n{_dict_to_yaml(v, level + 1)}\"             else:                 yaml_str += f\"{indent}- {k}: {str(v)}\\n\"         return yaml_str      # Print updated YAML reference section     print(         \"Scan complete, new mkdocs.yaml reference section is:\\n\\n\",         _dict_to_yaml(nav_tree_sorted),     )      # Save new YAML reference section     if save:         (PACKAGE_DIR.parent / \"nav_menu_updated.yml\").write_text(             _dict_to_yaml(nav_tree_sorted)         ) In\u00a0[\u00a0]: Copied! <pre>def main():\n    \"\"\"Main function to extract class and function names, create Markdown files, and generate a YAML navigation menu.\"\"\"\n    nav_items = []\n\n    for py_filepath in PACKAGE_DIR.rglob(\"*.py\"):\n        classes, functions = extract_classes_and_functions(py_filepath)\n\n        if classes or functions:\n            py_filepath_rel = py_filepath.relative_to(PACKAGE_DIR)\n            md_filepath = REFERENCE_DIR / py_filepath_rel\n            module_path = f\"{PACKAGE_DIR.name}.{py_filepath_rel.with_suffix('').as_posix().replace('/', '.')}\"\n            md_rel_filepath = create_markdown(\n                md_filepath, module_path, classes, functions\n            )\n            nav_items.append(str(md_rel_filepath))\n\n    create_nav_menu_yaml(nav_items)\n</pre> def main():     \"\"\"Main function to extract class and function names, create Markdown files, and generate a YAML navigation menu.\"\"\"     nav_items = []      for py_filepath in PACKAGE_DIR.rglob(\"*.py\"):         classes, functions = extract_classes_and_functions(py_filepath)          if classes or functions:             py_filepath_rel = py_filepath.relative_to(PACKAGE_DIR)             md_filepath = REFERENCE_DIR / py_filepath_rel             module_path = f\"{PACKAGE_DIR.name}.{py_filepath_rel.with_suffix('').as_posix().replace('/', '.')}\"             md_rel_filepath = create_markdown(                 md_filepath, module_path, classes, functions             )             nav_items.append(str(md_rel_filepath))      create_nav_menu_yaml(nav_items) In\u00a0[\u00a0]: Copied! <pre>main()\n</pre> main()"}]}